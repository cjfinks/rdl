 % ----------------------------------------------------------------
% AMS-LaTeX Paper ************************************************
% **** -----------------------------------------------------------
\documentclass[11pt]{amsart}
\usepackage{amssymb,amsmath}
\usepackage{colordvi}
\usepackage[pdftex]{graphicx}
\usepackage{sidecap}

%\usepackage[left=2.5cm,top=2cm,right=2.5cm,nohead,nofoot]{geometry}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\title[The robust theory of adaptive compressive sampling]{The robust theory of adaptive \\ compressive sampling (ACS)}


\author[C.J. Hillar]{Christopher J. Hillar}
\address{Mathematical Sciences Research Institute, 17 Gauss Way, Berkeley, CA 94720} 
\address{Redwood Center for Theoretical Neuroscience, University of California, Berkeley, CA 94720} 
\email{chillar@msri.org}

\author[D. Rhea]{Darren Rhea}
\address{Department of Mathematics, University of California, Berkeley, CA 94720} 


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}

\maketitle

\section{Introduction}

We need to generalize Theorem 1 below to noise case (deterministic error).

\section{The ACS Reconstruction Theorem}

%\begin{figure}[t]
%	\begin{minipage}[t]{.65\linewidth} 
%	\centering
%	\includegraphics[width=1\linewidth]{figures/acs_graphic4}
%	\end{minipage}
%	\label{ACS_fig}
%	\caption{\textbf{ACS schematic.}  Above, rectangles correspond to real vectors of various dimensions (the height of the rectangle indicates dimension size).  A signal $\mathbf{x}$ with sparse cause $\mathbf{a}$ in dictionary $\Psi$ (i.e. $\mathbf{x} = \Psi \mathbf{a}$ with $\mathbf{a}$ sparse) is linearly sampled as $\mathbf{y} = \Phi \mathbf{x}$ by a measurement matrix $\Phi$ that satisfies compressive sampling condition (\ref{CSassump}). The ACS circuit uses sparse coding \cite{olshausenfield1996} to learn a dictionary $\Theta$ for expressing $\mathbf{y} = \Theta \mathbf{b}$ with $\mathbf{b}$ sparse.  Once ACS converges (Definition \ref{def:converged}), the original signal $\mathbf x$ can be recovered as $\mathbf{x} = RM \mathbf{b}$ for ACS output $\mathbf{b}$ and reconstruction matrix $RM$ as defined in Section \ref{RMsection} (this is Theorem \ref{expectationThm}).  Moreover, as Theorem \ref{mainPsiRMThm} explains, there is a fixed permutation matrix $P$ and a fixed invertible diagonal matrix $D$ such that ACS output $\mathbf{b}$ (upon receiving $\mathbf{y} = \Phi \Psi \mathbf{a}$) satisfies $\mathbf{b} = PD \mathbf{a}$. }
%\end{figure}

We shall assume throughout that patterns $\mathbf{x} \in \mathbb R^n$ in the sender region have $k$-\textit{sparse} (simply called \textit{sparse} if $k$ is understood) causes $\mathbf{a}$ in a fixed %(full rank)\footnote{The matrix $M \in \mathbb R^{n\times p}$ has \textit{full rank} if the number of linearly independent rows of $M$ (its \textit{rank}) satisfies rank$(M) = \min\{n,p\}$ (note that rank always satisfies rank$(M) \leq \min\{n,p\}$).  It can be shown, in a precise sense not elaborated on here, that a ``random" matrix will have full rank.} 
\textit{dictionary} $\Psi \in \mathbb R^{n \times p}$; that is, each pattern $\mathbf{x} \in \mathbb R^n$ can be expressed as $\mathbf{x} = \Psi \mathbf{a}$ for a column vector $\mathbf{a} \in \mathbb R^p$ with only $k \ll n \leq p$ nonzero entries.  The %(full rank) 
\textit{sampling  matrix} $\Phi \in \mathbb R^{m \times n}$ is also assumed to satisfy a \textit{compressive sampling} condition with respect to dictionary  $\Psi$.  Specifically, we assume that the matrix $A = \Phi \Psi$ is injective on the set of sparse vectors:
\begin{equation}\label{CSassump}
A \mathbf{a}_1 = A \mathbf{a}_2  \ \ \text{for $k$-sparse}  \ \mathbf{a}_1,\mathbf{a}_2 \in \mathbb R^p \ \   \Longrightarrow \  \  \mathbf{a}_1 =  \mathbf{a}_2.
\end{equation}
Clearly, this is a \textit{necessary} condition for recovering sparse causes of sampled patterns.  It turns out that condition (\ref{CSassump}) is also \textit{sufficient}, and this is the main content of the ACS Theorem (Theorem \ref{mainPsiRMThm}).  

We next describe very general conditions under which (\ref{CSassump}) holds for the matrix $A = \Phi \Psi$.  As is well-known in the compressive sampling community, assumption (\ref{CSassump}) is fulfilled with high probability for suitably random\footnote{Somewhat surprisingly, there is no known deterministic construction of such a $\Phi$ even though ``most" matrices will work.} $\Phi $ and \textit{any} fixed orthogonal (square) matrix $\Psi$ as long as 
\begin{equation}\label{mnlogn}
m \geq C k \log n.  
\end{equation}
Here, $C$ is an absolute constant that does not depend on $n$ or $k$.  In other words,  condition (\ref{CSassump}) holds with high probability when the sampling size $m$ is at least nearly linear in the complexity $k$ of the signals.  The logarithmic term in (\ref{mnlogn}) is a mild but necessary penalty for the ambient dimensionality $n$ of the signals.
%Moreover, with high probability such a $\Phi$ will satisfy (\ref{CSassump}) with respect to an exponential number of fixed bases $\Psi$.
For a more detailed discussion of these facts (including proofs) and their relationship to approximation theory and concentration of measure phenomenon, we refer the reader to \cite{Baraniuk2008} and the references therein.  When $\Psi$ is not orthogonal (and possibly nonsquare), assumption (\ref{CSassump}) is still fulfilled with very high probability for random $\Phi$ as long as (\ref{mnlogn}) holds and $\Psi$ has certain incoherence properties \cite{BiRiTsy08, RasWaiYu09}.

%As discussed in the body of the main article, t
Briefly in words, the \textit{adaptive compressive sampling} (ACS) scheme is a (unsupervised) dictionary learning \cite{olshausenfield1996} of linearly sampled  signals.
%(or subsampled if there is a compression $\frac{m}{n} < 1$)  
Again in words, when we say that ACS has \textit{converged} on a sparsity inducing dictionary $\Theta$, we mean that compressed, sparsely encoded signals can be represented as a sparse linear combination of columns of the dictionary $\Theta$, the representation being inferred by a sparse recovery procedure $f$.  The precise mathematical definition is as follows.

\begin{definition}\label{def:converged}
We say that ACS learning has \textit{converged on a sparsity-inducing dictionary} $\Theta  \in \mathbb R^{m \times p}$ if the sampling $\mathbf{y} = \Phi \mathbf{x}$ by  the compression matrix $\Phi$ of the sender region's signal $\mathbf{x} = \Psi \mathbf{a}$ (with $\mathbf{a}$ $k$-sparse) always satisfies \[\mathbf{y} = \Theta\mathbf{b}\] for a $k$-sparse vector $\mathbf{b} = \widehat{\mathbf{b}}(\mathbf{y})$ inferred from the convex optimization:\footnote{For some fixed $\lambda > 0$.  Here, we recall that for a column vector $\mathbf{b} = (b_1,\ldots,b_p)^{\top} \in \mathbb R^p$, the $\ell_1$ norm of $\mathbf{b}$ is $|\mathbf{b}|_1 = |b_1| + \cdots + |b_p|$.  Also, for a vector $\mathbf{z} = (z_1,\ldots,z_m)^{\top} \in \mathbb R^m$, the $\ell_2$ norm of $\mathbf{z}$ is $||\mathbf{z}||_2 = (z_1^2 + \cdots + z_m^2)^{1/2}$.}
\begin{equation}\label{ACSbhat}
\begin{split}
%\widehat{\mathbf{b}}( \mathbf{y}) =  \underset{\mathbf{b} \  \text{s.t.} \ \mathbf{y} = \Theta \mathbf{b}}{\operatorname{argmin}} \ |\mathbf{b}|_1.
 \widehat{\mathbf{b}}(\mathbf{y}) = \ & \underset{\mathbf{b} \in \mathbb R^p}{\operatorname{argmin}} \left\{ || \mathbf{y} - \Theta \mathbf{b} ||^2_2 + \lambda |\mathbf{b}|_1 \right\}.
\end{split}
\end{equation}
\end{definition}
% (\ref{ACSbhat}). 
%[*** we should cite here some convex optimization book about how this is unique, etc etc... ***]
%[***Somewhere we need to remark that (\ref{ACSbhat}) makes $\widehat{b}$ into a measurable function.  This is for the RM construction part of the story. ***]
In other words, ACS converges when the learning has succeeded to make $\Theta$ a sparse dictionary of the compressed data $\mathbf{y}$.  

As we explain in this section, once ACS has converged, the sparse causes $\mathbf{a}$ of an uncompressed signal $\mathbf{x} = \Psi \mathbf{a}$ are faithfully recovered by the output $\widehat{\mathbf{b}}$ of ACS (see Figure \ref{ACS_fig} and Theorem \ref{mainPsiRMThm} below).   
%Additionally, the number of elements that are active over all outputs of ACS is no more than that of the sparse representations $\mathbf{a}$ for $\mathbf{x}$.  
More precisely, we prove that any procedure which takes input $\mathbf{y} = \Phi \Psi \mathbf{a}$ (with $\mathbf{a}$ sparse) and produces sparse vectors $f(\mathbf{a})$ satisfying \[\mathbf{y} = \Theta f(\mathbf{a})\] 
for a matrix $\Theta$ must necessarily recover the sparse causes $\mathbf{a}$ up to a fixed diagonal scaling $D$ and permutation (or relabeling) $P$; that is, $f(\mathbf{a}) = PD\mathbf{a}$.  

This result, the content of Theorem \ref{mainHelperThm} below, is surprising and remarkably general; it says that any dictionary learning scheme producing sparse reconstructions in a compressed space automatically gives faithful transmission of sparse signals -- regardless of the original dictionary $\Psi$ or sampling matrix $\Phi$.\footnote{Of course, as long as $A= \Phi \Psi$ satisfies the necessary condition for recovery (\ref{CSassump}).} The recovery method (\ref{ACSbhat}) for our choice of procedure is natural in this context because of its efficient use in compressive sampling \cite{candes2006} to recover sparse vectors. 

Although self-contained and (mathematically) elementary, our proof of Theorem \ref{mainHelperThm} relies on abstract ideas from Ramsey theory (see Theorem \ref{InfSetCor} below).  Recall that a \textit{permutation matrix} is a $\{0,1\}$-matrix $P$ with exactly one $1$ in each column and exactly one $1$ in each row (thus, $P\mathbf{v}$ for a vector $\mathbf{v}$ just permutes its entries).\footnote{Also, $PP^{\top} = P^{\top}P = I$, where $I$ denotes the $p \times p$ identity matrix, and $M^{\top}$ for a matrix $M$ is its transpose.}  

\begin{theorem}\label{mainHelperThm}
Suppose that $f: \mathbb R^p \to \mathbb R^p$ is a map sending $k$-sparse vectors to $k$-sparse vectors.  Also, suppose that $A \in \mathbb R^{m \times p}$ satisfies compressive sampling condition (\ref{CSassump}) and that $B \in \mathbb R^{m \times p}$ is a matrix such that:
\begin{equation}\label{bhatProperty}
A \mathbf{a} = B f(\mathbf{a}), \ \ \  \text{for all $k$-sparse $\mathbf{a} \in \mathbb R^p$}.
\end{equation}
Then, there exists an invertible diagonal matrix $D \in \mathbb R^{p \times p}$ and a permutation matrix $P \in \mathbb R^{p \times p}$ such that 
\begin{equation}\label{BPDA}
A = BPD %\ \ \  \text{and} \ \ \ \ PP^{\top} f(\mathbf{a})  =  f(\mathbf{a}), \text{ for all $k$-sparse $\mathbf{a} \in \mathbb R^p$}.
\end{equation}
%Moreover, if $p = q$ or if $f$ additionally satisfies $f(r \mathbf{a}) \in \text{\rm Span}\{f(\mathbf{a})\}$ for $1$-sparse $\mathbf{a}$ and $r \in \mathbb R$, then for all $k$-sparse $\mathbf{a}$, we have  
and for all $k$-sparse $\mathbf{a}$,
\begin{equation}\label{faPDa}
 f(\mathbf{a})  = PD\mathbf{a}.
\end{equation}
\end{theorem}
\begin{remark}
For those readers with some abstract algebra experience, we remark that our proof of Theorem \ref{mainHelperThm} generalizes easily to the case when $\mathbb R$ is replaced by any field such as the rational numbers $\mathbb Q$.
\end{remark}


Before proving this (seemingly technical) theorem above, we explain how our main application, the ACS Theorem, follows directly from it.

\begin{theorem}[The ACS Theorem]\label{mainPsiRMThm}
Suppose that ACS converges on a sparsity-inducing dictionary $\Theta \in \mathbb R^{m \times p}$.  If $A = \Phi \Psi$ satisfies the compressive sampling condition (\ref{CSassump})%and both $\Phi,\Psi$ have full rank
, then there is a fixed invertible diagonal matrix $D \in \mathbb R^{p \times p}$ and a fixed permutation matrix $P \in \mathbb R^{p \times p}$ such that:  
\begin{equation}\label{thetaeqn}
\Phi \Psi = \Theta P D
\end{equation}
and for all signals $\mathbf{x} = \Psi \mathbf{a}$ (with $\mathbf{a}$ $k$-sparse) which sample to $\mathbf{y} = \Phi \mathbf{x}$, the ACS output $\mathbf{b} = \widehat{\mathbf{b}}(\mathbf{y})$ from (\ref{ACSbhat}) satisfies 
\begin{equation}\label{bintermsofa}
\mathbf{b} = PD  \mathbf{a}.  
\end{equation}
\end{theorem}

\begin{proof} Let $\mathbf{x} = \Psi \mathbf{a}$ for a sparse vector $\mathbf{a}$, and set $\mathbf{y} = \Phi \mathbf{x}$.  By assumption, the sparse output $\mathbf{b} = \widehat{\mathbf{b}}$ of ACS satisfies $\mathbf{y} = \Theta \mathbf{b}$.  %Let $\Psi^{\dagger} := \Psi^{\top}(\Psi \Psi^{\top})^{-1}$ be the \textit{pseudo-inverse} of $\Psi$, which satisfies $\Psi \Psi^{\dagger} = I$.\footnote{The $n \times n$ matrix $\Psi \Psi^{\top}$ is invertible since the rank of $\Psi$ is $n$; see \cite[p. 13]{Horn1990}.}  Also, let $\Phi^{\dagger} = \Phi^{\top}(\Phi \Phi^{\top})^{-1}$ be the pseudo-inverse of $\Phi$.  Writing $\Theta =  \Phi \Psi  \cdot \Psi^{\dagger} \Phi^{\dagger}\Theta$, 
It follows that  
\begin{equation}\label{BeqAallk}
\Phi \Psi \cdot \mathbf{a} =  \Theta \cdot  \widehat{\mathbf{b}}(\mathbf{y}), \ \ \ \text{for all $k$-sparse } \mathbf{a}.
\end{equation}
Set $A = \Phi \Psi$, $B =\Theta$, and $f(\mathbf{a}) =  \widehat{\mathbf{b}} (\Phi \Psi \mathbf{a})$.  We now use statement (\ref{BeqAallk}) and Theorem \ref{mainHelperThm} to conclude directly that (\ref{thetaeqn}) and (\ref{bintermsofa}) hold.
\end{proof}

\begin{corollary}
Suppose $\Phi = I$ so that ACS reduces to sparse coding \cite{olshausenfield1996}.  If ACS converges on a sparsity-inducing dictionary $\Theta$, then it must be a scaled permutation of the original dictionary $\Psi$.
\end{corollary}
\begin{proof}
Since $\Phi$ is the identity matrix, it automatically satisfies (\ref{CSassump}).  Theorem \ref{mainPsiRMThm} then says that $\Theta = \Psi D^{-1}P^{\top}$ for an invertible diagonal matrix $D$ and permutation $P$.
\end{proof}


\bibliographystyle{plain}
\bibliography{acs}


\end{document}
