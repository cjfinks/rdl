% REFERENCE NOTES
% 'Spark' of a matrix coined and defined in "Optimally  sparse representation in  general  (non-orthogonal)
% dictionaries via L1 minimization" and applied to the study of uniqueness in "Sparse signal reconstruction % from limited data using FOCUSS:   A   re-weighted   norm   minimization   algorithm"

% QUESTIONS
% 1) Are necessary conditions for uniqueness in the case where A (or sources) is known still necessary
% conditions for when A (or sources) is unknown? Permutation scaling ambiguity...
% 2) If 1) is true, can we derive the conditions on A and the data for SCA from the necessary conditions
% for these simpler constrained problems?

\documentclass[journal, onecolumn]{IEEEtran}

% *** MATH PACKAGES ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Robust Identifiability in Sparse Dictionary Learning}

\author{Charles~J.~Garfinkle,  Christopher~J.~Hillar%
\thanks{The research of Garfinkle and Hillar was conducted while at the Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA; e-mails: cjg@berkeley.edu, chillar@msri.org.}}%

\maketitle

\begin{abstract}
We study uniqueness in sparse dictionary learning when reconstruction of data is approximate.
\end{abstract}

\begin{IEEEkeywords}
bilinear inverse problem, identifiability, dictionary learning, sparse coding, matrix factorization, compressed sensing, combinatorial matrix theory, blind source separation
\end{IEEEkeywords}

%===================================
% 			INTRODUCTION
%===================================

\section{Introduction}

\IEEEPARstart{O}{ne} of the fundamental questions in data analysis is how to represent the data in a way that reveals structure. Recently, algorithms have been developed for uncovering \emph{sparse} structure in a given dataset $\mathbf{y}_1, \ldots, \mathbf{y}_N \in \mathbb{R}^n$ by approximately solving the constrained optimization problem:
\begin{align}\label{DictionaryLearning}
\min_{A, \mathbf{a}_i} \sum_{i=1}^N \|\mathbf{y}_i - A\mathbf{a}_i\|_2 \indent \text{subject to } \|\mathbf{a}_i\|_0 \leq k 
\end{align}
%
for unknown $A \in \mathbb{R}^{n \times m}$ and $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with $k < m$. (It almost goes without saying that $m < N$ as well, lest the solution be completely trivial.) Known variously as dictionary learning, sparse coding, or sparse matrix factorization, this framework emphasizes parsimony in representation by approximating each $\mathbf{y}_i$ as some linear combination of at most $k$ columns of some matrix $A$, typically with $k \ll m$. 

Approximating the optimal solution to \eqref{DictionaryLearning} has been shown in general to be NP-hard \cite{Razaviyayn} \textbf{[Razaviyayn 2015, but they did it within an additive error. Tollman 2015 did it for multiplicative error for the dual problem. Should we say this isn't necessarily a big deal since there may be some \emph{particular} instances where NP-hard problems are solvable in poly time?}. Nonetheless, it is natural to wonder when the solution to \eqref{DictionaryLearning} is in some sense unique, or at least approximately so. This is a question of fundamental concern in \emph{sparse component analysis} (SCA) \cite{Georgiev05}, wherein one assumes the linear model:
\begin{align}\label{LinearModel}
\mathbf{y}_i = A\mathbf{a}_i + \mathbf{\eta}_i 
\end{align}
%
with the vector $\mathbf{\eta}_i \in \mathbb{R}^n$ accounting for both "noise" in the data acquisition process and the degree to which the model deviates from reality (e.g. the $\mathbf{a}_i$ may be known to be only approximately $k$-sparse). The goal in SCA is to recover (approximately) the sparse \emph{sources} $\mathbf{a}_i$ from the observed \emph{mixtures} $\mathbf{y}_i$ by approximately solving \eqref{DictionaryLearning}. \textbf{How do sparsity of $\mathbf{a}_i$ and upper bound on $\|\mathbf{\eta}_i\|$ tradeoff to make \eqref{DictionaryLearning} solve \eqref{LinearModel}?}

Letting $Y$ and $X$ be the matrices with columns $\mathbf{y}_i$ and $\mathbf{a}_i$, respectively, the optimization \eqref{DictionaryLearning} is seen to minimize the matrix norm $\|Y-AX\|_{2,1}$ subject to sparsity constraints on the columns of $X$. From this matrix factorization perspective it is clear that SCA is a special case of bilinear inverse problem and thus has an associated \emph{ambiguity transform group}. Clearly, if $AX$ approximates $Y$ then so does $(AD^{-1}P^{-1})(PDX)$ for any permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ and moreover this transformation preserves the sparsity of the columns of $X$. The SCA problem can thus at best (even when $\mathbf{\eta}_i = 0$) be solved up to this inherent permutation and scaling ambiguity, which motivates the following definition and problem.

\begin{definition}\label{Uniqueness}
A set of vectors $\{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ has a \textbf{$k$-sparse representation} in $\mathbb{R}^m$ when for some $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ we have $\mathbf{y}_i = A\mathbf{a}_i$ for all $i = 1, \ldots, N$. We say that $A$ and the $\mathbf{a}_i$ forming this representation are \textbf{identifiable} (up to permutation and scaling ambiguity) given $Y$ when every other $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ for which $\mathbf{y}_i = B\mathbf{b}_i$ for all $i = 1, \ldots, N$ necessarily satisfy $A = BPD$ and $\mathbf{b}_i = PD\mathbf{a}_i$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. The representation is \textbf{stable} when there exists some $\varepsilon > 0$ such that if $B$ and the $\mathbf{b}_i$ only satisfy $\|\mathbf{y}_i - B\mathbf{b}_i\|_2 \leq \varepsilon$ for all $i = 1, \ldots, N$, then
\begin{align}\label{def1}
\|A - BPD\|_2 \leq C_1 \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|_2 \\
\|\mathbf{b}_i - PD\mathbf{a}_i\|_2 \leq C_2 \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|_2
\end{align}
for some $C_1, C_2>0$. We call $\varepsilon$ and $C_1$ the \textbf{error threshold} and \textbf{stability constant}, respectively, associated with the $k$-sparse representation formed by $A$ and the $\mathbf{a}_i$.
\end{definition}

Suppose now that $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ is a dataset for which for some $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ we have $\|\mathbf{y}_i - A\mathbf{a}_i\|_2 \leq \varepsilon$ for $i = 1, \ldots, N$ for some $\varepsilon > 0$. That is, every data point can be approximated (up to error $\varepsilon$) by some linear combination of $k$ columns of $A$. If the set $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$ wih error threshold $2\varepsilon$ and stability constant $C$ then, by the triangle inequality, any alternate $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ satisfying $\|\mathbf{y}_i - B\mathbf{b}_i\| \leq \varepsilon$ for $i = 1, \ldots, N$ is such that $\|A\mathbf{a}_i - B\mathbf{b}_i\| \leq 2\varepsilon$, hence according to Definition \ref{Uniqueness} it must also satisfy \eqref{def1}. We say such datasets $Y$ have stable $k$-sparse \emph{approximate representations} in $\mathbb{R}^m$.

Some readers may find the following alternative definition more appealing:

\begin{definition}\label{Uniqueness}
We say a dataset $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ has a stable $k$-sparse approximation in $\mathbb{R}^m$ when:
\begin{enumerate}
\item There exists some $A \in \mathbb{R}^{n \times m}$ and $\varepsilon > 0$ such that for every $\mathbf{y} \in Y$, the $\varepsilon$-ball $\mathcal{B}_2(\mathbf{y}, \varepsilon)$ centered at $\mathbf{y}$ intersects exactly one subspace spanned by $k$ columns of $A$.
\item Any matrix $B \in \mathbb{R}^{n \times m}$ for which every such ball $\mathcal{B}_2(\mathbf{y}, \varepsilon)$ intersects a subspace spanned by $k$ columns of $B$ necessarily satisfies $\|A - BPD\|_2 \leq C_1\varepsilon$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$, invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ and $C > 0$.
\end{enumerate}
\end{definition}

\begin{problem}\label{DUTproblem}
Let $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N \} \subset \mathbb{R}^n$ be generated as $\mathbf{y}_i = A\mathbf{a}_i  + \mathbf{n}_i$ for some matrix $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_i \in \mathbb{R}^m$ with $\|\mathbf{n}_i\|_2 \leq \varepsilon$. When does $Y$ have a stable $k$-sparse approximate representation in $\mathbb{R}^m$?
\end{problem}

Conditions for the \emph{identifiability} of the parameters underlying SCA were first investigated for the case $\mathbf{y}_i = A\mathbf{a}_i$ by Georgiev et. al. \cite{Georgiev05} and subsequently by Aharon et. al. \cite{Aharon06}. The most general conditions to date have recently been proven by Hillar and Sommer [cite], who provide deterministic and probabilistic guarantees for the identifiability of the model parameters contained in $A$ and the original sparse codes $\mathbf{a}_i$. We refine their proofs to take into account the possibility of noise contaminating the measurements (or inadequacy of the model) and significantly reduce their theoretically required number of samples. These \emph{robust} identifiability conditions describe when the true model parameters can be uniquely determined in the sense of Definition \ref{Uniqueness}. Moreover, we provide guarantees for unique recovery of $A$ and the $\mathbf{a}_i$ when only an upper bound on the dimensionality of the sources is known. Our conditions are derived from the underlying geometry of the dictionary learning problem, hence they apply regardless of which algorithm is used to solve \eqref{DictionaryLearning}. Table \ref{results} gives a summary of our results.

\begin{table}\label{results}
\begin{center}
$\begin{array}{c|cc} \text{Dataset } Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, \ldots,A\mathbf{a}_N + \mathbf{\eta}_N,\} \text{ with $k$-sparse } \mathbf{a}_i \in \mathbb R^m \text{ chosen:}   & \text{Total sufficient samples } N \text{ for a unique recovery}  \\\hline 
\text{in general position with $k{m \choose k}$ from each support set in $\mathcal{T}$, independent of $A$ satisfying spark condition (\ref{SparkCondition})}  & mk{m \choose k} \\\hline     
 \text{$k+1$ chosen "randomly" from each support set in $\mathcal{T}$, given matrix $A$ satisfying spark condition (\ref{SparkCondition})}  & m(k+1) \ \ \text{(with probability one)} \\\hline 
\text{$k+1$ chosen "randomly" from each support set in $\mathcal{T}$, independent of $A \notin Z$ 
}  & m(k+1) \ \ \text{(with probability one)} \\\hline 
\text{"randomly" from each support set in $\mathcal{T}$, given matrix $A$ satisfying spark condition (\ref{SparkCondition})}  & m(k+1)\beta^{-1}\ \ \text{(with probability $1-\beta$)} \\\hline 
\end{array}$
\end{center}
\caption{The number of subsamples sufficient for $Y = \{\mathbf{y}_1, \ldots,\mathbf{y}_N\}$ to have a unique sparse coding.}
\label{table_N}
\vspace{-.3 in}
\end{table}

Before stating our first theorem, it is instructive to first consider the simpler case where we wish to identify the sources $\mathbf{a}_i$ from the noiseless mixture $\mathbf{y}_i = A\mathbf{a}_i$ when $A$ is already known. Evidently, $A$ must satisfy the following \emph{spark condition}
\begin{align}\label{SparkCondition}
A\mathbf{a}_1 = A\mathbf{a}_2 \implies \mathbf{a}_1 = \mathbf{a}_2 \indent \text{for all $k$-sparse } \mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^m
\end{align}
%
for such recovery to be possible. This condition ensures that distinct sparse sources are distinguishable in their measurements. As it turns out, this is also enough to guarantee uniqueness in the case where we don't know $A$ a priori, as our main result states:

%=== STATEMENT OF DETERMINISTIC UNIQUENESS THEOREM ===%

\begin{theorem}\label{DeterministicUniquenessTheorem}
Given positive integers $n, m$ and $k < m$, there exist $N =  mk{m \choose k}$ $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates a set of vectors $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{theorem}

In fact, there are many such sets of deterministically produced $\mathbf{a}_i$; we give a parametrized family in Section \ref{DUT}. Note that Theorem 1 in [HS11] is a direct consequence of this more general result, with an added improvement on $N$, the number of samples required to guarantee uniqueness. In fact, all of the theorems and corollaries in [HS11] are corollaries of our theorems. Our proofs are generalizations of the basic framework laid out in their paper.

The stability constant $C$ associated to a given $k$-sparse representation can be explicitly calculated from certain properties $A$ and the $\mathbf{a}_i$ which we now define. A simple compactness argument can be made to show that any matrix satisfying \eqref{SparkCondition} must also satisfy
\begin{align}\label{RIP}
\|A\mathbf{a}_1 - A\mathbf{a}_2 \|_2 \geq  \alpha \|\mathbf{a}_1 - \mathbf{a}_2\|_2 \indent \text{ for all $k$-sparse } \mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^m
\end{align}
%
for some $\alpha \in (0,1]$. We can equally well say that $\|A\mathbf{x}\| \geq \alpha\|\mathbf{x}\|$ for all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$. We define the \emph{$2k$-sparse lower bound of A} to be the largest constant for which \eqref{RIP} holds and denote it by $\alpha_{2k}(A)$. Since $A$ is a real matrix, it also has bounded norm and hence \eqref{RIP} is equivalent to the \emph{restricted isometry property} \cite{CandesTao05} familiar from work in the field of compressed sensing.

It can be shown (see the Appendix) that \eqref{SparkCondition} implies the existence of a nonzero \emph{Friedrichs angle} $\theta_F \in (0,\frac{\pi}{2}]$ (also known as the minimal Jordan angle) between all pairs of subspaces spanned by the columns of $A$ of dimensionality less than or equal to $k$. For completeness we state the definition of $\theta_F$ here:

\begin{definition}\label{FriedrichsDefinition}
The \emph{Friedrichs angle} $\theta_F(V,W) \in [0,\frac{\pi}{2}]$ between subspaces $V,W \subseteq \mathbb{R}^m$ is the minimal angle formed between unit vectors in $V \cap (V \cap W)^\perp$ and $W \cap (W \cap V)^\perp$. That is,
\begin{align}
\cos\left[\theta_F(V,W)\right] := \max\left\{ \frac{ \langle v, w \rangle }{\|v\|\|w\|}: v \in V \cap (V \cap W)^\perp, w \in W \cap (V \cap W)^\perp \right\}.
\end{align}
\end{definition}

In what follows, we will use the notation $[m]$ for the set $\{1, ..., m\}$, and ${[m] \choose k}$ for the set of subsets of $[m]$ of cardinality $k$. We now define special collections of such subsets. Given positive integers $k < m$ and some permutation $\sigma \in \Sigma_m$ (where $\Sigma_m$ denotes the symmetric group on $m$ elements), let
\begin{align}
S_\sigma(i) := \{1+\sigma(i), \ldots, 1+\sigma(i + (k-1)) \} \indent \text{for} \indent i = 0, \ldots, m-1
\end{align}
%
with addition modulo $m$. In words, $S_\sigma$ parametrizes the collection of sets that form intervals of length $k$ within the set of elements of $[m]$ arranged in the cyclic order induced by $\sigma$.

For notational convenience we make the following definition as well.

\begin{definition}\label{SpecialSupportSet}
For any sequence $V_1, \ldots, V_k$ of closed subspaces of $\mathbb{R}^m$, let
\begin{align}
\xi(V_1, \ldots, V_k) := \left[1 - \prod_{i=1}^{k-1} \left(1 - \cos^2\left[ \theta_F(V_i, \cap_{j=i+1}^k V_j) \right] \right) \right]^{1/2}.
\end{align}
Given $A \in \mathbb{R}^{n \times m}$ and $k < m$, we define $\phi_k(A) \in [0,1]$ for $k \geq 2$ to be:
\begin{align}\label{rho}
\phi_k(A) := \min_{ \substack{\sigma \in \Sigma_m \\ i_1 \neq \cdots \neq i_k \in [m] } } \left\{ 1 - \xi(\text{Span}\{A_{S_{\sigma}(i_1)}\}, \ldots, \text{Span}\{A_{S_{\sigma}( i_k)}\}) \right\},
\end{align}
%
with $\phi_1(A) := 1$.
\end{definition}

We now state the specifics of Theorem \ref{DeterministicUniquenessTheorem} in terms of these quantities.

%=== SPECIFICS OF DETERMINISTIC THEOREM ===%

\begin{remark}[Theorem \ref{DeterministicUniquenessTheorem} cont'd]
Specifically, the $\mathbf{a}_i$ are constructed so that any $k$ of them are linearly independent. In particular, any $k$ vectors $\mathbf{a}_i$ sharing the same support satisfy
\begin{align}\label{DataSpread}
\|\sum_{j = 1}^k c_j \mathbf{a}_{i_j}\|_2 \geq \rho \|c\|_2 \indent \text{for all } c = (c_1, ..., c_k) \in \mathbb{R}^k.
\end{align}
%
for some $\rho > 0$. If $A$ has unit norm columns and satisfies spark condition \eqref{SparkCondition}, then for any matrix $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ for which $\|\mathbf{y}_i - B\mathbf{b}_i\| < \frac{[\alpha_{2k}(A)]^2\phi_k(A)\rho}{\sqrt{k}}$ for all $i = 1, \ldots, N$ we have
\begin{align}
\|A - BPD\|_2 \leq \left(  \frac{k^{2/3}}{\alpha_{k}(A)\phi_k(A)\rho} \right) \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|
\end{align}
%
for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$, and
\begin{align}\label{b_PDa}
\|\mathbf{b}_i - PD\mathbf{a}_i\|_1 \leq \frac{ \|D\|_2}{\alpha_{2k}(A)}  \left( 1 + \frac{k^{2/3}}{\alpha_{k}(A)\phi_k(A)\rho}\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right) \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|.
\end{align}
\end{remark}

Our next result exploits randomness to give a different construction requiring fewer samples. Fix a matrix $A$ satisfying spark condition \eqref{SparkCondition} and a small $\beta > 0$. There is a simple random drawing procedure (Definition \ref{RandomDraw}) for generating $N = \beta^{-1}m(k+1)$ $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N$ such that $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, \ldots, A\mathbf{a}_N + \mathbf{\eta}_N\}$ has a unique sparse coding (with probability at least $1 - \beta$), provided $\|\mathbf{\eta}_i\|$ is bounded by a small enough constant. In particular, sparse matrix factorization of sparsely coded data is typically unique for a number of samples linear in the dimensionality and sparsity level of the sparse codes, provided their supports are distributed enough over those contained in a special support set (specifically, $\mathcal{T}$ from Definition \ref{SpecialSupportSet}).

A subtle difference between these two results is that $\mathbf{a}_1, \ldots, \mathbf{a}_N$ in Theorem \ref{DeterministicUniquenessTheorem} are independent of the generation matrix $A$. The following statement is the best we are able to do in this direction using random methods. With probability one, a random draw of $(k+1)$ $k$-sparse $\mathbf{a}_i$ from each of the $m$ support sets in $\mathcal{T}$ satisfies the following property. There is a set of matrices $Z \in \mathbb{R}^{n \times m}$ with Lebesgue measure zero such that if $A \notin Z$, then there exists some $\varepsilon_0$ small enough such that $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, \ldots, A\mathbf{a}_N + \mathbf{\eta}_N\}$ for $\|\mathbf{\eta}_i\| \leq \varepsilon_0$ has a unique sparse coding.

It is important for us to note how these theorems of ours fit in with others comprising the field of \emph{theoretical dictionary learning}. While a collection of greedy optimization algorithms have been proposed for solving \eqref{DictionaryLearning}, as yet there exist no global convergence guarantees for these techniques. Local identifiability analyses have demonstrated that the global minimum is a local minimum of cost functions of L0 and L1 cost functions, (local) convergence guarantees of greedy and convex algorithms. A most welcome corollary to our theorems is a closed form expression for checking when any dictionary learning algorithm has converged to a global minimum of \eqref{DictionaryLearning} by asserting whether or not the reconstruction error of the proposed solution is indeed small enough. 

It is also informative to elaborate on the relationship of our results to the field of compressive sensing. [ref?] CS theory provides conditions under which it is possible to recover data vectors $\mathbf{x}$ with sparse structure after they have been linearly subsampled as $\mathbf{y} = \Phi \mathbf{a}$ by a known compression matrix $\Phi$. The sparsity usually enforced is that the vectors $\mathbf{x}$ can be expressed as $\mathbf{x} = \Psi\mathbf{a}$ using a known dictionary matrix $\Psi$ and $m$-dimensional vectors $\mathbf{a}$ with at most $k < m$ nonzero entries. Such vectors $\mathbf{a}$ are called $k$-\emph{sparse}. A necessary condition for the unique recovery of $\mathbf{a}$ given $\mathbf{y}$ is that the generation matrix $A = \Phi\Psi$ satisfy the spark condition \eqref{SparkCondition}. Otherwise, different sparse sources would be indistinguishable in the compressed space. Provided the dimension $n$ of $\mathbf{y}$ satisfies
\begin{align}\label{CScondition}
n \geq Ck\log\left(\frac{m}{k}\right),
\end{align}
%
the theory guarantees that with high probability a randomly generated $\Phi$ will yield an $A$ satisfying \eqref{SparkCondition}. In contrast, the goal of SCA is to recover both the code vectors \emph{and} the generation matrix from measurements. We show that the same uniqueness conditions required by CS also guarantee uniqueness in SCA given enough samples distributed over suitable supports.

The organization of this paper is as follows.in Section \ref{DUT} we state our main tool from combinatorial matrix theory and then derive from it Theorem \ref{DeterministicUniquenessTheorem}. We next provide precise statements in Section \ref{PUT} of our randomized theorems and then prove them in Section \ref{PUTproof}. In Section (??) we describe how to extend the theorems in the previous sections to the case where we have only an upper bound on the number of sparse components. The final section is a discussion. An appendix contains the proof of Lemma \ref{MainLemma} as well as some auxiliary proofs. 

Have the cool small dimensional example here. Data in $3D$-space coded by planes. $n=3, k=2$. 

Implication: overcomplete dimensionality of any dataset with respect to $\varepsilon$. If you coded very well some data sparsely then you can do a check and if that check holds up then anyone else who codes the data sparsely too has learned the same dictionary. 
 
%============================================
% PROOF OF DETERMINISTIC UNIQUENESS THEOREM
%============================================

\section{Deterministic Uniqueness Theorem}\label{DUT}

Recall that $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ for real vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$ is the vector space consisting of their $\mathbb{R}$-linear span:
%
\[ \text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\} = \left\{ \sum_{i=1}^\ell t_i\mathbf{v}_i : t_1, \ldots, t_\ell \in \mathbb{R}\right\}. \]
%
Finally, for a subset $S \subseteq [m]$ and matrix $A$ with columns $\{A_1,...,A_m\}$ we define
%
\[ \text{Span}\{A_S\} = \text{Span}\{A_s: s \in S\}. \]

%======== CASE K = 1 ============

Before proving Theorem \ref{DeterministicUniquenessTheorem} in full generality it is illustrative to consider the case when $k=1$. In this simple case, we only need $N = m$ samples to guarantee uniqueness. Set $\mathbf{a}_i = \mathbf{e}_i$ $(i = 1, \ldots, m)$ to be the standard basis vectors in $\mathbb{R}^m$ and fix some $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} with unit norm columns. Suppose now that for some $B \in \mathbb{R}^{n \times m}$ and 1-sparse $\mathbf{b}_i \in \mathbb{R}^m$ we have  $\|A\mathbf{a}_i - B\mathbf{b}_i\|_2 \leq \varepsilon < \frac{\alpha}{\sqrt{2}}$ for all $i \in [m]$. Since the $\mathbf{b}_i$ are 1-sparse, there must exist $c_1, \ldots, c_m \in \mathbb{R}$ such that 
\begin{align}\label{1D}
\|A\mathbf{e}_i - c_iB\mathbf{e}_{\pi(i)}\|_2 \leq \varepsilon \indent \text{for all} \indent i \in [m].
\end{align}
for some map $\pi: [m] \to [m]$. 
Note that if $c_i = 0$ for some $i$ then $\|A\mathbf{e}_i\| < 1$ (since $\alpha \leq 1$) which contradicts that $A$ has unit norm columns. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m]$. Then $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2  \leq \varepsilon$ and $\|A\mathbf{e}_j - c_jB\mathbf{e}_{\ell}\|_2 \leq \varepsilon$. Summing and scaling these two inequalities by $|c_j|$ and $|c_i|$, respectively, we apply the triangle inequality and then \eqref{RIP} to yield
\begin{align*}
(|c_i| + |c_j|) \varepsilon
&\geq |c_j|\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2 + |c_i|\|c_jB\mathbf{e}_{\ell} - A\mathbf{e}_j\|_2 \\
&\geq \|c_jAe_i + c_iAe_j\|_2 \\
&\geq \alpha\|c_je_i + c_ie_j\|_2
\end{align*}
%
which is in contradiction with the fact that $\|x\|_1 \leq \sqrt{2}\|x\|_2$ for all $x \in \mathbb{R}^2$ and $\varepsilon < \frac{\alpha}{\sqrt{2}}$. Hence, $\pi$ is injective. Letting $P$ and $D$ denote the following permutation and diagonal matrices, respectively:
\begin{equation}\label{PandD}
P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right), \ \ D = \left(\begin{array}{ccc}c_1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & c_m\end{array}\right),
\end{equation}
%
we see that \eqref{1D} becomes $\|(A - BPD)\mathbf{e}_i\|_2 \leq C\varepsilon$ for all $i \in [m]$, where $C = 1$. 

% ======== b - PDa =========
We note that from this result we can, in general, derive an upper bound on $\|\mathbf{b}_i - PD\mathbf{a}_i\|$ as well. By the triangle inequality, we have that $\|(A-BPD)\mathbf{x}\| \leq \|\mathbf{x}\|_1C\varepsilon$ for all $k$-sparse $\mathbf{x} \in \mathbb{R}^m$. By \eqref{RIP}, we have:
\begin{align*}
\alpha_{2k}(A) \|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i\|_2 
&\leq \|A(D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i)\|_2 \\
&\leq \|(A - BPD)D^{-1}P^{-1}\mathbf{b}_i\|_2 + \|B\mathbf{b}_i - A\mathbf{a}_i\|_2 \\
&\leq \varepsilon (1 + C\|D^{-1}P^{-1}\mathbf{b}_i\|_1). \\
\end{align*}

Hence, 
\begin{align*}
\|\mathbf{b}_i - PD\mathbf{a}_i\|_2 &=  \|PD(D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i)\|_2 \\
&\leq \|D\|_2 \|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i\|_2 \\
&\leq \frac{\varepsilon \|D\|_2}{\alpha_{2k}(A)}\left( 1 + C\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right)
\end{align*}

% ========== DEFINITIONS FOR MAIN LEMMA (K>1) ================

Unfortunately, the proof for larger $k$ is more challenging since in general it is nontrivial to produce $P$ and $D$ as in \eqref{PandD}. Our main tool for the proof is a result in combinatorial linear algebra. First, we define the following metric on the set of linear subspaces of $\mathbb{R}^m$:

\begin{definition}
Let $V, W$ be subspaces of $\mathbb{R}^m$ and let $d(v,W) := \inf\{\|v-w\|_2: w \in W\} = \|v - \Pi_W v\|_2$ where $\Pi_W$ is the orthogonal projection operator onto subspace $W$. The \emph{gap} metric $\Theta$ on subspaces of $\mathbb{R}^{m}$ is [see Theory of Linear Operators in a Hilbert Space p. 69 who cites first reference]
\begin{equation}\label{SubspaceMetric}
\Theta(V,W) := \max\left( \sup_{\substack{v \in V \\ \|v\| = 1}} d(v,W), \sup_{\substack{w \in W \\ \|w\| = 1}} d(w,V) \right).
\end{equation}
\end{definition}
%
In our proof we will make use of the following useful facts about $d$. The first, 
\begin{equation}\label{SubspaceMetricSameDim}
\dim(W) = \dim(V) \implies \sup_{\substack{v \in V \\ \|v\| = 1}}  d(v,W)  = \sup_{\substack{w \in W \\ \|w\| = 1}} d(w,V),
\end{equation}
%
is proven in \cite{Morris10} (Lemma 3.3). The second is:
\begin{lemma}\label{MinDimLemma}
Let $V, W$ be subspaces of $\mathbb{R}^{m}$. Then
\begin{equation}\label{MinDim}
d(v,W) < \|v\|_2 \indent \forall v \in V \implies \dim(V) \leq \dim(W).
\end{equation}
\end{lemma}

\begin{proof}
If $\dim(V) > \dim(W)$ then there exists some $v' \in V \cap W^\perp$ and by Pythagoras' Theorem, $\|v' - w\|_2^2 = \|v'\|_2^2 + \|w\|_2^2 \geq \|v\|_2^2$ for all $w \in W$. This is in contradiction with the LHS of \eqref{MinDim}, which states that for all $v \in V$ there exists some $w \in W$ such that $\|v - w\|_2 < \|v\|_2$.
\end{proof}

We are now in a position to state our main result from combinatorial matrix theory.

%===========          MAIN LEMMA (K > 1)             =================

\begin{lemma}[Main Lemma]\label{MainLemma}
Fix positive integers $n, m$ and $k$ such that $k < m$. Let $A, B \in \mathbb{R}^{n \times m}$ and suppose that $A$ satisfies the spark condition \eqref{SparkCondition} with unit norm columns. If there exists for some permutation $\sigma \in \Sigma_m$ a map $\pi: \{S_{\sigma}(i), \ldots, S_{\sigma}(m-1)\} \to {[m] \choose k}$ and some $\Delta < \frac{\alpha_{2k}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound}
\Theta(\text{Span}\{A_{S_{\sigma}(i)}\}, \text{Span}\{B_{\pi(S_{\sigma}(i))}\}) \leq \frac{ \phi_k(A) }{k} \Delta \indent \text{for all} \indent i \in [m]
\end{equation}
%
then there exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that
\begin{align}
\|A - BPD\|_2 \leq \Delta
\end{align}
\end{lemma}

We defer the proof of this lemma to the Appendix. 

%========          PROOF OF THEOREM 1        ============

\begin{proof}[Proof of Theorem \ref{DeterministicUniquenessTheorem}]
We assume $k > 1$ (the $k=1$ case was proven for $N=m$ at the beginning of this section). First, we produce a set of $N = mk{m \choose k}$ vectors in $\mathbb{R}^k$ in general linear position (i.e. any set of $k$ of them are linearly independent). Specifically, let $\gamma_1, ..., \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ "Vandermonde" sub-determinant is nonzero). Next, fix some $\sigma \in \Sigma_m$ and form the $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with supports $S_\sigma(j)$ for $j \in [m]$ (partitioning the $a_i$ evenly among these supports, i.e. for each support $S_\sigma(j)$ there are $k{m \choose k}$ vectors $a_i$ with that support) by setting the nonzero values of vector $\mathbf{a}_i$ to be those contained in the $i$th column of $V$. Note that by construction every $k$ vectors $a_i$ are linearly independent. 

We will show how the existence of these $\mathbf{a}_i$ proves the theorem. First, we will show that they satisfy property \eqref{DataSpread}. Consider the compact set $\mathcal{C} = \{c \in \mathbb{R}^k: \|c\|_2 = 1\}$ and the continuous map
\begin{align*}
\phi: \mathcal{C} &\to \mathbb{R} \\
(c_1, ..., c_k) &\mapsto \|\sum_{j = 1}^k c_j \mathbf{a}_{i_j}\|_2.
\end{align*}

By general linear position of the $\mathbf{a}_i$, we know that $0 \notin \phi(\mathcal{C})$. Since $\mathcal{C}$ is compact, we have by continuity of $\phi$ that $\phi(\mathcal{C})$ is also compact; hence it is closed and bounded. Therefore $0$ can't be a limit point of $\phi(\mathcal{C})$ and there must be some $\rho > 0$ such that the neighbourhood $\{x: x < \rho\} \subseteq \mathbb{R} \setminus \phi(\mathcal{C})$. Hence $\phi(c) \geq \rho$ for all $c \in \mathcal{C}$. The property \eqref{DataSpread} follows by the association $c \mapsto \frac{c}{\|c\|_2}$ and the fact that there are only finitely many subsets of $k$ vectors $\mathbf{a}_i$ (actually, for our purposes we need only consider those subsets of $k$ vectors $\mathbf{a}_i$ having the same support), hence there is some minimal $\rho$ satisfying \eqref{DataSpread} for all of them. (We refer the reader to the Appendix for a lower bound on $\rho$ given as a function of $k$ and an arithmetic sequence $\gamma_1, \ldots, \gamma_N$ used to generate the $a_i$.)

Let $A \in \mathbb{R}^{n \times m}$ be some matrix with unit norm columns satisfying \eqref{RIP} and suppose that for some $B \in \mathbb{R}^{n \times m}$ there exist $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ such that $\|A\mathbf{a}_i - B\mathbf{b}_i\| \leq \varepsilon$ for all $i \in \{1, \ldots, N\}$. Since there are $k{m \choose k}$ vectors $\mathbf{a}_i$ with a given support $S$, the pigeon-hole principle implies that there is some set of indices $\mathcal{I}$ of cardinality at least $k$ such that all $\mathbf{a}_i$ and $\mathbf{b}_i$ with $i \in \mathcal{I}$ have supports $S$ and $S' \in {[m] \choose k}$, respectively. \textbf{[Do we really need this many samples per support? Consider the information gained by noting that we have vectors distributed over multiple supports and that $A$ satisfies the spark condition.]}

It follows from the general linear position of the $\mathbf{a}_i$ and the fact that every $k$ columns of $A$ are linearly independent that the set $\{A\mathbf{a}_i: i \in \mathcal{I}\}$ is a basis for $\text{Span}\{A_S\}$. Hence, fixing $\mathbf{z} \in \text{Span}\{A_S\}$, there exists a unique set of $c_i \in \mathbb{R}$ (for notational convenience we index these $c_i$ with $\mathcal{I}$ as well) such that $\mathbf{z} = \sum_{i \in \mathcal{I}} c_iA\mathbf{a}_i$. Letting $\mathbf{z'} = \sum_{i \in \mathcal{I}} c_iB\mathbf{b}_i \in \text{Span}\{B_{S'}\}$, we have by the triangle inequality that
\begin{align}\label{4}
\|\mathbf{z} - \mathbf{z'}\|_2
\leq \sum_{i \in \mathcal{I}} |c_i| \| A\mathbf{a}_i - B\mathbf{b}_i \|_2 \leq \varepsilon \sum_{i \in \mathcal{I}} |c_i|.
\end{align}

Since $\text{supp}(\mathbf{a}_i) = S$ for all $i \in \mathcal{I}$ and $A$ satisfies the spark condition, we have:
\begin{align}\label{len}
\|\mathbf{z}\|_2 = \|\sum_{i \in \mathcal{I}}^k c_i A \mathbf{a}_i\|_2 
= \|A (\sum_{i \in \mathcal{I}} c_i \mathbf{a}_i) \|_2 
\geq \alpha_{k}(A) \|\sum_{i \in \mathcal{I}}^k c_i \mathbf{a}_i\|_2 
\geq \frac{\alpha_{k}(A)\rho}{\sqrt{k}}\sum_{i \in \mathcal{I}}^k |c_i|,
\end{align}
%
where for the last inequality we have applied the property \eqref{DataSpread}, noting that $\sqrt{k}\|c\|_2 \geq \|c\|_1$ for all $c \in \mathbb{R}^k$. Combining \eqref{4} and \eqref{len}, we see that for all $\mathbf{z} \in \text{Span}\{A_S\}$ there exists some $\mathbf{z}' \in \text{Span}\{B_{S'}\}$ such that $\|\mathbf{z} - \mathbf{z}'\|_2 \leq \frac{ \varepsilon \sqrt{k}}{ \alpha_{k}(A) \rho } \|\mathbf{z}\|_2$. It follows that $d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \frac{ \varepsilon \sqrt{k} }{ \alpha_{k}(A) \rho }$ for all unit vectors $\mathbf{z} \in \text{Span}\{A_S\}$. Hence,
\begin{align}\label{ABSubspaceDistance}
\sup_{ \substack{ \mathbf{z} \in \text{Span}\{A_{S}\} \\ \|\mathbf{z}\| = 1} } d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \frac{ \varepsilon \sqrt{k} }{ \alpha_{k}(A) \rho }.
\end{align}

If $\varepsilon < \frac{\alpha\rho}{\sqrt{k}}$, we have that $\dim(\text{Span}\{B_{S'}\}) \geq \dim(\text{Span}\{A_S\}) = k$ by Lemma \ref{MinDimLemma} and the fact that every $k$ columns of $A$ are linearly independent . In fact, since $|S'| = k$, we have $\dim(\text{Span}\{B_{S'}\}) = \dim(\text{Span}\{A_S\})$. Recalling \eqref{SubspaceMetricSameDim},  we see the association $S \mapsto S'$ thus defines a map $\pi: \{S_1, \ldots, S_m\} \to {[m] \choose k}$ satisfying $\Theta(\text{Span}\{A_S\}, \text{Span}\{\mathcal{B_{\pi(S)}}\}) \leq \frac{ \varepsilon \sqrt{k} }{ \alpha_{k}(A) \rho }$.

Now suppose $\varepsilon < \frac{[\alpha_{k}(A)]^2\phi_k(A)\rho}{\sqrt{k}}$. In this case, we indeed have $\varepsilon < \frac{\alpha_{k}(A)\rho}{\sqrt{k}}$ (since $\alpha \leq 1$, $\phi_k(A) \leq 1$ and $k \geq 1$) so that 
\begin{align}\label{SubspaceDistanceUpperBound}
\Theta(\text{Span}\{A_{S_i}\}, \text{Span}\{\mathcal{B}_{\pi(S_i)}\}) \leq \frac{\phi_k(A)}{k}\Delta
\indent \text{for all} \indent i = 0, \ldots, m-1,
\end{align}
%
where $\Delta = \frac{\varepsilon k^{2/3}}{\alpha_{k}(A)\phi_k(A)\rho} < \frac{\alpha_{k}(A)}{\sqrt{2}}$. It follows by Lemma \ref{MainLemma} that there exists a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that for all $i \in \{1, \ldots, m\}$,
$\|A - BPD\|_2 \leq C\varepsilon$ for $C = \frac{k^{2/3}}{\alpha_{k}(A)\phi_k(A)\rho}$.
\end{proof}


%===================================
% DIFFERENT CODING DIMENSIONS
%===================================

\section{Assuming the Spark Condition on $B$}\label{mleqm}

In this section we state a version of Theorem \ref{DeterministicUniquenessTheorem} and Lemma \ref{MainLemma} assuming that $B$ also satisfies the spark condition (in addition to $A$ satisfying the spark condition). In doing so, we can address the issue of recovering $A \in \mathbb{R}^{n \times m}$ and the $\mathbf{a}_i \in \mathbb{R}^m$ when only an upper bound $m'$ on the number $m$ of sparse sources is known.

\begin{theorem}\label{DeterministicUniquenessTheorem2}
Given positive integers $n, m, m'$ and $k$ with $k < m \leq m'$, there exist $N =  mk{m' \choose k}$ $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every pair of matrices $A, B \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} are such that if $\| A\mathbf{a}_i - B\mathbf{b}_i \|_2 \leq \varepsilon$ for all $i = 1, \ldots, N$ for some $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^{m'}$ and small enough $\varepsilon$ then $\|A - BPD\| \leq C\varepsilon$ for some $C > 0$, partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and diagonal matrix $D \in \mathbb{R}^{m' \times m}$ and $\|\mathbf{b}_i - PD\mathbf{a}_i\|_1 \leq \frac{\varepsilon \|D\|_2}{\alpha}\left( 1 + C\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right)$ where $\alpha = \min( \alpha_{2k}(A), \alpha_{2k}(B))$.


Specifically, the $\mathbf{a}_i$ are constructed so that any $k$ of them sharing the same support satisfy \eqref{DataSpread} for some $\rho > 0$. Letting $\Phi := \min(\phi_k(A), \phi_k(B))$ and letting $\alpha_k := \min( \alpha_k(A), \alpha_k(B) )$ and $\alpha_{2k} := \min( \alpha_{2k}(A), \alpha_{2k}(B) )$, then provided $\varepsilon < (2k)^{-2/3}\alpha_k^2\rho \Phi$ we have
\begin{align}
\|A - BPD\|_2 \leq \frac{2\varepsilon k^{2/3} }{\alpha_k\rho \Phi}
\end{align}
%
and 
\begin{align}\label{b_PDa}
\|\mathbf{b}_i - PD\mathbf{a}_i\|_1 \leq \frac{\varepsilon}{\alpha_{2k}\|D\|_2}  \left( 1 + \frac{2 k^{2/3}}{\alpha_k\rho \phi_k(A)}\|D^TP^T\mathbf{b}_i\|_1 \right).
\end{align}
\end{theorem}

The proof of Theorem \ref{DeterministicUniquenessTheorem2} is very similar to the proof of Theorem \ref{DeterministicUniquenessTheorem}, the only difference being that now we establish a map $\pi: [m] \to [m']$ satisfying the requirements of Lemma \ref{MainLemma2}, the statement of which follows, by pigeonholing $k{m' \choose k}$ vectors with respect to $[m']$ holes.

\begin{lemma}[Main Lemma for $m \leq m'$]\label{MainLemma2}
Fix positive integers $n, m, m'$ and $k$ with $k < m \leq m'$. Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfy spark condition \eqref{SparkCondition}, with $A$ having unit norm columns. Let $\alpha_{2k} = \min(\alpha_{2k}(A), \alpha_{2k}(B))$.  If there exists for some permutation $\sigma \in \Sigma_m$ a map $\pi: \{S_{\sigma}(i), \ldots, S_{\sigma}(m-1)\} \to {[m'] \choose k}$ and some $\Delta < \frac{\alpha_{2k}}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound2}
\Theta(\text{Span}\{A_{S_\sigma(i)}\}, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{ \min(\phi_k(A), \phi_k(B)) }{k} \Delta \indent \text{for all} \indent i \in [m]
\end{equation}
%
then there exist a partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and a diagonal matrix $D \in \mathbb{R}^{m' \times m}$ such that
\begin{align}
\|A - BPD\|_2 \leq \Delta \indent \text{for all} \indent i \in [m].
\end{align}
\end{lemma}

We again defer the proof of this lemma to the Appendix.

%===================================
% 			DISCUSSION
%===================================

\section{Discussion}

The theory of CS informs also informs another practical consequence of our result. Since our derived sample complexity is independent of the ambient dimension of the data, $n$, given a lower bound on the sparsity of the latent variables $\mathbf{a}_i$ we can generate a random matrix to compress the data to a dimension in the regime of \eqref{CScondition} before applying a dictionary learning. This could significantly reduce the computational cost of dictionary learning when the sparsity is high by reducing the number of parameters required to define each dictionary element. Such improvements are crucial to scaling up dictionary learning to larger datasets. \textbf{[Is this actually a significant computational boost..?]}

Uniqueness in data analysis.

Identifying uniqueness: the bispectrum

%========================================
%      	APPENDIX: COMBINATORICS
%========================================

\appendices
\section{Combinatorial Matrix Theory}

In this section, we prove Lemma \ref{MainLemma}, which was a main ingredient in our proof of Theorem \ref{DeterministicUniquenessTheorem}. We suspect that there is an appropriate generalization to matroids. For readers willing to assume a priori that the spark condition holds for $B$ as well as for $A$, a proof of this case (Lemma \ref{MainLemma2} from section \ref{mleqm}) is provided in Appendix \ref{mleqmAppendix}. The additional assumption simplifies the proof and allows us to extend stability conditions to the case where only an upper bound on $m$ is known a priori. We now prove some auxiliary lemmas before deriving Lemma \ref{MainLemma}.

%===== SPAN INTERSECTION LEMMA =====

\begin{lemma}\label{SpanIntersectionLemma}
Let $M \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $M$ are linearly independent, then for any $\mathcal{T} \subseteq \bigcup_{\ell \leq k} {[m] \choose \ell}$,
\begin{align}
y \in \text{Span}\{M_{\cap \mathcal{T}}\}  \Longleftrightarrow y \in \bigcap_{S \in \mathcal{T}} \text{Span}\{M_S\}.
\end{align}
\end{lemma}

\begin{proof}The forward direction is trivial; we prove the reverse direction. Enumerate $\mathcal{T} = (S_1, \ldots, S_{|\mathcal{T}|})$ and let $\mathbf{y} \in \text{Span}\{M_{S_1}\} \cap \text{Span}\{M_{S_2}\}$. Then there exists some $\mathbf{x}_1$ with support contained in $S_1$ such that $\mathbf{y} = M\mathbf{x}_1$ and some $\mathbf{x}_2$ with support contained in $S_2$ such that $\mathbf{y} = M\mathbf{x}_2$. We therefore have $M(\mathbf{x}_1 - \mathbf{x}_2) = 0$, which implies that $\mathbf{x}_1 = \mathbf{x}_2$ by the spark condition. Hence $\mathbf{x}_1$ and $\mathbf{x}_2$ have the same support contained in both $S_1$ and $S_2$, i.e. $\mathbf{y} \in \text{Span}\{M_{S_1 \cap S_2}\}$. This carries over by induction to the entire sequences of supports in $\mathcal{T}$. 
\end{proof}

%===== DISTANCE TO INTERSECTION LEMMA =====

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be closed subspaces of $\mathbb{R}^m$ and let $V = \cap_{i=1}^k V_i$. For every $x \in \mathbb{R}^m$,
\begin{align}\label{DTILeq}
\|x - \Pi_V x\|_2 \leq \frac{1}{1 - \xi(V_1, \ldots, V_k)} \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2.
\end{align}
\end{lemma}
%
where the expression for $\xi$ is given in Definition \ref{SpecialSupportSet}.

\begin{proof} 
Fix $x \in \mathbb{R}^m$ and $k \geq 2$. The proof can be subdivided into two steps. First, we will show that
\begin{align}\label{induction}
\|x - \Pi_Vx\|_2 \leq \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2 + \|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} x - \Pi_V x\|_2.
\end{align}
%
We have by the triangle inequality that
\begin{align*}
\|x - \Pi_Vx\|_2 &= \|x - \Pi_{V_k} x\|_2 + \|\Pi_{V_k}(I - \Pi_{V_{k-1}}) x\|_2 + \|\Pi_{V_k}\Pi_{V_{k-1}}x - \Pi_Vx\|_2 \\
&\leq \sum_{i=k-1}^k\|x - \Pi_{V_i} x\|_2 + \|\Pi_{V_k}\Pi_{V_{k-1}} x - \Pi_V x\|_2,
\end{align*}
%
where in the second line we have used the fact that $\|\Pi_{V_k}\|_2 \leq 1$. If $k=2$ then we are done; otherwise, we may repeat this manipulation another $k-2$ times until we arrive at \eqref{induction}. Next, we show how the result \eqref{DTILeq} follows from \eqref{induction}. To do so, we make use of the following result from \cite{Deutsch} (Theorem 9.33):
\begin{align}\label{dti2}
\|\left(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}\right)x - \Pi_Vx\|_2 \leq \xi \|x\|_2 \indent \text{for all} \indent x \in \mathbb{R}^m.
\end{align}
%
Recall the definition of $\xi$ and note that
\begin{align}\label{dti1}
\|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)\|_2 
&= \|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}) x - \Pi_V x \|_2,
\end{align}
%
since $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$.
%
We therefore have by \eqref{dti2} and \eqref{dti1} that
\begin{align*}
\|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}) x - \Pi_V x \|_2
&= \|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)\|_2 \\
&\leq \xi \|x - \Pi_Vx\|_2.
\end{align*}

Substituting the LHS into \eqref{induction}, we get
\begin{align*}
\|x - \Pi_Vx\|_2 \leq \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2 + \xi \|x - \Pi_Vx\|_2,
\end{align*}
%
from which the result follows by rearranging for $\|x - \Pi_Vx\|_2$.
\end{proof}

%======= GRAPH THEORY LEMMA =======

\begin{lemma}\label{NonEmptyLemma} Fix positive integers $k < m$ and $\sigma \in \Sigma_m$. Suppose there exists a map $\pi: \{S_\sigma(1), \ldots, S_\sigma(m)\} \to {\mathbb{Z}/m\mathbb{Z} \choose k}$ such that for all $\mathcal{I} \in {[m] \choose k}$,
\begin{align}\label{EmptyToEmpty}
 \bigcap_{i \in \mathcal{I}} S_\sigma(i) = \emptyset \Longrightarrow \bigcap_{i \in \mathcal{I}} \pi(S_\sigma(i)) = \emptyset.
\end{align}
%
Then  $\pi(S_\sigma(i)) \cap \cdots \cap \pi(S_\sigma(i+(k-1))) \neq \emptyset$ for all $i \in \mathbb{Z}/m\mathbb{Z}$.
\end{lemma}

\begin{proof} Consider the set $Q_m = \{ (i,j) : i \in \mathbb{Z}/m\mathbb{Z}, j \in \pi(S_\sigma(i)) \}$, which has $mk$ elements. By the pigeon-hole principle, there is some $p \in \mathbb{Z}/m\mathbb{Z}$ and at least $k$ distinct $i_1, \ldots, i_k$ such that $\{(i_1, p), \ldots, (i_k, p)\} \subseteq Q_m$. Hence, $p \in \pi(S_\sigma(i_1)) \cap \cdots \cap \pi(S_\sigma(i_k))$ and by \eqref{EmptyToEmpty} there must be some $v \in \mathbb{Z}/m\mathbb{Z}$ such that $v \in S_\sigma(i_1) \cap \cdots \cap S_\sigma(i_k)$. This is only possible (given $\mathcal{T}$) if $i_1, \ldots, i_k$ are consecutive modulo $\mathbb{Z}/m\mathbb{Z}$, i.e. $\{i_1, \ldots, i_k\} = \{v - (k-1), \ldots, v\}$. (Given an element in $\mathbb{Z}/m\mathbb{Z}$, there are only $k$ elements of $\mathcal{T}$ which contain that element; these are $S_\sigma(v-(k-1)), \ldots, S_\sigma(v)$.)

We now show that there exists no additional $i \notin \{v - (k-1), \ldots, v\}$ such that $p \in \pi(S_\sigma(i))$. This would complete the proof as follows: by letting $Q_{m-1} \subset Q_m$ be the set of elements of $Q_m$ not having $p$ as a second coordinate, we would have $|Q_{m-1}| = (m-1)k$ and the result follows by iterating the arguments above. To see why there can be no such $i$, note that if this were not the case then we would have $p \in \pi(S_\sigma(i)) \cap \pi(S_\sigma(v - k+1)) \cap \cdots \cap \pi(S_\sigma(v))$ and \eqref{EmptyToEmpty} would imply that every $k$-element subset of $\{i\} \cup \{v-(k-1), \ldots, v\}$ is a set of consecutive integers modulo $m$. This is only possible if $k = m-1$, i.e. if $\{i\} \cup \{v-(k-1), \ldots, v\} = \mathbb{Z}/m\mathbb{Z}$; but then there cannot have been $m$ distinct elements of ${\mathbb{Z}/m\mathbb{Z} \choose m-1}$ all containing $p$ since there are only ${m-1 \choose m-2}  = m-1$ such elements.
\end{proof}

%==== PROOF OF MAIN LEMMA =======

\begin{proof}[Proof of Lemma \ref{MainLemma} (Main Lemma)]
We assume $k \geq 2$ since the case $k = 1$ was proven at the beginning of Section \ref{DUT}. We begin by proving that $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$ for all $i \in [m]$. Fix $i \in [m]$ and note that by \eqref{GapUpperBound} we have for all unit vectors $u \in \text{Span}\{A_{S_\sigma(i)}\}$ that $d(u, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{\phi_k(A)}{k} \Delta$. Note also that $\frac{\phi_k(A)}{k} \Delta < 1$; hence by Lemma \ref{MinDimLemma} we have $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) \geq \dim(\text{Span}\{A_{S_\sigma(i)}\}) = k$. Since $|\pi(S_\sigma(i))| = k$ we in fact have $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$, i.e. the columns of $B_{\pi(S_\sigma(i))}$ are linearly independent. 

We will now show that
\begin{align}\label{fact2}
|\bigcap_{i \in \mathcal{I}} \pi(S_\sigma(i))| \leq |\bigcap_{i \in \mathcal{I}} S_\sigma(i) | \indent \text{for all} \indent \mathcal{I} \in {[m] \choose k}.
\end{align}

Fix $\mathcal{I} = \{i_1, \ldots, i_k\} \in {[m] \choose k}$. Assumption \eqref{GapUpperBound} implies that for all unit vectors $\mathbf{u} \in \cap_{i \in \mathcal{I}} \text{Span}\{B_{\pi(S_\sigma(i))}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(i)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $i \in \mathcal{I}$. By Lemma \ref{DistanceToIntersectionLemma}, we have:
\begin{align*}
d\left( \mathbf{u}, \bigcap_{i \in \mathcal{I}} \text{Span}\{A_{S_{\sigma}(i)}\} \right) 
\leq \Delta \left( \frac{\phi_k(A)}{1 - \xi(\text{Span}\{A_{S_{\sigma}(i_1)}\}, \ldots, \text{Span}\{A_{S_\sigma(i_k)}\})} \right) \leq \Delta,
\end{align*}
%
where the second inequality follows immediately from the definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_i)}\} \subseteq \cap_{i \in \mathcal{I}} \text{Span}\{B_{\pi(S_i)}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in \mathcal{I}}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\cap_{i \in \mathcal{I}}  S_\sigma(i)}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\cap_{i \in \mathcal{I}} S_\sigma(i)}\} \right) \leq \Delta \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_\sigma(i))}\})$ and $W = \text{Span}\{A_{\cap_{i \in \mathcal{I}} S_\sigma(i)}\}$, that $\dim(V) \leq \dim(W)$ and \eqref{fact2} follows by the linear independence of the columns of $A_{S_\sigma(i)}$ and $B_{\pi(S_\sigma(i))}$ for all $i \in [m]$.

Suppose now that $\mathcal{I} = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in \mathcal{I}} S_\sigma(i) = \{\sigma(i)\}$. By \eqref{fact2} we have that $\cap_{i \in \mathcal{I}} \pi(S_\sigma(i))$ is either empty or it contains a single element. Lemma \ref{NonEmptyLemma} ensures that the latter case is the only possibility. Thus the association $\sigma(i) \mapsto \cap_{i \in \mathcal{I}} \pi(S_\sigma(i))$ defines a map $\hat \pi: [m] \to [m]$. Recalling \eqref{SubspaceMetricSameDim}, it follows from \eqref{fact1} that for all unit vectors $\mathbf{u} \in \text{Span}\{A_{i}\}$ we have $d\left( \mathbf{u}, \text{Span}\{B_{\hat \pi(i)}\}\right) \leq \Delta$ also, i.e. for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, there exists some $c_i \in \mathbb{R}$ such that $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\hat \pi(i)}\| \leq \Delta$ where $\Delta < \frac{\alpha_{2k}(A)}{\sqrt{2}}$. This is exactly the supposition in \eqref{1D} and the result follows from the subsequent arguments of Section \ref{DUT}. 
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}

%================================
% APPENDIX: ADAPTED MAIN LEMMA
%================================

\section{Assuming the Spark Condition on $B$}\label{mleqmAppendix}

In this section we prove Lemma \ref{MainLemma2} from Section \ref{mleqm}.

%=== PROOF OF ADAPTED MAIN LEMMA ===

\begin{proof}
We begin with the case $k=1$ and then show how the case for $k>1$ reduces to this. Since $\phi_1(A) = \phi_1(B) = 1$, equation \eqref{GapUpperBound2} asserts that for all canonical basis vectors $\mathbf{e}_i \in \mathbb{R}^m$ there is some $c_i \in \mathbb{R}$ such that 
\begin{align}\label{yepyep}
\|A\mathbf{e}_i - c_iB\mathbf{e}_{\pi(i)}\|_2 \leq \Delta \indent \text{for all} \indent i \in [m].
\end{align}
Note that if $c_i = 0$ for some $i$, then $\|A\mathbf{e}_i\| < 1$ (since $\alpha \leq 1$), contradicting the fact that $A$ has unit norm columns. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m']$. Then $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2  \leq \Delta$ and $\|A\mathbf{e}_j - c_jB\mathbf{e}_{\ell}\|_2 \leq \Delta$. Summing and scaling these two inequalities by $|c_j|$ and $|c_i|$, respectively, the triangle inequality and \eqref{RIP} yields
\begin{align*}
(|c_i| + |c_j|) 2\varepsilon
&\geq |c_j|\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2 + |c_i|\|c_jB\mathbf{e}_{\ell} - A\mathbf{e}_j\|_2 \\
&\geq \|c_jAe_i + c_iAe_j\|_2 \\
&\geq \alpha\|c_je_i + c_ie_j\|_2
\end{align*}
%
which is in contradiction with the fact that $\|x\|_1 \leq \sqrt{2}\|x\|_2$ for all $x \in \mathbb{R}^2$ and $\Delta < \frac{\alpha}{\sqrt{2}}$. Hence, $\pi$ is injective. Letting $P$ and $D$ denote the following partial permutation and diagonal matrices, respectively:
\begin{equation}\label{PandD}
P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}, \mathbf{0}, \cdots, \mathbf{0} \right), \ \ D = \left(\begin{array}{ccc}c_1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & c_m \\ 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0
\end{array}\right),
\end{equation}
%
we see that \eqref{yepyep} becomes $\|(A - BPD)\mathbf{e}_i\|_2 \leq \Delta$ for all $i \in [m]$.

Now suppose $k \geq 2$ and for some fixed $i \in [m]$ let $\mathcal{I} = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in \mathcal{I}} S_\sigma(i) = \sigma(i)$. Assumption \eqref{GapUpperBound} implies that for all unit vectors $\mathbf{u} \in \cap_{j \in \mathcal{I}} \text{Span}\{B_{\pi(S_\sigma(j))}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(j)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $j \in \mathcal{I}$. By Lemma \ref{DistanceToIntersectionLemma} we have that:
\begin{align}\label{sym2}
d\left( \mathbf{u}, \bigcap_{j \in \mathcal{I}} \text{Span}\{A_{S_{\sigma}(j)}\} \right) 
\leq \Delta \left( \frac{\phi_k(A)}{1 - \xi(\text{Span}\{A_{S_{\sigma}(i-k+1)}\}, \ldots, \text{Span}\{A_{S_\sigma(i)}\})} \right) \leq \Delta,
\end{align}
%
where the second inequality is a simple consequence of the definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_i)}\} \subseteq \cap_{i \in \mathcal{I}} \text{Span}\{B_{\pi(S_i)}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in \mathcal{I}}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\sigma(i)}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\sigma(i)}\} \right) \leq \Delta \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in \mathcal{I}}\pi(S_\sigma(i))}\})$ and $W = \text{Span}\{A_{\sigma(i)}\}$, that $\dim(V) \leq \dim(W)$. By symmetry, \eqref{fact1} can also be proven with the roles of $A$ and $B$ reversed. Hence, $\dim(V) = \dim(W) = 1$ and it follows that $|\cap_{i \in \mathcal{I}} \pi(S_\sigma(i))| = 1$ since, by the spark condition, the columns of $B_{\cap_{i \in \mathcal{I}} \pi(S_\sigma(i))}$ are linearly independent. Thus the association $\sigma(i) \mapsto \cap_{i \in \mathcal{I}} \pi(S_\sigma(i))$ defines a map $\pi: [m] \to [m']$ such that $d\left( \mathbf{u}, \text{Span}\{B_{ \pi(S_\sigma(i))}\}\right) \leq \Delta$ for all $i \in [m]$, i.e. for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, there exists some $c_i \in \mathbb{R}$ such that $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\hat \pi(i)}\| \leq \Delta$ where $\Delta < \frac{\alpha_{2k}(A)}{\sqrt{2}}$. This is exactly the supposition in \eqref{yepyep}.
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}


%===================================
% 		APPENDIX: CALCULATING C
%===================================

\section{Calculating C}

In this section we demonstrate how an upper bound on the constant $C$ can be derived for a particular $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$. \textbf{[Alternatively, see Lemma 2.2 in Grcar - A matrix lower bound, for a lower bound on rectangular matrices (though not k-restricted)]}

%=== MATRIX LOWER BOUND LEMMA ===

\begin{lemma}\label{MatrixLowerBoundLemma}
Let $\gamma_1 < ... < \gamma_N$ be any distinct numbers such that $\gamma_{i+1} = \gamma_i + \delta$ and form the $k \times N$ Vandermonde matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$. Then for all $S \in {[N] \choose k}$, 
\begin{align}
	\|V_S x\|_2 > \rho \|x\|_1 \indent \text{where} \indent \rho = \frac{\delta^k}{\sqrt{k}} \left( \frac{k-1}{k} \right)^\frac{k-1}{2} \prod_{i = 1}^k (\gamma_1 + (i-1)\delta) \indent \text{for all} \indent x \in \mathbb{R}^k
\end{align}
\end{lemma}

\begin{proof} 
The determinant of the Vandermonde matrix is
\begin{align}
	\det(V) = \prod_{1 \leq j \leq k} \gamma_j \prod_{1 \leq i \leq j \leq k} (\gamma_j - \gamma_i) \geq \delta^k \prod_{i = 1}^k (\gamma_1 + (i-1)\delta).
\end{align}	
Since the $\gamma_i$ are distinct, the determinant of any $k \times k$ submatrix of $V$ is nonzero; hence $V_S$ is nonsingular for all $S \in {[N] \choose k}$. Suppose $x \in \mathbb{R}^k$. Then $\|x\|_2 = \|V_S^{-1} V_S x\|_2 \leq \|V_S^{-1}\| \|V_S x\|_2$, implying $\|V_Sx\|_2 \geq \|V_S^{-1}\|^{-1}\|x\|_2 \geq \frac{1}{\sqrt{k}} \|V_S\|_2^{-1}\|x\|_1$. For the Euclidean norm we have $\|V_S^{-1}\|_2^{-1} = \sigma_{\min}(V_S)$, where $\sigma_{\min}$ is the smallest singular value of $V_S$. A lower bound for the smallest singular value of a nonsingular matrix $M \in \mathbb{R}^{k \times k}$ is given in [Hong and Pan]:
\begin{align}
	\sigma_{\min}(M) > \left( \frac{k-1}{k} \right)^\frac{k-1}{2} |\det M|
\end{align}
%
and the result follows. 
\end{proof}

%===================================
% 		ACKNOWLEDGEMENT
%===================================

\section*{Acknowledgment}
We would like to thank Fritz Sommer for turning our attention to the dictionary learning problem. We also thank Bizzyskillet on SoundCloud for "The No-Exam Jams", which played on repeat during many long hours of designing proofs. Finally, we thank Ian Morris for posting equation \eqref{SubspaceMetricSameDim} on Stack Exchange and for providing a reference to his proof.

%===================================
% 			REFERENCES
%===================================

\bibliographystyle{IEEEtran}
\bibliography{RobustIdentifiability}

%===================================
% 			BIOGRAPHY
%===================================

\begin{IEEEbiographynophoto}{Charles J. Garfinkle}
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Christopher J. Hillar}
\end{IEEEbiographynophoto}

%===================================
% 			SCRAP PAPER
%===================================

\section*{SCRAP PAPER}

%========================
%            INTRODUCTION
%========================
      
\subsection{Introduction}

We also require the data to satisfy certain properties. Consider the problem where we wish to identify the mixing matrix $A$ from the mixtures $\mathbf{y}_i$ when the sources $\mathbf{a}_i$ are known. In this case, a necessary condition for uniqueness of $A$ given $\mathbf{y}_i = A \mathbf{a}_i + \mathbf{\eta}_i$ (even when $\mathbf{\eta}_i=0$) is:
\begin{align}\label{SparkCondition2}
A^{(1)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N) = A^{(2)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N)  \text{ for all } i \in [n] \implies A^{(1)}  = A^{(2)} \indent \text{for all } A^{(1)}, A^{(2)} \in \mathbb{R}^{n \times m}.
\end{align}

Otherwise, blah blah blah (figure this out). Trying to introduce the "spread" of the data as a necessary condition.

%======================================
% STATEMENTS OF PROBABILISTIC THEOREMS
%======================================

\subsection{Statements of Probabilistic Theorems}\label{PUT}

We next give precise statements of our probabilistic versions of Theorem 1, all of which rely on the following construction.

\begin{definition}[Random Sparse Vectors]\label{RandomDraw}
Given the support set for its $k$ nonzero entries, a random draw of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all of them in ${[m] \choose k}$ and then a random draw.
\end{definition}

\begin{definition}[Special Random Sparse Vectors]\label{SpecialRandomDraw}
Given the support set for its $k$ nonzero entries, a random draw of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all $m$ of them in $\mathcal{T}$ and then a random draw.
\end{definition}

\begin{theorem}\label{Theorem2}
Fix positive integers $k < m$ and $n$, and a generation matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition}. If $(k+1)$ $k$-sparse $\mathbf{a}_i$ are randomly drawn from each support set in $\mathcal{T}$, then $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, \ldots ,  A\mathbf{a}_N + \mathbf{\eta}_N \}$ for $\|\mathbf{\eta}_i\| \leq \varepsilon$ has a unique sparse coding with probability one, provided $\varepsilon$ is small enough.
\end{theorem}

The following is a direct application of this result, partially answering one of our questions from the introduction.

\begin{corollary}
Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random \textbf{[insert footnote explaining 'random']} $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing   such an $A$, for $\varepsilon$ small enough we have with probability one that a dataset $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, . . . , A\mathbf{a}_N + \mathbf{\eta}_1 \}$ for $\|\mathbf{\eta}_i\| \leq \varepsilon$ generated from $N = (k + 1)m$ $k$-sparse samples $\mathbf{a}_i$ uniquely prescribes $A$ and these sparse vectors $\mathbf{a}_i$ up to an approximate fixed permutation and scaling.
\end{corollary}

We now state our third theorem. Note that an \emph{algebraic set} is a solution to a finite set of polynomial equations. 

\begin{theorem}\label{Theorem3}
Fix positive integers $k < m$ and $n$. If $(k + 1)$ $k$-sparse $\mathbf{a}_i$ are randomly drawn from each support set in $\mathcal{T}$, then with probability one the following holds. There is an algebraic set $Z \subset \mathbb{R}^{n \times m}$ of Lebesgue measure zero with the following property: if $A \notin Z$ then there exists some $\varepsilon$ small enough such that $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, . . . , A\mathbf{a}_N + \mathbf{\eta}_1 \}$ for $\|\mathbf{\eta}_i\| \leq \varepsilon$ has a unique sparse coding.
\end{theorem}

\begin{corollary}
Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}. With probability one, a random draw of $N = (k + 1)m$ $k$-sparse samples $\mathbf{a}_i$ with supports in $\mathcal{T}$ satisfies: for almost every matrix $A$ there is some $\varepsilon$ small enough that gives $Y = \{A\mathbf{a}_1 + \mathbf{\eta}_1, . . . , A\mathbf{a}_N + \mathbf{\eta}_1 \}$ for $\|\mathbf{\eta}_i\| \leq \varepsilon$ a unique sparse coding.
\end{corollary}

%===================================
% PROOFS OF PROBABILISTIC THEOREMS
%===================================

\subsection{Proofs of Probabilistic Theorems}\label{PUTproof}

Definition \ref{RandomDraw}: can I bound the probability of drawing from one of the $m!$ special support sets without explicitly making that restriction?

We first generalize Lemma 3 in \cite{HS11} to the noisy case:

\begin{lemma}
Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{RIP}. With probability one, if $(k+1)$ $k$-sparse vectors $\mathbf{a}_i \in \mathbb{R}^{n \times m}$ are such that $d(A\mathbf{a}_i,V) \leq (??)$ for some $k$-dimensional subspace $V \subset \mathbb{R}^m$ then all of the $(k+1)$ vectors $\mathbf{a}_i$ have the same supports.
\end{lemma}
\begin{proof}
We need only show that the $k+1$ vectors $A\mathbf{a}_i$ are linearly dependent; the rest follows by Lemma 3 from \cite{HS11}. Let these $k+1$ vectors $\mathbf{a}_i$ be indexed by $\mathcal{I}$ and let $W = \text{Span}\{A\mathbf{a}_{i \in \mathcal{I}}\}$. Then for all $w \in W$ we can write $w = \sum_{i \in \mathcal{I}} c_iA\mathbf{a}_i$ for some set of $c_i \in \mathbb{R}$. Letting $v = \sum_{i \in \mathcal{i}} c_iv_i$, it follows that
\[ \|w - v\| = \|\sum_{i \in \mathcal{I}} c_i A\mathbf{a}_i - \sum_{i \in \mathcal{I}} c_i v_i \| 
\leq \sum_{i \in \mathcal{I}} |c_i| \|A\mathbf{a}_i - v_i\| \leq 2\varepsilon \sum_{i \in \mathcal{I}}|c_i| \]

Need RHS less thatn $\|w\|$ to prove that $\dim(W) \leq \dim(V) = k$...
\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2} ($\varepsilon = 0$)]
Consider any alternate factorization $\mathbf{y}_i = B\mathbf{b}_i$ for $Y$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that those $\mathbf{y}$ indexed by $J(S)$ span at most a $k$-dimensional space. By Lemma 3 in \cite{HS11}, either $|J(S)| \leq k$ or with probability one all $\mathbf{a}_j$ with $j \in J(S)$ have the same support $S'$. Since there are only $(k+1)$ vectors $\mathbf{a}_i$ with a given support, the latter case actually implies (with probability one) that $|J(S)| = k+1$. If $N=(k+1){m \choose k}$, though, then with probability one we would reach a contradiction if $|J(S)| \leq k$ for any $S \in {[m] \choose k}$; hence with probability one we have $|J(S)| = k+1$. Can we reach the same conclusion when $N = m(k+1)$? (Perhaps by creating 'virtual data' spanning all supports by those data points with supports in $\mathcal{T}$)?

\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2}]
Let $\mathbf{y}_i = A\mathbf{a}_i + \eta_i$ for all $i \in \{1, \ldots, N\}$ and suppose there is some $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ such that $\|\mathbf{y}_i - B\mathbf{b}_i\| \leq \varepsilon$ for all $i \in \{1, \ldots, N\}$. Then by the triangle inequality we have $\|A\mathbf{a}_i - B\mathbf{b}_i\| \leq 2\varepsilon$ for all $i \in \{1, \ldots, N\}$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that for all $j \in J(S)$ there exists some $v \in \text{Span}\{B_S\}$ such that $\|A\mathbf{a}_i - v\| \leq 2\varepsilon$. By Lemma (??), with probability one, either $|J(S)| \leq k$ for all $\mathbf{a}_j$ with $j \in J(S)$ have the same support.
\end{proof}


\end{document}


