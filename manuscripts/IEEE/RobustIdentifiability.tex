% REFERENCE NOTES
% 'Spark' of a matrix coined and defined in "Optimally  sparse representation in  general  (non-orthogonal)
% dictionaries via L1 minimization" and applied to the study of uniqueness in "Sparse signal reconstruction % from limited data using FOCUSS:   A   re-weighted   norm   minimization   algorithm"

% QUESTIONS

\documentclass[journal, onecolumn]{IEEEtran}

% *** MATH PACKAGES ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Robust Identifiability in Sparse Dictionary Learning}

\author{Charles~J.~Garfinkle,  Christopher~J.~Hillar%
\thanks{The research of Garfinkle and Hillar was conducted while at the Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA; e-mails: cjg@berkeley.edu, chillar@msri.org.  Hillar was supported, in part, by National Science Foundation grants IIS-1219212 and IIS-1219199.}}%

\maketitle

\begin{abstract}
In sparse component analysis (SCA), a dictionary is learned to linearly code a sparse mixture of signals.  It has been previously established  under mild assumptions (e.g., the spark condition from compressed sensing) that such a coding of sparse data is unique up to a relabelling and scaling of dictionary columns and sparse codes.  Often, however, data is only approximately sparse, and noise is present in the generative SCA model.  We extend the known uniqueness results to this more general setting and prove that approximately sparse datasets are still uniquely coded up to the level of noise in the input.  Our results help explain the ubiquitous finding that diverse SCA algorithms determine unique structure when applied to approximately code the same sparse mixtures.
\end{abstract}

\begin{IEEEkeywords}
Bilinear inverse problem, identifiability, dictionary learning, sparse coding, matrix factorization, compressed sensing, combinatorial linear algebra, blind source separation
\end{IEEEkeywords}

%===================================
% 			INTRODUCTION
%===================================

\section{Introduction}
[************** ADD all references and make all references have single letter first name and then last name (no middle initial) *********]
\IEEEPARstart{A}{} number of modern approaches to signal processing model datasets $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ with unknown structure as sparse linear combinations of the columns $\{A_1,\ldots,A_m\}$ of a learned \textit{dictionary} matrix $A \in \mathbb R^{n\times m}$.  Typically, this is accomplished by approximately solving the following constrained optimization problem:
\begin{align}\label{DictionaryLearning}
\arg \min_{A, \, \mathbf{a}_i} \  \sum_{i=1}^N |\mathbf{y}_i - A\mathbf{a}_i|_2, \indent \text{subject to } |\mathbf{a}_i|_0 \leq k.
\end{align}
As the dictionary learning problem \eqref{DictionaryLearning} is NP-hard in general \cite{Tillmann15}, algorithms solving it implement strategies that lead to approximate solutions, typically by replacing the $\ell_0$ pseudo-norm $|\cdot|_0$ (the number of nonzero entries in a vector) with the $\ell_1$-norm $|\cdot|_1$. For a recent comprehensive survey of such techniques, see the review paper \cite{Zhang15}.

Here, rather than study how to obtain such optima, we examine their uniqueness properties. This is of fundamental concern when performing \emph{sparse component analysis} (SCA) \cite{Georgiev05}, wherein the dataset $Y$ is modeled as a sequence of \emph{measurements}:
\begin{align}\label{LinearModel}
\mathbf{y} = A\mathbf{a} + \mathbf{n},
\end{align}
with each \textit{source} $\mathbf{a} \in \mathbb R^m$ containing at most $k$ nonzero entries (such vectors are called \emph{$k$-sparse}).
The vector $\mathbf{n}$ is an error or \textit{noise} term that accounts for both measurement inaccuracy and the degree to which the model deviates from reality (e.g., the true $\mathbf{a}$ may only be approximately sparse).  In short, the goal of SCA is to estimate a set of sparse sources and a dictionary from noisy measurements by approximately solving objective \eqref{DictionaryLearning}.

Even if the uncertainty $\mathbf{n}$ required to explain the data is exactly zero, however, there are still many solutions to the SCA problem. Let $Y$ and $X$ be the matrices with columns $\mathbf{y}_i$ and $\mathbf{a}_i$, respectively, so that $Y = AX$ for $X$ with $k$-sparse columns. Then we also have $Y = BX'$ for $X'$ with $k$-sparse columns whenever $B = AD^{-1}P^{-1}$ and $X' = PDX$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. Thus, the ``uniqueness" of any solution to \eqref{DictionaryLearning} can only be defined up to an equivalence of mappings of the form $PD$, which together constitute an \emph{ambiguity transform group} \cite{BilinInv} associated to this particular bilinear inverse problem. The question of when every solution $Y = BX'$ must necessarily be part of this equivalence class (i.e., $X' = PDX$ and $B = AD^{-1}P^{-1}$ for some $P$, $D$) was addressed in the theoretical works \cite{Georgiev05, Aharon06, Hillar15} and determines whether $A$ and $X$ are identifiable from $Y$ up to this inherent ambiguity.

We provide the following robust extension to the notion of identifiability in sparse dictionary learning.
%Let $\mathbf{e}_1, \ldots, \mathbf{e}_m$ be the standard basis in $\mathbb{R}^m$. 

\begin{definition}\label{Uniqueness}
A dataset $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ has a \textbf{$k$-sparse representation} in $\mathbb{R}^m$ if for some $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$, we have $\mathbf{y}_i = A\mathbf{a}_i$ for $i = 1, \ldots, N$. We say this representation is \textbf{robustly identifiable} if for every $\delta \geq 0$ there exists a nonnegative $\varepsilon = \varepsilon(\delta)$ (with $\varepsilon > 0$ when $\delta > 0$) such that if $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ satisfy:
\[|\mathbf{y}_i - B\mathbf{b}_i|_2 \leq \varepsilon,\indent \text{ for } i = 1, \ldots, N,\]
%
then there necessarily exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ with
\begin{align}\label{def1}
|\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i|_1 \leq \delta \ \ \text{ and } \ \ |A_j - (BPD)_j|_2 \leq \delta,\indent  \text{ for } \ \ i = 1, \ldots, N; \, j = 1, \ldots, m.
\end{align}
\end{definition}

In short, a $k$-sparse representation of a dataset is robustly identifiable when the only other matrices and $k$-sparse vectors that can well-approximate the data are necessarily close to some permutation and scaling of the original codes and dictionary.  

To see how this definition directly applies to approximately sparse datasets, fix $A$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N$ such that $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ is robustly identifiable. Let $\delta > 0$ and suppose that $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ satisfies $|\mathbf{y}_i - A\mathbf{a}_i|_2 \leq \varepsilon/2$ for $i = 1, \ldots, N$, where $\varepsilon$ is as in Def.~\ref{Uniqueness}. % for some $\varepsilon_1 \leq \varepsilon$. 
If it so happens that some alternate dictionary $B$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N$ also give a coding with $|\mathbf{y}_i - B\mathbf{b}_i|_2 \leq \varepsilon/2$ for all $i$, then by the triangle inequality, we necessarily have every such alternate $B$ and $\mathbf{b}_i$ satisfying \eqref{def1} for some $P$ and $D$. 
In such cases, we say that the dataset $Y$ has a \textit{unique $k$-sparse representation} in $\mathbb{R}^m$. 

We now state the central problem addressed in this paper.

\begin{problem}\label{DUTproblem}
Let $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N \} \subset \mathbb{R}^n$ be generated as $\mathbf{y}_i = A\mathbf{a}_i  + \mathbf{n}_i$ for some matrix $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_i \in \mathbb{R}^m$ with bounded noise $\mathbf{n}_i \in \mathbb{R}^n$. When does $Y$ have a unique $k$-sparse representation in $\mathbb{R}^m$?
\end{problem}

Evidently, regardless of noise level, unique recovery of the unknown $k$-sparse vectors $\mathbf{a}_i$ from mixtures $\mathbf{y}_i$ -- even when the measurement matrix $A$ is known -- requires that $A$ be injective on the set of $k$-sparse vectors; that is,
\begin{align}\label{SparkCondition}
A\mathbf{a}_1 = A\mathbf{a}_2 \implies \mathbf{a}_1 = \mathbf{a}_2, \ \ \text{ for all $k$-sparse } \mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^m,
\end{align}
commonly known in the literature as the \emph{spark condition}. Our main contribution here is the result that this necessary condition on $A$ is also enough to guarantee unique recovery of the $\mathbf{a}_i$ when $A$ is itself unknown a priori, provided the $\mathbf{a}_i$ are sufficiently diverse.  

Recall that a \textit{cyclic order} on $[m] := \{1, \ldots,m\}$ is an arrangement of $[m]$ in a circular necklace, and that an \textit{interval} is a contiguous sequence of such elements.  %Our main theorem guaranteeing robust identifiability is as follows.

%=== STATEMENT OF DETERMINISTIC UNIQUENESS THEOREM ===%
\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix positive integers $n, m$, $k < m$, and a cyclic order on $[m]$. If $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in the cyclic order there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ in general linear position supported on that interval, then every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a robustly identifiable $k$-sparse representation.

Specifically, there exist positive constants $C$ and $\varepsilon_0$ such that the following holds. If any matrix $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ are such that $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [N]$, then provided $\varepsilon < \varepsilon_0$, we have
\begin{align}\label{Cstable}
|A_j-(BPD)_j|_2 \leq C\varepsilon,\indent  \text{for all } j \in [m],
\end{align}
and 
\begin{align}\label{b-PDa}
|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i|_1 &\leq \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right),\indent  \text{for all } i \in [N].
\end{align}
\end{theorem}
[**** Chaz says there is something weird about $k=1$ and general linear position here ***]
An important consequence of this result is that for sufficiently small reconstruction error, the original dictionary and sparse codes are determined up to a commensurate error, independent of alternate representation.  (In other words, we may choose $\varepsilon$ on the same order as $\delta$ in Def.~\ref{Uniqueness}.) Formulae for  $C$ and $\varepsilon_0$ in terms of $A$ and the $\mathbf{a}_i$ are provided in \eqref{epsilon0} and \eqref{Cdef}, below. 

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given positive integers $n, m$, and $k < m$, there exist $N =  m(k-1){m \choose k}+m$ vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a robustly identifiable $k$-sparse representation.
\end{corollary}

Our proof of Theorem \ref{DeterministicUniquenessTheorem} is a delicate refinement of the arguments in \cite{Hillar15} to handle measurement error.  Here, we also reduce the theoretically required number of samples given by \cite{Hillar15} from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$, and we provide guarantees for the case when only an upper bound on the number of columns in $A$ is known (see Theorem \ref{DeterministicUniquenessTheorem2} below). 

It is also informative to relate our results to the field of compressed sensing (CS) \cite{candes2006near, donoho2006compressed, candes2006stable}. This theory describes conditions under which a signal $\mathbf{x} \in \mathbb{R}^n$ that is sparse in some known basis (i.e. $\mathbf{s} = \Psi \mathbf{x}$ is $k$-sparse for some invertible $\Psi$) can be feasibly recovered after it has been linearly subsampled as $\mathbf{y} = \Phi \mathbf{x}$ by a known compression matrix $\Phi \in \mathbb{R}^{n \times m}$. The underlying logic is simple: if the generation matrix $A = \Phi\Psi$ satisfies the spark condition \eqref{SparkCondition} then $\mathbf{s}$ is identifiable given $\mathbf{y}$ and the signal $\mathbf{x}$ can then be reconstructed as $\Psi^{-1}\mathbf{s}$. The following key result in CS allows for the construction of such compression matrices using random methods. Provided the dimension $n$ of $\mathbf{y}$ satisfies:
\begin{align}\label{CScondition}
n \geq \alpha k\log\left(\frac{m}{k}\right),
\end{align}
where $\alpha$ is a positive constant, then a ``randomly" generated $A$ satisfies the spark condition with high probability.  But what if we don't know this compression matrix, or the sparsifying basis $A$? Our theorems provide conditions under which $A$ can be robustly identified anyway (given enough samples), even in the presence of noise. 

Given Theorem \ref{DeterministicUniquenessTheorem}, it is also possible to provide probabilistic extensions (Theorems \ref{Theorem2} and \ref{Theorem3}).  To keep our exposition simple, our statements are based upon the following elementary construction of \emph{random sparse vectors} (although many ensembles will suffice, e.g. \cite[Sec.~\S 4]{baraniuk2008simple}).

\begin{definition}[Random $k$-Sparse Vectors]\label{RandomDraw}
Given the support set for its $k$ nonzero entries, a \textbf{random draw} of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all ${m \choose k}$ of them and then a random draw.
\end{definition}

We state the main implications of those results (elaborated upon in Section \ref{PUT} more fully) here. Recall that an \emph{algebraic set} is a solution to a finite set of polynomial equations.

\begin{corollary}
Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing such an $A$, we have with probability one that a dataset $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N\}$ generated from $N = m(k-1){m \choose k}+m$ $k$-sparse samples $\mathbf{a}_i$, consisting of $(k-1){m \choose k}+1$ samples supported on each interval of length $k$ in some cyclic ordering of $[m]$, has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

\begin{corollary}
Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}.  If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval then, with probability one, almost every matrix $A \in \mathbb{R}^{n \times m}$ gives a robustly identifiable $Y = \{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$.
\end{corollary}

It is important to note where this result fits into the field of theoretical dictionary learning. While certain algorithms have been shown to have local converge to a global minimum or global convergence to a local minimum of approximations to \eqref{DictionaryLearning}, to our knowledge there exist no algorithms with guaranteed global convergence to a global solution. Our conditions apply regardless of whichever algorithm is used to solve \eqref{DictionaryLearning} since they are derived from the underlying geometry of the dictionary learning problem, providing for a universal check when any dictionary learning algorithm has converged to (near) a global minimum of \eqref{DictionaryLearning}.

% by determining whether the reconstruction error of the proposed solution is indeed small enough.



We produce such a family of $\mathbf{a}_i$ using ``Vandermonde" matrices in Section \ref{DUT} and calculate explicit numerical bounds on $\varepsilon_0$ and $C$ for such a family in Appendix C. [*** need to finish that...they depend on A too, though... ***] 

% Moreover, the generality of the construction allows us to easily extend the theorem to cases where the $A$ and $\mathbf{a}_i$ are randomly generated.
[************ Check this to make sure these claims are true *******]




[********** Discuss that it would be awesome to find the best dependence of $\varepsilon$ on $\delta$ and best constants and number of samples for this problem **********]

The organization of the rest of this paper is as follows. In Section \ref{Preliminaries} we list additional definitions and key lemmas, including our main tool from combinatorial matrix theory (Lemma \ref{MainLemma}). In Section \ref{DUT} we derive Theorem \ref{DeterministicUniquenessTheorem}. In Section \ref{PUT}, we state and prove probabilistic versions of Theorem \ref{DeterministicUniquenessTheorem}. In Section \ref{mleqm}, we describe how to extend all of these results to the case where we have only an upper bound on the dimensionality of the sparse sources. The final section is a discussion, and an appendix contains the proof of Lemma \ref{MainLemma}.
[ **** Make sure to change this as the structure of the paper evolves ****]


\section{Preliminaries}\label{Preliminaries}
In this section, we review our main tools, which include general notions of angle (Def.~\ref{FriedrichsDefinition}) and distance (Def.~\ref{GapMetricDef}) between vector subspaces as well as a uniqueness result in combinatorial matrix theory (Lemma~\ref{MainLemma}).
Let ${[m] \choose k}$ be the set of subsets of $[m]$ of cardinality $k$ and $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ for real vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$ be the vector space consisting of their $\mathbb{R}$-linear span.
%
%\[ \text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\} = \left\{ \sum_{i=1}^\ell t_i\mathbf{v}_i : t_1, \ldots, t_\ell \in \mathbb{R}\right\}. \]
%
Also, given $S \subseteq [m]$ and $M \in \mathbb{R}^{n \times m}$ with columns $\{M_1,\ldots,M_m\}$, we define $M_S$ to be the matrix with columns $\{M_i: i \in S\}$ and $\text{Span}\{M\} := \text{Span}\{M_1, \ldots, M_m\}$.  The \emph{lower bound} of a matrix $M \in \mathbb{R}^{n \times m}$ \cite{grcar2010matrix} is the largest number for which $|M\mathbf{x}|_2 \geq \alpha|\mathbf{x}|_2$ for all $\mathbf{x} \in \mathbb{R}^m$ and we define the \textit{$k$-restricted lower bound} to be:
\begin{align}
\ell_k(M) := \max \{ \alpha : |M\mathbf{x}|_2 \geq \alpha|\mathbf{x}|_2 \text{ for all $k$-sparse } \mathbf{x} \in \mathbb{R}^m\}.
\end{align}

Clearly, $\ell_{k}(M) \geq \ell_{k'}(M)$ whenever $k < k'$. Note also that since all real matrices have bounded spectral norms, if $\ell_k(M) > 0$ for some $k$ then $M$ satisfies the \emph{restricted isometry property} (RIP) \cite{CandesTao05} from compressive sensing, since there exist $\alpha, \beta > 0$ such that:
\begin{align}
\alpha|\mathbf{x}|_2 \leq |M\mathbf{x}|_2 \leq \beta|\mathbf{x}|_2 \ \ \text{for all $k$-sparse } \mathbf{x} \in \mathbb{R}^m.
\end{align}
%
Scaling $M$ by a factor of $2/(\beta + \alpha)$ yields $\alpha = 1-\delta$ and $\beta = 1+\delta$ for $\delta = (\beta - \alpha)/(\beta + \alpha)$, which is the form in which the RIP is usually presented.

Between any pair of subspaces in a Euclidian space one can define a set of \textit{principal} (or \textit{canonical} or \textit{Jordan}) angles which are invariant to isomorphic transformations of the space. It can be shown that if the principal angles between these subspaces are all zero, then one subspace is a subset of the other. Hence, if $M$ satisfies \eqref{SparkCondition} then there is at least one nonzero such angle between all pairs of subspaces spanned by at most $k$ columns of $M$. The smallest nonzero principal angle between a pair of subspaces, known also as the Friedrichs angle, can be defined in terms of its cosine as follows:

\begin{definition}\label{FriedrichsDefinition}
The \textbf{Friedrichs angle} $\theta_F = \theta_F(U,V) \in [0,\frac{\pi}{2}]$ between subspaces $U,V \subseteq \mathbb{R}^n$ is the minimal angle formed between unit vectors in $U \cap (U \cap V)^\perp$ and $V \cap (U \cap V)^\perp$. That is,
\begin{align}
\cos{\theta_F} := \max\left\{ \frac{ \langle u, v \rangle }{|u||v|}: u \in U \cap (U \cap V)^\perp, v \in V \cap (U \cap V)^\perp \right\}.
\end{align}
\end{definition}

For example, if $k=2$ and $n=3$ this is the angle between the normal vectors of the two planes. (In this case there is only one nonzero principal angle between non-identical planes.) We also define the following quantity.

\begin{definition}\label{SpecialSupportSet}
Fix $A \in \mathbb{R}^{n \times m}$ and $k < m$. Setting $\phi_1(A) := 1$, define for $k \geq 2$:
\begin{align}\label{rho}
\phi_k(A) := \min_{ S_1,\ldots,S_k \in {[m] \choose k} } \left(1 - \xi( \text{Span}\{A_{S_1}\}, \ldots,  \text{Span}\{A_{S_k}\}) \right),
\end{align}
where for any set $\mathcal{V} = \{V_1, \ldots, V_k\}$ of closed subspaces of $\mathbb{R}^m$, 
\begin{align}
\xi(\mathcal{V}) := \min_{\sigma \in \frak{S}_k} \left(1 - \prod_{i=1}^{k-1} \sin^2  \theta_F \left(V_{\sigma(i)}, V_{\sigma(i)} \cap \cdots \cap V_{\sigma(k)}\right)  \right)^{1/2},
\end{align}
%
and $\frak{S}_k$ is the set of permutations on $k$ elements. We note that $\phi_k(A) \in [0,1]$.
\end{definition}

The quantity $\phi$ is based on a value derived in \cite{Deutsch} to describe the convergence of the alternating projections algorithm for projecting a point onto the intersection of a set of subspaces. We use it to bound the distance between a point and the interesection of a set of subspaces given an upper bound on the distance from that point to each individual subspace. 

%=== SPECIFICS OF DETERMINISTIC THEOREM ===%

We are now in a position to explicitly state the constants $C$ and $\varepsilon_0$ referred to in Theorem \ref{DeterministicUniquenessTheorem}. Letting $X  = (\mathbf{a}_1 \cdots \mathbf{a}_N) \in \mathbb{R}^{m \times N}$, $I(S) := \{i : \text{supp}(\mathbf{a}_i) = S\}$, and $T$ be the set of intervals of length $k$ in a cyclic order of $[m]$, set:
\begin{align}\label{epsilon0}
\varepsilon_0 = \frac{ \phi_k(A) \ell_{2k}(A) }{\rho k^2 \sqrt{2}} \min_{S \in T} \ell_k(AX_{I(S)}),
\end{align}
%
where $\rho := \max_{i \in [m]} |A\mathbf{e}_i|_2 $. If $A$ has unit norm columns (as is standard in applications) and $0 \leq \varepsilon < \varepsilon_0$, then \eqref{Cstable} and \eqref{b-PDa} hold with
\begin{align}\label{Cdef}
C = \frac{\ell_{2k}(A)}{ \varepsilon_0 \sqrt{2k}}.
\end{align}

[**** Shouldn't we put here either here or in the Intro the explicit calculation for the Vandermonde matrices construction?? ****]

\begin{definition}\label{GapMetricDef}
Let $U, V$ be subspaces of $\mathbb{R}^m$ and let $d(u,V) := \inf\{|u-v|_2: v \in W\} = |u - \Pi_V u|_2$ where $\Pi_V$ is the orthogonal projection operator onto subspace $V$. The \textbf{gap metric} $\Theta$ on subspaces of $\mathbb{R}^{m}$ is \cite{akhiezer2013theory}:
\begin{equation}\label{SubspaceMetric}
\Theta(U,V) := \max\left( \sup_{\substack{u \in U, |u|_2 = 1}} d(u,V), \sup_{\substack{v \in V, |v|_2 = 1}} d(v,U) \right).
\end{equation}
\end{definition}

We are now state our main result from combinatorial matrix theory, generalizing \cite[Lemma 1]{Hillar15} to the noisy case.

%===========          MAIN LEMMA (K > 1)             =================

\begin{lemma}[Main Lemma]\label{MainLemma}
Fix positive integers $n, m$, $k < m$, and let $T$ be the set of intervals of length $k$ in some cyclic ordering of $[m]$. Let $A, B \in \mathbb{R}^{n \times m}$ and suppose that $A$ satisfies the spark condition \eqref{SparkCondition}.  If there exists a map $\pi: T \to {[m] \choose k}$ and some $\Delta < \frac{\ell_{2}(A)}{\sqrt{2}} (\max_{i \in [m]}|A\mathbf{e}_i|_2)^{-1}$ such that 
\begin{equation}\label{GapUpperBound}
\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{ \phi_k(A) }{k} \Delta, \indent \text{for all} \indent S \in T,
\end{equation}
%
then there exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and an invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ with
\begin{align}
\|(A - BPD)\mathbf{e}_i\|_2 \leq \Delta \max_{i \in [m]}|A\mathbf{e}_i|_2, \indent \text{for all } i \in [m].
\end{align}
\end{lemma}
We defer the proof of this lemma to Appendix \ref{appendixA}. In words, it says that the vectors forming the columns of $A$ are nearly identical to those forming the columns of $B$ provided that for a special set $T$ of $m$ subspaces, each spanned by $k$ columns of $A$, there exist $k$ columns of $B$ which span a nearby subspace with respect to the gap metric.

In our proof of robust identifiability, we will use the following useful facts about the distance $d$ from Def.~\ref{GapMetricDef}. The first, 
\begin{equation}\label{SubspaceMetricSameDim}
\dim(W) = \dim(V) \implies \sup_{\substack{v \in V, |v|_2 = 1}}  d(v,W)  = \sup_{\substack{w \in W, |w|_2 = 1}} d(w,V),
\end{equation}
is proven in \cite[Lemma 3.3]{Morris10}. The second is:
\begin{lemma}\label{MinDimLemma}
Let $U, V$ be subspaces of $\mathbb{R}^{m}$. Then,
\begin{equation}\label{MinDim}
d(u,V) < |u|_2 \ \ \text{\rm for all } u \in U \setminus{\{0\}} \implies \dim(U) \leq \dim(V).
\end{equation}
\end{lemma}

\begin{proof}
We prove the contrapositive.  If $\dim(U) > \dim(V)$, then a dimension argument ($\dim U + \dim V^\perp > m$) gives a nonzero $u \in U \cap V^\perp$.  In particular, we have $|u - v|_2^2 = |u|_2^2 + |v|_2^2 \geq |u|_2^2$ for all $v \in V$, and thus $d(u,V) \geq |u|_2$.
\end{proof}

%%%---NORMALIZED DICTIONARY LEMMA---%%%
\begin{lemma}\label{NormalizedDictionaryLemma}
[*** We could in principle just use this lemma instead to extend norm 1 col proofs to arbitrary length cols. *** ]
Fix matrices $A, \tilde{A} \in \mathbb{R}^{n \times m}$ where $\tilde{A} = AE$ for some invertible diagonal matrix $E = \text{diag}(\lambda_i) \in \mathbb{R}^{m \times m}$, $\lambda_i \in \mathbb{R}$ for all $i \in [m]$. If there exists a matrix $B \in \mathbb{R}^{n \times m}$ such that $\|(A - B)e_i\| \leq \varepsilon$ for all $i \in [m]$, then the matrix $\tilde{B} = BE$ satisfies $\|(\tilde{A} - \tilde{B})e_i\| \leq \lambda \varepsilon$ for all $i \in [m]$, where $\lambda = \max_i |\lambda_i|$.

This lemma allows us to extend uniqueness guarantees (up to permutation, scaling, and error) for matrices with unit norm columns to those without and vice versa. 
\end{lemma}

\emph{Proof of Lemma \ref{NormalizedDictionaryLemma}:} For all $i \in [m]$, we have:
\begin{align*}
\|(\tilde{A} - \tilde{B})e_i\| = \|(A-B)Ee_i\| = |\lambda_i| \|(A-B)e_i\| \leq |\lambda_i| \varepsilon \leq \lambda \varepsilon 
\indent \blacksquare
\end{align*}

 
%============================================
% PROOF OF DETERMINISTIC UNIQUENESS THEOREM
%============================================

\section{Deterministic Identifiability}\label{DUT}

%======== CASE K = 1 ============

Before proving Theorem \ref{DeterministicUniquenessTheorem} in full generality, consider the case when $k=1$. Suppose we have $N = m$ $1$-sparse vectors $\mathbf{a}_i = c_i\mathbf{e}_i$ $(i = 1, \ldots, m)$ for $c_1, \ldots, c_m \in \mathbb{R}^m/\{0\}$. Fix some $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} and let $\rho = \max_{i \in [m]} |A\mathbf{e}_i|_2$. By \eqref{epsilon0} we have:
\begin{align}
\varepsilon_0 
= \frac{ \phi_1(A) \ell_{2}(A) }{\rho \sqrt{2}} \min_{i \in [m]} \ell_1(c_iA\mathbf{e}_i) 
= \frac{\ell_2(A)}{ \rho \sqrt{2}} \min_{i \in [m]}|c_iA\mathbf{e}_i|_2
\leq \frac{\ell_2(A)}{ \sqrt{2}} \min_{i \in [m]}|c_i| \\
\end{align}
%
and by \eqref{Cdef} we have
\begin{align}
C = \frac{\ell_{2}(A)}{ \varepsilon_0 \sqrt{2}} = \frac{ \rho }{ \min_{i \in [m]} |c_iA\mathbf{e}_i|_2 }
\geq (\min_{i \in [m]} |c_i|)^{-1}. \\
\end{align} 

Suppose that for some $B \in \mathbb{R}^{n \times m}$ and 1-sparse $\mathbf{b}_i \in \mathbb{R}^m$ we have  $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon < \varepsilon_0$ for all $i \in [m]$. Since the $\mathbf{b}_i$ are 1-sparse, there must exist $c'_1, \ldots, c'_m \in \mathbb{R}$ and some map $\pi: [m] \to [m]$ such that 
\begin{align}\label{1D}
|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\pi(i)}|_2 \leq \varepsilon \ \ \text{for all} \  i \in [m].
\end{align} 
Note that $c'_i \neq 0$ for all $i$ since then otherwise we reach the contradiction $|c_iA\mathbf{e}_i|_2 < \varepsilon_0 \leq (\ell_2(A)/\sqrt{2}) \min_{i \in [m]}|c_i| \leq \min_{i\in [m]}|c_iA\mathbf{e}_i|_2$ (by definition of $\ell_2(A)$). We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m]$. Then, $|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\ell}|_2  \leq \varepsilon$ and $|c_jA\mathbf{e}_j - c'_jB\mathbf{e}_{\ell}|_2 \leq \varepsilon$. Using the triangle inequality followed by application of the definition of $\ell$, we have:
%Summing and scaling these two inequalities by $|c'_j|$ and $|c'_i|$, respectively, we have:
\begin{align*}
(|c'_i| + |c'_j|) \varepsilon
&\geq |c'_j||c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\ell}|_2 + |c'_i||c'_jB\mathbf{e}_{\ell} - c_jA\mathbf{e}_j|_2 \\
&\geq |c'_jc_iAe_i - c'_ic_jAe_j|_2 \\
&\geq \ell_2(A)|c'_jc_ie_i - c'_ic_je_j|_2 \\
&\geq \frac{\ell_2(A)}{\sqrt{2}}|c'_jc_ie_i - c'_ic_je_j|_1 \\
&\geq \frac{\ell_2(A)}{\sqrt{2}} \left( |c'_j| + |c'_i| \right) \min_{\ell \in [m]} |c_\ell | \\
&\geq \varepsilon_0 \left( |c'_j| + |c'_i| \right),
\end{align*}
%
which contradicts $\varepsilon < \varepsilon_0$. Hence, $\pi$ is injective. Letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{c'_1}{c_1},\ldots,\frac{c'_m}{c_m})$, we see that \eqref{1D} becomes 
\begin{align}
|(A - BPD)\mathbf{e}_i|_2 = |A\mathbf{e}_i - \frac{c'_i}{c_i}B\mathbf{e}_{\pi(i)}|_2 \leq \frac{\varepsilon}{|c_i|} \leq (\min_{i \in [m]} |c_i|)^{-1} \varepsilon \leq C\varepsilon \ \ \text{for all } i \in [m].
\end{align}

\begin{remark}
We demonstrate with the following counter-example that \eqref{Cstable} with $C$ as defined in \eqref{Cdef} is not necessarily implied if $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [m]$ only holds for $\varepsilon \geq \varepsilon_0$. Consider the dataset $\mathbf{a}_i = \mathbf{e}_i$ for $i = 1, \ldots, m$ and let $A$ be the identity matrix in $\mathbb{R}^{m \times m}$. Then $\varepsilon_0 = 1/\sqrt{2}$ since $\ell_2(A) = 1$ (we have $|A\mathbf{x}|_2 = |\mathbf{x}|_2$ for all $\mathbf{x} \in \mathbb{R}^m$) and $C = 1$. Consider the alternate dictionary $B = \left(\mathbf{0}, \frac{1}{2}(\mathbf{e}_1 + \mathbf{e}_2), \mathbf{e}_3, \ldots, \mathbf{e}_{m} \right)$ and let $\mathbf{b}_i = \mathbf{e}_2$ for $i = 1, 2$ and $\mathbf{b}_i = \mathbf{e}_i$ for $i = 3, \ldots, m$. Then $|A\mathbf{a}_i - B\mathbf{b}_i| = 1/\sqrt{2}$ for $i = 1, 2$ and $0$ otherwise, i.e. the smallest $\varepsilon$ for which $|A\mathbf{a}_i - B\mathbf{b}_i| \leq \varepsilon$ for all $i \in [m]$ is $\varepsilon = 1/\sqrt{2} = \varepsilon_0$. If there were permutation and diagonal matrices $P \in \mathbb{R}^{m \times m}$ and $D \in \mathbb{R}^{m \times m}$ such that $|(A-BPD)\mathbf{e}_i| \leq C\varepsilon$ for all $i \in [m]$, then we would reach the contradiction $1 = |P^{-1}\mathbf{e}_1|_2 = |(A-BPD)P^{-1}\mathbf{e}_1|_2 \leq 1/\sqrt{2}$.
\end{remark}

% ======== b - PDa =========
\begin{remark}
From this result we can, in general, bound $|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i|_1$ as well. Specifically, we will show that \eqref{b-PDa} always follows from \eqref{Cstable} when $\varepsilon < \varepsilon_0$, with $\varepsilon_0$ as defined in \eqref{epsilon0} and $C$ is defined as in \eqref{Cdef}. Note that for all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$ we have by the triangle inequality:
\begin{align*}
|(A-BPD)\mathbf{x}|_2 
\leq C\varepsilon|\mathbf{x}|_1
\leq \sqrt{2k} C \varepsilon |\mathbf{x}|_2
\end{align*}

Thus,
\begin{align*}
|BPD\mathbf{x}|_2 
\geq | |A\mathbf{x}|_2 - |(A-BPD)\mathbf{x}|_2 |
\geq (\ell_{2k}(A) - \sqrt{2k}C\varepsilon ) |\mathbf{x}|_2.
\end{align*}
Hence, $\ell_{2k}(BPD) \geq \ell_{2k}(A) - \sqrt{2k}C\varepsilon$ which is positive by \eqref{Cdef}, and it follows that:
\begin{align*}
|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i|_1
&\leq \sqrt{2k} |D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i|_2 \\
&\leq \frac{\sqrt{2k}}{\ell_{2k}(BPD)}|BPD(D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i)|_2 \\
&\leq \frac{\sqrt{2k}}{\ell_{2k}(BPD)} (|B\mathbf{b}_i - A\mathbf{a}_i|_2 + |(A - BPD)\mathbf{a}_i|_2) \\
&\leq \frac{\varepsilon\sqrt{2k}}{\ell_{2k}(BPD)}(1+C|\mathbf{a}_i|_1) \\
&\leq \frac{\varepsilon\sqrt{2k}(1+C|\mathbf{a}_i|_1)}{\ell_{2k}(A) - C\varepsilon\sqrt{2k}} \\
&= \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right),
\end{align*}
%
where in the last line we have used \eqref{Cdef} to substitute for $C$ in the denominator.  
\end{remark}

% ========== DEFINITIONS FOR MAIN LEMMA (K>1) ================

It remains to show that \eqref{Cstable} with $C$ given in \eqref{Cdef} follows from $\varepsilon < \varepsilon_0$ for $k > 1$. Our main tool for the proof is Lemma \ref{MainLemma}.


%========          PROOF OF THEOREM 1        ============

\begin{proof}[Proof of Theorem \ref{DeterministicUniquenessTheorem} and Corollary \ref{DeterministicUniquenessCorollary}]
Let $T$ be the set of intervals of length $k$ in the given cyclic order of $[m]$.  From above, we may assume that $k > 1$. The first step is to produce a set of $N = m(k-1){m \choose k}+m$ vectors in $\mathbb{R}^k$ in general linear position (i.e., any $k$ of them are linearly independent). Specifically, let $\gamma_1, \ldots, \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Next, form the $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with supports $S \in T$ (partitioning the $a_i$ evenly among these supports so that each support contains $(k-1){m \choose k}+1$ vectors $a_i$) by setting the nonzero values of vector $\mathbf{a}_i$ to be those contained in the $i$th column of $V$.

Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. We claim that $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$. Suppose that for some $B \in \mathbb{R}^{n \times m}$ there exist $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ such that $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [N]$. Since there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ with a given support $S \in T$, the pigeon-hole principle implies that there exists some $S' \in {[m] \choose k}$ and some set of indices $I(S)$ of cardinality $k$ such that all $\mathbf{a}_i$ and $\mathbf{b}_i$ with $i \in I(S)$ have supports $S$ and $S'$, respectively.

Let $X = (\mathbf{a}_1 \cdots \mathbf{a}_N)$ and $\tilde{X} = (\mathbf{b}_1 \cdots \mathbf{b}_N)$ be the matrices formed by horizontally stacking the column vectors $\mathbf{a}_i$ and, respectively, the $\mathbf{b}_i$. It follows from the general linear position of the $\mathbf{a}_i$ and the linear independence of every $k$ columns of $A$ that the columns of the $n \times k$ matrix $AX_{J(S)}$ are linearly independent, i.e. $\ell(AX_{J(S)}) > 0$, and form a basis for $\text{Span}\{A_{S}\}$. Fixing $\mathbf{z} \in \text{Span}\{A_{S}\}$, there then exists a unique $\mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{z} = AX_{J(S)}\mathbf{c}$. Letting $\mathbf{z'} = B\tilde{X}_{J(S)}\mathbf{c}$, which is in $\text{Span}\{B_{S'}\}$, we have:
\begin{align*}
|\mathbf{z} - \mathbf{z'}|_2 = |\sum_{j=1}^N c_i(AX_{J(S)} - B\tilde{X}_{J(S)})\mathbf{e}_j|_2 
\leq \varepsilon \sum_{j=1}^N |c_j| 
\leq \varepsilon |\mathbf{c}|_2 \sqrt{k}  
\leq \frac{\varepsilon \sqrt{k}}{\ell(AX_{J(S)})} |AX_{J(S)}\mathbf{c}|_2.
= \frac{\varepsilon \sqrt{k}}{\ell(AX_{J(S)})} |\mathbf{z}|_2.
\end{align*}

Hence,
\begin{align}\label{ABSubspaceDistance}
\sup_{ \substack{ \mathbf{z} \in \text{Span}\{A_{S}\} \\ |\mathbf{z}|_2 = 1} } d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \frac{\varepsilon\sqrt{k}}{\ell(AX_{J(S)})}.
\end{align}

We now show that \eqref{Cstable} and \eqref{b-PDa} both follow if $\varepsilon < \varepsilon_0$, with $\varepsilon_0$ as defined in \eqref{epsilon0}. In fact, to show \eqref{Cstable} we only require $\varepsilon <  \frac{\varepsilon_0 \sqrt{k} \ell_2(A)}{\ell_{2k}(A)}$. In this case the RHS of \eqref {ABSubspaceDistance} works out to be strictly less than 1, since:
\begin{align}\label{rhs}
\frac{\varepsilon\sqrt{k}}{\ell(AX_{J(S)})} 
< \frac{\sqrt{k}}{\ell(AX_{J(S)})} \left( \frac{\varepsilon_0 \sqrt{k} \ell_2(A)}{\ell_{2k}(A)} \right)
= \frac{\ell_2(A) \phi_k(A)}{\rho \ell(AX_{J(S)}) k \sqrt{2}} \min_{S \in T}\ell_k(AX_{I(S)})
\leq \frac{\phi_k(A)}{k} \left( \frac{\ell_2(A)}{ \rho \sqrt{2}} \right)
\end{align}
%
where $I(S) = \{i: \text{supp}(\mathbf{a}_i)=S\}$ and since $\ell_2(A) \leq \rho \sqrt{2}$ and $\phi_k(A) \in [0,1]$. It follows by Lemma \ref{MinDimLemma} that $\dim(\text{Span}\{B_{S'}\}) \geq \dim(\text{Span}\{A_{S}\}) = k$ (since every $k$ columns of $A$ are linearly independent). Since $|S'| = k$, we have $\dim(\text{Span}\{B_{S'}\}) \leq k$, hence $\dim(\text{Span}\{B_{S'}\}) = \dim(\text{Span}\{A_{S}\})$. Recalling \eqref{SubspaceMetricSameDim},  we see the association $S \mapsto S'$ thus defines a map $\pi: T \to {[m] \choose k}$ satisfying
\begin{align}\label{yeyeye}
\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{\varepsilon\sqrt{k}}{\ell(AX_{J(S)})} \indent \text{for all } S \in T.
\end{align}

From \eqref{rhs} and \eqref{yeyeye} we see that the inequality $\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{ \phi_k(A) }{k} \Delta$ is satisfied for this support $S$ and $\Delta < \frac{\ell_2(A)}{\rho \sqrt{2}}$ by setting $\Delta = \frac{\varepsilon k\sqrt{k}}{\phi_k(A)\ell(AX_{J(S)})}$. We therefore satisfy \eqref{GapUpperBound} for $\Delta = \frac{\varepsilon k\sqrt{k}}{\phi_k(A)\min_{S \in T} \ell_k(AX_{I(S)})} = \frac{\varepsilon \ell_{2k}(A)}{\varepsilon_0 \rho \sqrt{2k}}$. It follows by Lemma \ref{MainLemma} that there exists a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that for all $i \in \{1, \ldots, m\}$,
$|(A - BPD)\mathbf{e}_i|_2 \leq C\varepsilon$ for $C = \frac{ \ell_{2k}(A)}{\varepsilon_0 \sqrt{2k}}$. It follows by the arguments at the beginning of this section that \eqref{b-PDa} holds as well.
\end{proof}

%===================================
% DIFFERENT CODING DIMENSIONS
%===================================

\section{Unknown representation dimension}\label{mleqm}

In this section we state a version of Theorem \ref{DeterministicUniquenessTheorem} and Lemma \ref{MainLemma} assuming that $B$ also satisfies the spark condition (in addition to $A$ satisfying the spark condition). With this additional assumption, we can address the issue of recovering $A \in \mathbb{R}^{n \times m}$ and the $\mathbf{a}_i \in \mathbb{R}^m$ when only an upper bound $m'$ on the number $m$ of sparse sources is known.

\begin{theorem}\label{DeterministicUniquenessTheorem2}
Fix positive integers $n, m, m'$, and $k$ with $k < m \leq m'$. Fix $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ and let $X  = (\mathbf{a}_1 \cdots \mathbf{a}_N) \in \mathbb{R}^{n \times N}$ and $I(S) = \{i : \text{supp}(\mathbf{a}_i) = S\}$. If for some cyclic order on $[m]$ the $\mathbf{a}_i$ are such that for every interval of length $k$ in the order there are $(k-1){m' \choose k}+1$ vectors $\mathbf{a}_i$ in general linear position supported on that interval, then the following holds. Every pair of matrices $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfying spark condition \eqref{SparkCondition} are such that if 
\begin{align}
|A\mathbf{a}_i - B\mathbf{b}_i| < \frac{ \ell_{k}(A) \min(\phi_k(A), \phi_k(B))}{\sqrt{2k^3} } \min_{S \in T} \ell_k(AX_{I(S)}), \ \  \text{for all} \  i \in [N],
\end{align}
%
for some $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^{m'}$, then \eqref{Cstable} holds for some partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ (there is at most one nonzero entry in each row and column and these nonzero entries are all 1), diagonal matrix $D \in \mathbb{R}^{m' \times m}$ ($D_{ij} = 0$ whenever $i \neq j$) and $C$ as defined in \eqref{Cdef}. 
\end{theorem}

The proof of Theorem \ref{DeterministicUniquenessTheorem2} is very similar to the proof of Theorem \ref{DeterministicUniquenessTheorem}, the difference being that now we establish a map $\pi: [m] \to [m']$ satisfying the requirements of Lemma \ref{MainLemma2}, which we state next, by pigeonholing $k{m' \choose k}$ vectors with respect to holes $[m']$. The assumption that $B$ also satisfies the spark condition allows us to overcome the complication due to $m < m'$ to prove Lemma \ref{MainLemma2}, since the proof of Lemma \ref{NonEmptyLemma} relies on the fact that $m = m'$. 

\begin{lemma}[Main Lemma for $m \leq m'$]\label{MainLemma2}
Fix positive integers $n, m, m'$, and $k$ where $k < m \leq m'$, and let $T$ be the set of intervals of length $k$ in some cyclic ordering of $[m]$. Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfy spark condition \eqref{SparkCondition}, with $A$ having unit norm columns. If there exists a map $\pi: T \to {[m'] \choose k}$ and some $\Delta < \frac{\ell_{2}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound2}
\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{ \Delta }{k} \min(\phi_k(A), \phi_k(B)), \ \text{for all} \ S \in T,
\end{equation}
%
then there exist a partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and a diagonal matrix $D \in \mathbb{R}^{m' \times m}$ such that
\begin{align}
|(A - BPD)\mathbf{e}_i|_2 \leq \Delta, \ \text{for all} \ i \in [m].
\end{align}
\end{lemma}

We again defer the proof of this lemma to Appendix \ref{mleqmAppendix}.


%======================================
% PROBABILISTIC THEOREMS
%======================================

\section{Probabilistic Identifiability}\label{PUT}

We next give precise statements of our probabilistic versions of Theorem \ref{DeterministicUniquenessTheorem}, along with brief proofs relying largely to the methods used in \cite{Hillar15}. Our statements are based upon the following elementary construction of \emph{random sparse vectors} (although many ensembles will suffice, e.g. \cite[\S 4]{baraniuk2008simple}).

\begin{definition}[Random $k$-Sparse Vectors]\label{RandomDraw}
Given the support set for its $k$ nonzero entries, a random draw of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all ${m \choose k}$ of them and then a random draw.
\end{definition}

Our proofs rely on the following lemma, the proof of which can be found in \cite[Lemma 3]{Hillar15}:
\begin{lemma}\label{Hillar15lemma2}
Fix positive integers $n, m$ and $k < m$ and a matrix $M \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. With probability one, $M\mathbf{a}_1, \ldots, M\mathbf{a}_k$ are linearly independent whenever the $\mathbf{a}_i$ are random $k$-sparse vectors.
\end{lemma}

\begin{theorem}\label{Theorem2}
Fix positive integers $n, m$, $k < m$, and $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. If a set of $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for each interval of length $k$ in some cyclic order on $[m]$ there are $k{m \choose k}$ vectors $\mathbf{a}_i$ supported on that interval then $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robustly identifiable $k$-sparse representation with probability one.
\end{theorem}

\begin{proof}
By Lemma \ref{Hillar15lemma2} (setting $M$ to be the identity matrix), with probability one the $\mathbf{a}_i$ are in general linear position. Apply Theorem \ref{DeterministicUniquenessTheorem}.
\end{proof} 

\begin{corollary}
Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing such an $A$, we have with probability one that a dataset $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N\}$ generated from $N = m(k-1){m \choose k}+m$ $k$-sparse samples $\mathbf{a}_i$, consisting of $(k-1){m \choose k}+1$ samples supported by each interval of length $k$ in some cyclic ordering of $\{1, \ldots, m\}$, has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}
We now state our third theorem. Note that an \emph{algebraic set} is a solution to a finite set of polynomial equations. 

\begin{theorem}\label{Theorem3}
Fix positive integers $k < m$ and $n$ . If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval, then with probability one the following holds. There is an algebraic set $Z \subset \mathbb{R}^{n \times m}$ of Lebesgue measure zero with the following property: if $A \notin Z$ then $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{theorem}

\begin{proof}
By Lemma \ref{Hillar15lemma2} (setting $M$ to be the identity matrix), with probability one the $\mathbf{a}_i$ are in general linear position. By the same arguments made in the proof of Theorem 3 in \cite{Hillar15}, the set of matrices $A$ that fail to satisfy \eqref{SparkCondition} form an algebraic set of measure zero. Apply Theorem \ref{DeterministicUniquenessTheorem}.
\end{proof}

\begin{corollary}
Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}.  If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval then, with probability one, almost every matrix $A \in \mathbb{R}^{n \times m}$ gives $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}


%===================================
% 			DISCUSSION
%===================================

\section{Discussion}


Another implication of our theorem is the determination of an overcompleteness regime dimensionality of any dataset with respect to $\varepsilon$. If you coded very well some data sparsely then you can do a check and if that check holds up then anyone else who codes the data sparsely too has learned the same dictionary. 

The theory of CS informs also informs another practical consequence of our result. Since our derived sample complexity is independent of the ambient dimension of the data, $n$, given a lower bound on the sparsity of the latent variables $\mathbf{a}_i$ we can generate a random matrix to compress the data to a dimension in the regime of \eqref{CScondition} before applying a dictionary learning. This could significantly reduce the computational cost of dictionary learning when the sparsity is high by reducing the number of parameters required to define each dictionary element. Such improvements are crucial to scaling up dictionary learning to larger datasets. \textbf{[Is this actually a significant computational boost..?]}

Uniqueness in data analysis.

[********* Make some brief comments about possibile conseuqences for the brain / representability of signals, etc *********]


Identifying uniqueness: the bispectrum collapses the equivalence class of $PD$ matrices to a point.

%========================================
%      	APPENDIX: COMBINATORICS
%========================================

\appendices
\section{Combinatorial Matrix Theory}\label{appendixA}

In this section, we prove Lemma \ref{MainLemma}, which is the main ingredient in our proof of Theorem \ref{DeterministicUniquenessTheorem}. For readers willing to assume a priori that the spark condition holds for $B$ as well as for $A$, a shorter proof of this case (Lemma \ref{MainLemma2} from Section \ref{mleqm}) is provided in Appendix \ref{mleqmAppendix}. This additional assumption simplifies the argument and allows us to extend stability conditions to the case where only an upper bound on $m$ is known. 

Given positive integers $k < m$ and a permutation $\sigma \in \frak{S}_m$ (where $\frak{S}_m$ denotes the \textit{symmetric group} of bijections on $m$ elements), let
\begin{align}
S_\sigma(i) := \{1+\sigma(i), \ldots, 1+\sigma(i + (k-1)) \} \indent \text{for} \indent i = 0, \ldots, m-1
\end{align}
%
with addition modulo $m$. In words, the $S_\sigma(i)$ are the intervals of length $k$ within the set of elements of $\{1, \ldots, m\}$ arranged in the cyclic order $\sigma(1), \ldots, \sigma(m)$.

We now prove some auxiliary lemmas before deriving Lemma \ref{MainLemma}.

%===== SPAN INTERSECTION LEMMA =====

\begin{lemma}\label{SpanIntersectionLemma}
Let $M \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $M$ are linearly independent, then for any $\mathcal{T} \subseteq \bigcup_{\ell \leq k} {[m] \choose \ell}$,
\begin{align}
y \in \text{Span}\{M_{\cap \mathcal{T}}\}  \Longleftrightarrow y \in \bigcap_{S \in \mathcal{T}} \text{Span}\{M_S\}.
\end{align}
\end{lemma}

\begin{proof}The forward direction is trivial; we prove the reverse direction. Enumerate $\mathcal{T} = (S_1, \ldots, S_{|\mathcal{T}|})$ and let $\mathbf{y} \in \text{Span}\{M_{S_1}\} \cap \text{Span}\{M_{S_2}\}$. Then there exists some $\mathbf{x}_1$ with support contained in $S_1$ such that $\mathbf{y} = M\mathbf{x}_1$ and some $\mathbf{x}_2$ with support contained in $S_2$ such that $\mathbf{y} = M\mathbf{x}_2$. We therefore have $M(\mathbf{x}_1 - \mathbf{x}_2) = 0$, which implies that $\mathbf{x}_1 = \mathbf{x}_2$ by the spark condition. Hence $\mathbf{x}_1$ and $\mathbf{x}_2$ have the same support contained in both $S_1$ and $S_2$, i.e. $\mathbf{y} \in \text{Span}\{M_{S_1 \cap S_2}\}$. This carries over by induction to the entire sequence of supports in $\mathcal{T}$. 
\end{proof}

%===== DISTANCE TO INTERSECTION LEMMA =====

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $\mathcal{V} = \{V_1, \ldots, V_k\}$ be a set of closed subspaces of $\mathbb{R}^m$ and let $V = \bigcap \mathcal{V}$. For every $\mathbf{x} \in \mathbb{R}^m$,
\begin{align}\label{DTILeq}
|\mathbf{x} - \Pi_V \mathbf{x}|_2 \leq \frac{1}{1 - \xi(\mathcal{V})} \sum_{i=1}^k |x - \Pi_{V_i} x|_2,
\end{align}
\end{lemma}
where the expression for $\xi$ is given in Def.~\ref{SpecialSupportSet}.

\begin{proof} 
Fix $x \in \mathbb{R}^m$ and $k \geq 2$. The proof can be subdivided into two steps. First, we will show that for any $\sigma \in \frak{S}_k$,
\begin{align}\label{induction}
|x - \Pi_Vx|_2 \leq \sum_{i=1}^k |x - \Pi_{V_i} x|_2 + |\Pi_{V_{\sigma(k)}}\Pi_{V_{\sigma(k-1)}}\cdots\Pi_{V_{\sigma(1)}} x - \Pi_V x|_2.
\end{align}
%
Assume without loss of generality that $\sigma(i) = i$ for all $i \in [m]$. We have by the triangle inequality that
\begin{align*}
|x - \Pi_Vx|_2 &= |x - \Pi_{V_k} x|_2 + |\Pi_{V_k}(I - \Pi_{V_{k-1}}) x|_2 + |\Pi_{V_k}\Pi_{V_{k-1}}x - \Pi_Vx|_2 \\
&\leq \sum_{i=k-1}^k|x - \Pi_{V_i} x|_2 + |\Pi_{V_k}\Pi_{V_{k-1}} x - \Pi_V x|_2,
\end{align*}
%
where in the second line we have used the fact that $|\Pi_{V_k}|_2 \leq 1$. If $k=2$ then we are done; otherwise, we may repeat this manipulation another $k-2$ times until we arrive at \eqref{induction}. Next, we show how the result \eqref{DTILeq} follows from \eqref{induction}. To do so, we make use of the following result from \cite[Theorem 9.33]{Deutsch}:
\begin{align}\label{dti2}
|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} x - \Pi_Vx|_2 \leq c |x|_2 \indent \text{for all} \indent x \in \mathbb{R}^m.
\end{align}
%
where $c:= \left[1 - \prod_{i=1}^{k-1}(1-c_i^2)\right]^{1/2}$ and $c_i = \cos\theta_F\left(V_i, \cap_{j=i+1}^kV_j\right)$. Note that
\begin{align}\label{dti1}
|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)|_2 
&= |(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}) x - \Pi_V x |_2,
\end{align}
%
since $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$.
%
We therefore have by \eqref{dti2} and \eqref{dti1} that
\begin{align*}
|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} x - \Pi_V x |_2
&= |(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)|_2 \\
&\leq c |x - \Pi_Vx|_2.
\end{align*}

Substituting the left-hand side into \eqref{induction}, we get
\begin{align*}
|x - \Pi_Vx|_2 \leq \sum_{i=1}^k |x - \Pi_{V_i} x|_2 + c |x - \Pi_Vx|_2,
\end{align*}
%
from which it follows that
\begin{align}\label{ceq}
|x - \Pi_V x|_2 \leq \frac{1}{1 - c} \sum_{i=1}^k |x - \Pi_{V_i} x|_2.
\end{align}
Since the permutation $\sigma$ we chose was arbitrary, we can replace $c$ in \eqref{ceq} with $\xi(\mathcal{V})$ to obtain \eqref{DTILeq}.
\end{proof}

%======= GRAPH THEORY LEMMA =======

\begin{lemma}\label{NonEmptyLemma} Fix positive integers $k < m$, $\sigma \in \frak{S}_m$, and let $T = \{S_\sigma(1), \ldots, S_\sigma(m)\}$. Suppose there exists a map $\pi: T \to {\mathbb{Z}/m\mathbb{Z} \choose k}$ such that for all $J \in {[m] \choose k}$,
\begin{align}\label{EmptyToEmpty}
 \bigcap_{i \in J} S_\sigma(i) = \emptyset \ \Longrightarrow \ \bigcap_{i \in J} \pi(S_\sigma(i)) = \emptyset.
\end{align}
%
Then  $\pi(S_\sigma(i)) \cap \cdots \cap \pi(S_\sigma(i+(k-1))) \neq \emptyset$ for all $i \in \mathbb{Z}/m\mathbb{Z}$.
\end{lemma}

\begin{proof} Consider the set $Q_m = \{ (i,j) : i \in \mathbb{Z}/m\mathbb{Z}, j \in \pi(S_\sigma(i)) \}$, which has $mk$ elements. By the pigeon-hole principle, there is some $p \in \mathbb{Z}/m\mathbb{Z}$ and $J \in {[m] \choose k}$ such that $(i, p) \in Q_m$ for all $i \in J$. Hence, $p \in \cap_{i \in J} \pi(S_\sigma(i))$ and by \eqref{EmptyToEmpty} there must be some $v \in \mathbb{Z}/m\mathbb{Z}$ such that $\sigma(v) \in \cap_{i \in J} S_\sigma(i)$. This is only possible (since $|J| = k$) if the elements of $J$ are consecutive in $\mathbb{Z}/m\mathbb{Z}$, i.e. $J = \{v - (k-1), \ldots, v\}$. (The only elements in $T$ that contain $\sigma(v)$ are $S_\sigma(v-(k-1)), \ldots, S_\sigma(v)$.)

If there existed some $i \notin J$ such that $p \in \pi(S_\sigma(i))$ then we would have $p \in \pi(S_\sigma(i)) \cap \pi(S_\sigma(v - k+1)) \cap \cdots \cap \pi(S_\sigma(v))$ and \eqref{EmptyToEmpty} would imply that the intersection of every $k$-element subset of $\{S_\sigma(i)\} \cup \{S_\sigma(j): j \in J\}$ is non-empty. This would only be possible if $\{i\} \cup J = \mathbb{Z}/m\mathbb{Z}$, in which case the result then trivially holds since then $p \in \pi(S_\sigma(i))$ for all $i \in [m]$. If there exists no additional $i \notin J$ such that $p \in \pi(S_\sigma(i))$ then by letting $Q_{m-1} \subset Q_m$ be the set of elements of $Q_m$ not having $p$ as a second coordinate we have $|Q_{m-1}| = (m-1)k$ and the result follows by iterating the above arguments.
\end{proof}

%==== PROOF OF MAIN LEMMA =======

\begin{proof}[Proof of Lemma \ref{MainLemma} (Main Lemma)]
We assume $k \geq 2$ since the case $k = 1$ was proven at the beginning of Section \ref{DUT}. Note that $T = \{S_\sigma(1), \ldots, S_\sigma(m)\}$ for some $\sigma \in \frak{S}_m$. We begin by proving that $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$ for all $i \in [m]$. [*** We already proved this for $S' = \pi(S)$ in the proof of Theorem 1 -- should we remove this redundancy? We have to prove it again here though if we want the lemma to stand on its own. ***] Fix $i \in [m]$ and note that by \eqref{GapUpperBound} we have for all unit vectors $u \in \text{Span}\{A_{S_\sigma(i)}\}$ that $d(u, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for $\Delta < \frac{\ell_2(A)}{\sqrt{2}} \left( \max_{i \in [m]} |A\mathbf{e}_i| \right)^{-1}$. By definition of $\ell_2(A)$ we have for all $2$-sparse $\mathbf{x} \in \mathbb{R}^m$:
\begin{align}
\ell_2(A) \leq \frac{|A\mathbf{x}|_2}{|\mathbf{x}|_2} \leq \max_{i \in [m]} |A\mathbf{e}_i|_2 \frac{|\mathbf{x}|_1}{|\mathbf{x}|_2} \leq \sqrt{2} \max_{i \in [m]}|A\mathbf{e}_i|_2
\end{align}

Hence $\Delta < 1$. Since $\phi_k \in [0,1]$ we have $d(u, \text{Span}\{B_{\pi(S_\sigma(i))}\}) < 1$ and it follows by Lemma \ref{MinDimLemma} that $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) \geq \dim(\text{Span}\{A_{S_\sigma(i)}\}) = k$. Since $|\pi(S_\sigma(i))| = k$, we in fact have $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$. %, i.e. the columns of $B_{\pi(S_\sigma(i))}$ are linearly independent. 

We will now show that
\begin{align}\label{fact2}
|\bigcap_{i \in J} \pi(S_\sigma(i))| \leq |\bigcap_{i \in J} S_\sigma(i) | \indent, \text{for all} \ J \in {[m] \choose k}.
\end{align}

Fix $J \in {[m] \choose k}$. By \eqref{GapUpperBound} we have for all unit vectors $\mathbf{u} \in \cap_{i \in J} \text{Span}\{B_{\pi(S_\sigma(i))}\}$ that $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(i)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $i \in J$. It follows by Lemma \ref{DistanceToIntersectionLemma} that
\begin{align*}
d\left( \mathbf{u}, \bigcap_{i \in J} \text{Span}\{A_{S_{\sigma}(i)}\} \right) 
\leq \frac{ \phi_k(A) \Delta  }{1 - \xi( \{ \text{Span}\{A_{S_{\sigma}(i)}\}: i \in J\} ) } \leq \Delta,
\end{align*}
%
where the second inequality follows immediately from the definition of $\phi_k(A)$. 

Now, since \mbox{$\text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_\sigma(i))}\}$} and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\cap_{i \in J}  S_\sigma(i)}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\cap_{i \in J} S_\sigma(i)}\} \right) \leq \Delta \indent \text{for all unit vectors } \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma} (since $\Delta < 1$) that $\dim(\text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\}) \leq \dim(\text{Span}\{A_{\cap_{i \in J} S_\sigma(i)}\})$ and \eqref{fact2} follows by the linear independence of the columns of $A_{S_\sigma(i)}$ and $B_{\pi(S_\sigma(i))}$ for all $i \in [m]$.

Suppose now that $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_\sigma(i) = \sigma(i)$. By \eqref{fact2} we have that $\cap_{i \in J} \pi(S_\sigma(i))$ is either empty or it contains a single element. Lemma \ref{NonEmptyLemma} ensures that the latter case is the only possibility. Thus, the association $\sigma(i) \mapsto \cap_{i \in J} \pi(S_\sigma(i))$ defines a map $\hat \pi: [m] \to [m]$. Recalling \eqref{SubspaceMetricSameDim}, it follows from \eqref{fact1} that for all unit vectors $\mathbf{u} \in \text{Span}\{A_{\sigma(i)}\}$ we have $d\left( \mathbf{u}, \text{Span}\{B_{\hat \pi(\sigma(i))}\}\right) \leq \Delta$ also. Since $i$ is arbitrary and $\sigma$ is just a permutation, it follows that for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, letting $c_i = |A\mathbf{e}_i|_2^{-1}$ there exists some $c'_i \in \mathbb{R}$ such that $|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\hat \pi(i)}|_2 \leq \Delta$ where $\Delta < \frac{\ell_2(A)}{\sqrt{2}} \min_{i \in [m]} c_i$. This is exactly the supposition in \eqref{1D} and the result follows from the subsequent arguments of Section \ref{DUT}. 
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}

%================================
% APPENDIX: ADAPTED MAIN LEMMA
%================================

\section{Unknown $m$}\label{mleqmAppendix}

In this section we prove Lemma \ref{MainLemma2} from Section \ref{mleqm}.

%=== PROOF OF ADAPTED MAIN LEMMA ===

\begin{proof}
We consider the case when $k=1$ before showing how the case for $k>1$ reduces to this. Assumption \eqref{GapUpperBound2} implies that for all canonical basis vectors $\mathbf{e}_i \in \mathbb{R}^m$ there is some $c_i \in \mathbb{R}$ such that 
\begin{align}\label{yepyep}
|A\mathbf{e}_i - c_iB\mathbf{e}_{\pi(i)}|_2 \leq \Delta \indent \text{for all} \indent i \in [m].
\end{align}
Note that if $c_i = 0$ for some $i$, then $|A\mathbf{e}_i| < 1$ (since $\ell_2(A) \leq 1$), contradicting the fact that $A$ has unit norm columns. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = p$ for some $i \neq j$ and $\ell \in [m']$. Then $|A\mathbf{e}_i - c_iB\mathbf{e}_{p}|_2  \leq \Delta$ and $|A\mathbf{e}_j - c_jB\mathbf{e}_{p}|_2 \leq \Delta$. Summing and scaling these two inequalities by $|c_j|$ and $|c_i|$, respectively, the triangle inequality yields
\begin{align*}
(|c_i| + |c_j|) 2\varepsilon
&\geq |c_j||A\mathbf{e}_i - c_iB\mathbf{e}_{p}|_2 + |c_i||c_jB\mathbf{e}_{p} - A\mathbf{e}_j|_2 \\
&\geq |c_jAe_i - c_iAe_j|_2 \\
&\geq \ell_2(A)|c_je_i - c_ie_j|_2
\end{align*}
%
which is in contradiction with the fact that $|x|_1 \leq \sqrt{2}|x|_2$ for all $x \in \mathbb{R}^2$ and $\Delta < \frac{\ell_2(A)}{\sqrt{2}}$. Hence, $\pi$ is injective. Letting $P$ and $D$ denote the following partial permutation and diagonal matrices, respectively: 
\begin{equation}\label{PandD}
P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}, \mathbf{0}, \cdots, \mathbf{0} \right), \ \ D = \left(\begin{array}{ccc}c_1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & c_m \\ 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0
\end{array}\right),
\end{equation}
[******* put this $P$, $D$ ``inline" ******]
we see that \eqref{yepyep} becomes $|(A - BPD)\mathbf{e}_i|_2 \leq \Delta$ for all $i \in [m]$.

Now suppose $k \geq 2$. Note that $T = \{S_\sigma(1), \ldots, S_\sigma(m)\}$ for some $\sigma \in \frak{S}_m$. Fix $i \in [m]$ and let $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_\sigma(i) = \sigma(i)$. Assumption \eqref{GapUpperBound2} implies that for all unit vectors $\mathbf{u} \in \cap_{j \in J} \text{Span}\{B_{\pi(S_\sigma(j))}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(j)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $j \in J$. By Lemma \ref{DistanceToIntersectionLemma} we have that:
\begin{align}\label{sym2}
d\left( \mathbf{u}, \bigcap_{j \in J} \text{Span}\{A_{S_{\sigma}(j)}\} \right) 
\leq \Delta \left( \frac{\phi_k(A)}{1 - \xi(\{ \text{Span}\{A_{S_{\sigma}(j)}\} : j \in J \})} \right) \leq \Delta,
\end{align}
%
where the second inequality is by definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_i)}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\sigma(i)}\}$, we have
\begin{align}\label{fact12}
d\left( \mathbf{u}, \text{Span}\{A_{\sigma(i)}\} \right) \leq \Delta \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\})$ and $W = \text{Span}\{A_{\sigma(i)}\}$, that $\dim(V) \leq \dim(W)$. The same arguments can be used to prove \eqref{fact12} with the roles of $A$ and $B$ reversed. Hence, $\dim(V) = \dim(W) = 1$ and it follows that $|\cap_{i \in J} \pi(S_\sigma(i))| = 1$ since the columns of $B_{\cap_{i \in J} \pi(S_\sigma(i))}$ are linearly independent by the spark condition. Thus, the association $\sigma(i) \mapsto \cap_{i \in J} \pi(S_\sigma(i))$ defines a map $\pi: [m] \to [m']$ such that $d\left( \mathbf{u}, \text{Span}\{B_{ \pi(S_\sigma(i))}\}\right) \leq \Delta$ for all $i \in [m]$, i.e. for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, there exists some $c_i \in \mathbb{R}$ such that $|A\mathbf{e}_i - c_iB\mathbf{e}_{\hat \pi(i)}|_2 \leq \Delta$ where $\Delta < \frac{\ell_{2}(A)}{\sqrt{2}} \min_{i \in [m]} |c_i|$. This is exactly the supposition in \eqref{yepyep}.
\end{proof}

%\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
%\end{remark}


%===================================
% 		APPENDIX: CALCULATING C
%===================================

\section{Calculating C}

[********  Incorporate this better into the main text what the point of this is *********]

In this section we demonstrate how an upper bound on the constant $C$ can be derived for a particular $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$. \textbf{[Alternatively, see Lemma 2.2 in Grcar??????? - A matrix lower bound, for a lower bound on rectangular matrices (though not k-restricted)]}

%=== MATRIX LOWER BOUND LEMMA ===

\begin{lemma}\label{MatrixLowerBoundLemma}
Let $\gamma_1 < \cdots < \gamma_N$ be distinct numbers with $\gamma_{i+1} = \gamma_i + \delta$ and form the $k \times N$ Vandermonde matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$. Then for all $S \in {[N] \choose k}$, we have
\begin{align}
	|V_S x|_2 > \rho |x|_1, \indent \text{for all} \indent \mathbf{x} \in \mathbb{R}^k,
\end{align}
where \[\indent \rho := \frac{\delta^k}{\sqrt{k}} \left( \frac{k-1}{k} \right)^\frac{k-1}{2} \prod_{i = 1}^k (\gamma_1 + (i-1)\delta).\]
\end{lemma}


\begin{proof} 
The determinant of the Vandermonde matrix is
\begin{align}
	\det(V) = \prod_{1 \leq j \leq k} \gamma_j \prod_{1 \leq i \leq j \leq k} (\gamma_j - \gamma_i) \geq \delta^k \prod_{i = 1}^k (\gamma_1 + (i-1)\delta).
\end{align}	
Since the $\gamma_i$ are distinct, the determinant of any $k \times k$ submatrix of $V$ is nonzero; hence $V_S$ is nonsingular for all $S \in {[N] \choose k}$. Suppose $\mathbf{x} \in \mathbb{R}^k$. Then $|\mathbf{x}|_2 = |V_S^{-1} V_S \mathbf{x}|_2 \leq \|V_S^{-1}\|_2 |V_S \mathbf{x}|_2$, implying $|V_S \mathbf{x}|_2 \geq \|V_S^{-1}\|_2^{-1}|\mathbf{x}|_2 \geq \frac{1}{\sqrt{k}} \|V_S\|_2^{-1}|\mathbf{x}|_1$. For the Euclidean norm we have $\|V_S^{-1}\|_2^{-1} = \sigma_{\min}(V_S)$, where $\sigma_{\min}$ is the smallest singular value of $V_S$. A lower bound for the smallest singular value of a nonsingular matrix $M \in \mathbb{R}^{k \times k}$ is given in \cite{hong1992lower}:
\begin{align}
	\sigma_{\min}(M) > \left( \frac{k-1}{k} \right)^\frac{k-1}{2} |\det M|,
\end{align}
%
and the result follows. 
\end{proof}

%===================================
% 		ACKNOWLEDGEMENT
%===================================

\section*{Acknowledgment}
We would like to thank Fritz Sommer for turning our attention to the dictionary learning problem. We also thank Bizzyskillet on SoundCloud for "The No-Exam Jams", which played on repeat during many long hours of designing proofs. Finally, we thank Ian Morris for posting \eqref{SubspaceMetricSameDim} and a reference to his proof of it on the internet (``Stack Exchange").
Thank german dude who visited Redwood.  Thank Darren Rhea for early explorations in this problem.

%===================================
% 			REFERENCES
%===================================

\bibliographystyle{IEEEtran}
\bibliography{RobustIdentifiability}

%===================================
% 			BIOGRAPHY
%===================================

\begin{IEEEbiographynophoto}{Charles J. Garfinkle}
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Christopher J. Hillar}
completed a B.S. in Mathematics and a B.S. in Computer Science at Yale University.  Supported by an NSF Graduate Research Fellowship, he received his Ph.D. in Mathematics from the University of California (UC), Berkeley in 2005. From 2005-2008, he was a Visiting Assistant Professor and NSF Postdoctoral Fellow at Texas A\&M University. From 2008-2010, he was an NSF Mathematical Sciences Research Institutes Postdoctoral Fellow at the Mathematical Sciences Research Institute (MSRI) in Berkeley, CA.  In 2010, he joined the Redwood Center for Theoretical Neuroscience at UC Berkeley, and in 2011, he  became a research specialist in the Tecott mouse behavioral neuroscience lab at UC San Francisco.
\end{IEEEbiographynophoto}

%===================================
% 			SCRAP PAPER
%===================================

\section*{SCRAP PAPER}

\subsection{ Random sampling to fill a set of cyclic intervals }
\begin{problem}
Fix $m$ and $k < m$. What is the probability that $N$ random $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ satisfy: for every interval of length $k$ in some cyclic ordering of $\{1, \ldots, m\}$ there are at least $p = k{m \choose k}$ vectors $\mathbf{a}_i$ supported on that interval?
\end{problem}

There are $\frac{m!}{m} = (m-1)!$ cyclic orders on $m$ elements and the set of intervals of length $k$ in a given order is invariant to reversal of the order. There are therefore $\frac{(m-1)!}{2}$ sets $T \in { {[m] \choose k} \choose m}$ which consist of all length $k$ intervals in some cyclic order of $\{1, \ldots, m\}$. We want to know the probability that for at least one such $T$, every $S \in T$ has at least $mp$ samples.

Fix one such $T$ and suppose we have $N_T$ random $k$-sparse vectors $\mathbf{a}_i$ with supports in $T$. The number of possible ways in which there can be at least $p$ of these vectors supported on every support $S \in T$ is the number of nonnegative integer solutions $d_i$ to the equation
\[ (d_1 + p) + \ldots + (d_m + p) = N_T.\]
%
In general, the number of nonnegative integer solutions to the equation $d_1 + \ldots + d_m = M$ is ${M+m-1 \choose m-1}$. Hence, the probability that for every $S \in T$ there are at least $p$ vectors $\mathbf{a}_i$ supported on $S$ is:
\[ q(N_T) = \frac{ { N_T-m(p-1)-1 \choose m-1 } }{ {N_T+m-1 \choose m-1} } \]

We need at least $mp$ supports allocated to at least one such $T$ to have nonzero probability of success. Suppose the $N$ vectors are distributed over only $m$ supports. Then the probability that these $m$ supports form a valid set $T$ is $\frac{(m-1)!}{2} / { {m \choose k} \choose m}$ and the probability that every interval $S \in T$ has at least $mp$ vectors is 
\[ q(N) \frac{(m-1)!}{2} / { {m \choose k} \choose m}\]

Suppose now they are distributed over $m+1$ supports. Either these $m+1$ supports contain a single valid $T$ or no valid $T$. The probability that they contain a single valid $T$ is ${m+1 \choose m} \frac{(m-1)!}{2} / { {m \choose k} \choose m} = \frac{(m+1)!}{2m} / { {m \choose k} \choose m}$.

\emph{Ideas:} The probability of filling all the intervals of length $k$ in any two cyclic orders is the same. Hence $q_{\sigma_i} = q_{\sigma_j}$ for all cyclic orders $\sigma_i, \sigma_j$. If we have filled every interval of length $k$ in all cyclic orders then we have filled all supports (right? seems right).

The number of possible ways in which there can be at least $p$ of these vectors supported on every one of $r$ subsets is the number of nonnegative integer solutions $d_i$ to the equation
\[ d_1 + \ldots + d_{{m \choose k} }= N - rp.\]

We now need the probability that these $r$ subsets contain at least one valid $T$.  This is one minus the probability that they contain no valid $T$. 

\subsection{Bounding b - PDa}

Assuming $\mathbf{b}_i$ and $PD\mathbf{a}_i$ share the same support:
\begin{align*}
|\mathbf{b}_i - PD\mathbf{a}_i| 
&\leq \frac{1}{\ell_{k}(B)}|B(\mathbf{b}_i - PD\mathbf{a}_i)| \\
&\leq \frac{1}{\ell_{k}(B)} (|B\mathbf{b}_i - A\mathbf{a}_i| + |(A - BPD)\mathbf{a}_i|) \\
&\leq \frac{\varepsilon}{\ell_{k}(B)}(1+C|\mathbf{a}_i|_1).
\end{align*}

(Can we justify this assumption?) Without this assumption,

\begin{align*}
|\mathbf{b}_i - PD\mathbf{a}_i| 
&\leq \frac{1}{\ell_{2k}(B)}|B(\mathbf{b}_i - PD\mathbf{a}_i)| \\
&\leq \frac{1}{\ell_{2k}(B)} (|B\mathbf{b}_i - A\mathbf{a}_i| + |(A - BPD)\mathbf{a}_i|) \\
&\leq \frac{\varepsilon}{\ell_{2k}(B)}(1+C|\mathbf{a}_i|_1).
\end{align*}

How do we know $\ell_{2k}(B) > 0$? For all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$,
\begin{align*}
|B\mathbf{x}|_2 
&\geq | |A(P^{-1}D^{-1}\mathbf{x})|_2 - |(A-BPD)P^{-1}D^{-1}\mathbf{x}|_2 | \\
&\geq (\ell_{2k}(A) - \sqrt{2k}C\varepsilon)|P^{-1}D^{-1}\mathbf{x}|_2 \\
&\geq (\ell_{2k}(A) - \sqrt{2k}C\varepsilon)|D|_2|\mathbf{x}|_2 \\
&\geq (\ell_{2k}(A) - \sqrt{k}\ell_2(A))|D|_2|\mathbf{x}|_2. 
\end{align*}
%
if $\ell_{2k}(A) \geq \sqrt{k}\ell_2(A)$ (see Remark \ref{thm1specs})\ldots\textbf{umm..} 

\subsection{Spark Condition implies RIP}

\begin{lemma}
If a matrix $M \in \mathbb{n \times m}$ has full column rank then $\ell(M) > 0$. Spark condition implies $\ell_k(A) > 0$. \textbf{[re-word this]}
\end{lemma}

\begin{proof}
Consider the compact set $\mathcal{C} = \{c \in \mathbb{R}^k: |c|_2 = 1\}$ and the continuous map
\begin{align*}
\phi: \mathcal{C} &\to \mathbb{R} \\
(c_1, \ldots, c_k) &\mapsto |\sum_{j = 1}^k c_j \mathbf{a}_{i_j}|_2.
\end{align*}

By general linear position of the $\mathbf{a}_i$, we know that $0 \notin \phi(\mathcal{C})$. Since $\mathcal{C}$ is compact, we have by continuity of $\phi$ that $\phi(\mathcal{C})$ is also compact; hence it is closed and bounded. Therefore $0$ can't be a limit point of $\phi(\mathcal{C})$ and there must be some $\rho > 0$ such that the neighbourhood $\{x: x < \rho\} \subseteq \mathbb{R} \setminus \phi(\mathcal{C})$. Hence $\phi(c) \geq \rho$ for all $c \in \mathcal{C}$. The result follows by the association $c \mapsto \frac{c}{|c|_2}$ and the fact that there are only finitely many subsets of $k$ vectors $\mathbf{a}_i$ (actually, for our purposes we need only consider those subsets of $k$ vectors $\mathbf{a}_i$ having the same support), hence there is some minimal $\rho$ satisfying \eqref{DataSpread} for all of them. (We refer the reader to the Appendix for a lower bound on $\rho$ given as a function of $k$ and an arithmetic sequence $\gamma_1, \ldots, \gamma_N$ used to generate the $a_i$.)
\end{proof}

%========================
%            INTRODUCTION
%========================

      
\subsection{Introduction}

We also require the data to satisfy certain properties. Consider the problem where we wish to identify the mixing matrix $A$ from the mixtures $\mathbf{y}_i$ when the sources $\mathbf{a}_i$ are known. In this case, a necessary condition for uniqueness of $A$ given $\mathbf{y}_i = A \mathbf{a}_i + \mathbf{\eta}_i$ (even when $\mathbf{\eta}_i=0$) is:
\begin{align}\label{SparkCondition2}
A^{(1)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N) = A^{(2)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N)  \text{ for all } i \in [n] \implies A^{(1)}  = A^{(2)} \indent \text{for all } A^{(1)}, A^{(2)} \in \mathbb{R}^{n \times m}.
\end{align}

Otherwise, blah blah blah (figure this out). Trying to introduce the "spread" of the data as a necessary condition.

%===================================
% PROOFS OF PROBABILISTIC THEOREMS
%===================================

\subsection{Proofs of Probabilistic Theorems}\label{PUTproof}

Definition \ref{RandomDraw}: can I bound the probability of drawing from one of the $m!$ special support sets without explicitly making that restriction?

We first generalize Lemma 3 in \cite{Hillar15} to the noisy case:

\begin{lemma}
Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{RIP}. With probability one, if $(k+1)$ $k$-sparse vectors $\mathbf{a}_i \in \mathbb{R}^{n \times m}$ are such that $d(A\mathbf{a}_i,V) \leq (??)$ for some $k$-dimensional subspace $V \subset \mathbb{R}^m$ then all of the $(k+1)$ vectors $\mathbf{a}_i$ have the same supports.
\end{lemma}
\begin{proof}
We need only show that the $k+1$ vectors $A\mathbf{a}_i$ are linearly dependent; the rest follows by Lemma 3 from \cite{Hillar15}. Let these $k+1$ vectors $\mathbf{a}_i$ be indexed by $J$ and let $W = \text{Span}\{A\mathbf{a}_{i \in J}\}$. Then for all $w \in W$ we can write $w = \sum_{i \in J} c_iA\mathbf{a}_i$ for some set of $c_i \in \mathbb{R}$. Letting $v = \sum_{i \in J} c_iv_i$, it follows that
\[ |w - v| = |\sum_{i \in J} c_i A\mathbf{a}_i - \sum_{i \in J} c_i v_i | 
\leq \sum_{i \in J} |c_i| |A\mathbf{a}_i - v_i| \leq 2\varepsilon \sum_{i \in J}|c_i| \]

Need right-hand side less thatn $|w|$ to prove that $\dim(W) \leq \dim(V) = k$\ldots
\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2} ($\varepsilon = 0$)]
Consider any alternate factorization $\mathbf{y}_i = B\mathbf{b}_i$ for $Y$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that those $\mathbf{y}$ indexed by $J(S)$ span at most a $k$-dimensional space. By Lemma 3 in \cite{Hillar15}, either $|J(S)| \leq k$ or with probability one all $\mathbf{a}_j$ with $j \in J(S)$ have the same support $S'$. Since there are only $(k+1)$ vectors $\mathbf{a}_i$ with a given support, the latter case actually implies (with probability one) that $|J(S)| = k+1$. If $N=(k+1){m \choose k}$, though, then with probability one we would reach a contradiction if $|J(S)| \leq k$ for any $S \in {[m] \choose k}$; hence with probability one we have $|J(S)| = k+1$. Can we reach the same conclusion when $N = m(k+1)$? (Perhaps by creating 'virtual data' spanning all supports by those data points with supports in $\mathcal{T}$)?

\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2}]
Let $\mathbf{y}_i = A\mathbf{a}_i + \eta_i$ for all $i \in \{1, \ldots, N\}$ and suppose there is some $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ such that $|\mathbf{y}_i - B\mathbf{b}_i| \leq \varepsilon$ for all $i \in \{1, \ldots, N\}$. Then by the triangle inequality we have $|A\mathbf{a}_i - B\mathbf{b}_i| \leq 2\varepsilon$ for all $i \in \{1, \ldots, N\}$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that for all $j \in J(S)$ there exists some $v \in \text{Span}\{B_S\}$ such that $|A\mathbf{a}_i - v| \leq 2\varepsilon$. By Lemma (??), with probability one, either $|J(S)| \leq k$ for all $\mathbf{a}_j$ with $j \in J(S)$ have the same support.
\end{proof}




\end{document}

