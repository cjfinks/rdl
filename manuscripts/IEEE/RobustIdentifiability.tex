% REFERENCE NOTES
% 'Spark' of a matrix coined and defined in "Optimally  sparse representation in  general  (non-orthogonal)
% dictionaries via L1 minimization" and applied to the study of uniqueness in "Sparse signal reconstruction % from limited data using FOCUSS:   A   re-weighted   norm   minimization   algorithm"

% QUESTIONS

\documentclass[journal, onecolumn]{IEEEtran}

% *** MATH PACKAGES ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Robust Identifiability in Sparse Coding}

\author{Charles~J.~Garfinkle,  Christopher~J.~Hillar%
\thanks{The research of Garfinkle and Hillar was conducted while at the Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA; e-mails: cjg@berkeley.edu, chillar@msri.org.  Hillar was supported, in part, by National Science Foundation grants IIS-1219212 and IIS-1219199.}}%

\maketitle

\begin{abstract}
In sparse component analysis (SCA), a dictionary is learned to linearly code a sparse mixture of signals.  It has been previously established  under mild assumptions (e.g., the spark condition from compressed sensing) that such a coding of sparse data is unique up to a relabelling and scaling of dictionary columns and sparse codes.  Often, however, data is only approximately sparse, and noise is present in the generative SCA model.  We extend the known uniqueness results to this more general setting and prove that approximately sparse datasets are still uniquely coded up to the level of noise in the input.  Our results help explain the ubiquitous finding that diverse SCA algorithms determine unique structure when applied to approximately code the same sparse mixtures.
\end{abstract}

\begin{IEEEkeywords}
Bilinear inverse problem, identifiability, dictionary learning, sparse coding, matrix factorization, compressed sensing, combinatorial linear algebra, blind source separation
\end{IEEEkeywords}

%===================================
% 			INTRODUCTION
%===================================
\section{Introduction}
\IEEEPARstart{A}{} number of modern approaches to signal processing model datasets $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ with unknown structure as lying near a collection of low-dimensional subspaces spanned by the columns $\{A_1,\ldots,A_m\}$ of a learned \textit{dictionary} matrix $A \in \mathbb R^{n\times m}$. The implied forward model is that of a sequence of linear \emph{measurements}:
\begin{align}\label{LinearModel}
\mathbf{y} = A\mathbf{a} + \mathbf{n}
\end{align}
%
with the underlying \emph{sources} $\mathbf{a}_i \in \mathbb{R}^{m}$ each having few nonzero entries -- say, no more than $k$ (these vectors are called $k$-\emph{sparse}) -- and with the so-called \emph{noise} vector being of bounded energy $|\mathbf{n}|_2 \leq \varepsilon$. The noise accounts for uncertainty both due to measurement inaccuracy and the degree to which the sparse linear model fails to describe the true nature of the data.

We consider the well-posedness of the associated inverse problem: 
\begin{align}\label{InverseProblem}
\text{Find $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ such that $|\mathbf{y}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for $i = 1, \ldots, N$.}
\end{align}

At first glance \eqref{InverseProblem} is clearly not well-posed, as even when $\varepsilon = 0$ there are many possible solutions. Let $Y$ and $X$ be the matrices with columns $\mathbf{y}_i$ and $\mathbf{a}_i$, respectively, so that $Y = AX$ for $X$ with $k$-sparse columns. We then also have $Y = BX'$ for $X'$ with $k$-sparse columns whenever $B = AD^{-1}P^{-1}$ and $X' = PDX$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. Thus, the ``uniqueness" of any solution can only be defined up to an equivalence of mappings of the form $PD$, which together constitute an \emph{ambiguity transform group} \cite{BilinInv} associated to this particular bilinear inverse problem. The question of when every solution $Y = BX'$ must necessarily be part of this equivalence class (i.e., $X' = PDX$ and $B = AD^{-1}P^{-1}$ for some $P$, $D$) was addressed in the theoretical works \cite{Georgiev05, Aharon06, Hillar15} and determines whether $A$ and $X$ are \emph{identifiable} from $Y$ up to this inherent ambiguity. 

It remains to determine, however, the extent to which $A$ and $X$ can be determined up to these permutation-scaling ambiguities when $\mathbf{n} \neq 0$. In particular, is the inference stable with respect to noise? To address this, we consider the following \emph{robust} extension to the notion of identifiability in dictionary learning.

\begin{definition}\label{Uniqueness}
A dataset $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ has a \textbf{$k$-sparse representation} in $\mathbb{R}^m$ if for some dictionary $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$, we have $\mathbf{y}_i = A\mathbf{a}_i$ for $i = 1, \ldots, N$. We say this representation is \textbf{robustly identifiable} if for every $\delta_1, \delta_2 \geq 0$ there exists a nonnegative $\varepsilon = \varepsilon(\delta_1, \delta_2)$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ satisfy:
\[|\mathbf{y}_i - B\mathbf{b}_i|_2 \leq \varepsilon,\indent \text{ for } i = 1, \ldots, N,\]
%
then there necessarily exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ with
\begin{align}\label{def1}
|A_j - (BPD)_j|_2 \leq \delta_1 \ \ \text{and} \ \ |\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i|_1 \leq \delta_2 \  \ \text{ for } \ \ j = 1, \ldots, m; \ \ i = 1, \ldots, N.
\end{align}
\end{definition}

\begin{problem}\label{DUTproblem}
Let $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N \} \subset \mathbb{R}^n$ be generated as $\mathbf{y}_i = A\mathbf{a}_i$ for some matrix $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_i \in \mathbb{R}^m$. When does $Y$ have a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$?
\end{problem}

It is easy to see how a solution to Problem \ref{DUTproblem} directly informs the interpretation of any solution to \eqref{InverseProblem}. Let $\delta_1, \delta_2 > 0$ and set $\varepsilon = \frac{1}{2}\varepsilon(\delta_1, \delta_2)$, where $\varepsilon(\delta_1, \delta_2)$ is as in Def.~\ref{Uniqueness}. Since by \eqref{LinearModel} we have that $|\mathbf{y}_i - A\mathbf{a}_i|_2 \leq \varepsilon$ for $i = 1, \ldots, N$, by the triangle inequality and robust identifiability, any solution to \eqref{InverseProblem} necessarily satisfies \eqref{def1} for some $P$ and $D$.

Our main finding is that if the matrix $A \in \mathbb{R}^{n \times m}$ is injective on the set of all $k$-sparse vectors: 
\begin{align}\label{SparkCondition}
A\mathbf{a}_1 = A\mathbf{a}_2 \implies \mathbf{a}_1 = \mathbf{a}_2, \ \ \text{ for all $k$-sparse } \mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^m,
\end{align}
%
known in the literature as the \emph{spark condition}, then the $k$-sparse representation of $Y$ in Problem \ref{DUTproblem} is robustly identifiable provided the data are sufficiently diverse. Note that this condition on the matrix $A$ is necessary given only the knowledge that the $\mathbf{a}_i$ are $k$-sparse. 

Before stating our main result, we note how the spark condition relates to the \emph{lower bound} \cite{grcar2010matrix} of a matrix $A$, i.e. the largest number $\alpha$ such that $|A\mathbf{x}|_2 > \alpha|\mathbf{x}|_2$ for all $\mathbf{x}$. One can show by a simple compactness argument that every injective linear map has a nonzero lower bound; hence if $A$ satisfies the spark condition then every submatrix formed from $2k$ of its columns has a nonzero lower bound. We therefore define the following domain-restricted generalization of the lower bound of a matrix:
\begin{align}
L_k(A) := \max \{ \alpha : |A\mathbf{a}|_2 \geq \alpha|\mathbf{a}|_2 \text{ for all $k$-sparse } \mathbf{a} \in \mathbb{R}^m\}.
\end{align} 
Clearly, $L_k(A) \geq L_{k'}(A)$ whenever $k < k'$ and $L_{2k}(A) > 0$ for any $A$ satisfying \eqref{SparkCondition}. 

Recall that a \textit{cyclic order} on $[m] := \{1, \ldots,m\}$ is an arrangement of $[m]$ in a circular necklace, and that an \textit{interval} is a contiguous sequence of such elements.  

%=== STATEMENT OF DETERMINISTIC UNIQUENESS THEOREM ===%
\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix positive integers $n, m$, $k < m$, and a cyclic order on $[m]$. If $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in the cyclic order there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ in general linear position supported on that interval and $A \in \mathbb{R}^{n \times m}$ satisfies spark condition \eqref{SparkCondition} then $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robustly identifiable $k$-sparse representation.

Specifically, there exists a constant $C > 0$ for which the following holds for all $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$. If $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ are such that $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [N]$ then:
\begin{align}\label{Cstable}
|A_j-(BPD)_j|_2 \leq C\varepsilon,\indent  \text{for all } j \in [m].
\end{align}
If, moreover, we have $\varepsilon < \varepsilon_0$, where $\varepsilon_0 = \frac{L_{2k}(A)}{\sqrt{2k}}C^{-1}$, then $B$ also satisfies the spark condition and:
\begin{align}\label{b-PDa}
|\mathbf{a}_i - D^{-1}P^{-1}\mathbf{b}_i|_1 &\leq \frac{\varepsilon }{ \varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right),\indent  \text{for all } i \in [N].
\end{align}
\end{theorem}
[ *** Haven't changed the definition of $\varepsilon_0$ yet, nor the proofs, to reflect this new way of stating the Theorem *** ]
[ *** We haven't shown that $\text{supp}(\mathbf{b}_i) = \text{supp}(\mathbf{a}_i)$. Is this a problem? *** ]
An important consequence of this result is that for sufficiently small reconstruction error, the original dictionary and sparse vectors are determined up to a commensurate error, independent of alternate representation.  (In other words, we may choose $\varepsilon$ on the same order as $\delta_1$ or $\delta_2$ in Def.~\ref{Uniqueness}.) The constant $C$ is defined in terms of $A$ and the $\mathbf{a}_i$ in \eqref{Cdef}, below. 

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given positive integers $n, m$, and $k < m$, there exist $N =  m(k-1){m \choose k}+m$ vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a robustly identifiable $k$-sparse representation.
\end{corollary}

Our proof of Theorem \ref{DeterministicUniquenessTheorem} is a delicate refinement of the arguments in \cite{Hillar15} to handle measurement error.  Here, we also reduce the theoretically required number of samples given by \cite{Hillar15} from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$, and we provide guarantees for the case when only an upper bound on the number of columns in $A$ is known (see Theorem \ref{DeterministicUniquenessTheorem2} below). 

Given Theorem \ref{DeterministicUniquenessTheorem}, it is straightforward to provide probabilistic extensions (Theorems \ref{Theorem2} and \ref{Theorem3}) by drawing on a key result from the recently emergent field of compressed sensing (CS) \cite{candes2006near, donoho2006compressed, candes2006stable}. Briefly, CS theory describes conditions under which a signal $\mathbf{x} \in \mathbb{R}^n$ that is sparse enough in some known basis (i.e. $\mathbf{s} = \Psi \mathbf{x}$ is $k$-sparse for some invertible $\Psi$) can be feasibly recovered after it has been linearly subsampled as $\mathbf{y} = \Phi \mathbf{x} + \mathbf{n}$ by a known compression matrix $\Phi \in \mathbb{R}^{n \times m}$ with additive noise $\mathbf{n}$. The underlying logic is simple: if the generation matrix $A = \Phi\Psi$ satisfies the spark condition \eqref{SparkCondition} then $\mathbf{s}$ is robustly identifiable given $\mathbf{y}$ and the signal $\mathbf{x}$ can then be approximately reconstructed as $\Psi^{-1}\mathbf{s}$. The key result we refer to is that a random\footnote{[*** @chris: Should we cite Baraniuk again here? ***]} compression matrix $\Phi$ yields an $A$ satisfying the spark condition with probability one provided the dimension $n$ of $\mathbf{y}$ satisfies:
\begin{align}\label{CScondition}
n \geq \alpha k\log\left(\frac{m}{k}\right),
\end{align}
where $\alpha$ is a positive constant dependent on the distribution from which the entries of $\Phi$ are drawn. [*** does this depend on $\Psi$? ***] [ *** Isn't it the case that for $n \times m$ matrices with entries drawn from i.i.d. continuous distributions we have spark(A) = n+1 ? Should we say that instead? ***] But what if we don't know $\Phi$? Our theorems demonstrate that the matrix $A$ can still be estimated up to noise and an inherent permutation-scaling ambiguity by examining a sufficiently diverse set of samples $\mathbf{y}_1, \ldots, \mathbf{y}_N$. The signals $\mathbf{x}_i$ can then be reconstructed up to the degeneracy imparted by these inherent ambiguities as well. 

We state here the main implications of our probabilistic results. To keep our exposition simple, our statements are based upon the following elementary construction of \emph{random sparse vectors} (although many ensembles will suffice, e.g. \cite[Sec.~\S 4]{baraniuk2008simple}).

\begin{definition}[Random $k$-Sparse Vectors]\label{RandomDraw}
Given the support set for its $k$ nonzero entries, a \textbf{random draw} of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all ${m \choose k}$ of them and then a random draw.
\end{definition}

\begin{corollary}
Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing such an $A$, we have with probability one that a dataset $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N\}$ generated from a random draw of $N = m(k-1){m \choose k}+m$ $k$-sparse vectors $\mathbf{a}_i$, consisting of $(k-1){m \choose k}+1$ samples supported on each interval of length $k$ in some cyclic ordering of $[m]$, has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

\begin{corollary}
Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}.  If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval then, with probability one, almost every matrix $A \in \mathbb{R}^{n \times m}$ gives a robustly identifiable $Y = \{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$.
\end{corollary}

The organization of the rest of this paper is as follows. In Section \ref{Preliminaries} we list additional definitions and key lemmas, including our main tool from combinatorial matrix theory (Lemma \ref{MainLemma}). In Section \ref{DUT}, we derive Theorem \ref{DeterministicUniquenessTheorem}. In Section \ref{mleqm}, we describe how to extend these results to the case where we have only an upper bound on the dimensionality of the sparse sources (or, equivalently, the number of dictionary elements). In Section \ref{PUT}, we state and prove Theorems \ref{Theorem2} and \ref{Theorem3}, our probabilistic versions of Theorem \ref{DeterministicUniquenessTheorem}. The final section is a discussion, and an appendix contains the proof of Lemma \ref{MainLemma}.

%===================================
% 			Preliminaries
%===================================
\section{Preliminaries}\label{Preliminaries}
In this section, we briefly review standard definitions and outline our main tools, which include general notions of angle (Def.~\ref{FriedrichsDefinition}) and distance (Def.~\ref{GapMetricDef}) between vector subspaces as well as a robust uniqueness result in combinatorial matrix theory (Lemma~\ref{MainLemma}).
Let ${[m] \choose k}$ be the collection of subsets of $[m] := \{1,\ldots,m\}$ of cardinality $k$, and let $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ for real vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$ be the vector space consisting of their $\mathbb{R}$-linear span.  For a matrix $M$, the spectral norm is denoted $\|M\|_2$.
Also, given $S \subseteq [m]$ and $M \in \mathbb{R}^{n \times m}$ with columns $\{M_1,\ldots,M_m\}$, we define $M_S$ to be the submatrix with columns $\{M_j: j \in S\}$ and also set $\text{Span}\{M_S\} := \text{Span}\{M_j : j \in S\}$.  

Between any pair of subspaces in Euclidean space one can define the following generalized ``angle'':
\begin{definition}\label{FriedrichsDefinition}
The \textbf{Friedrichs angle} $\theta_F = \theta_F(U,V) \in [0,\frac{\pi}{2}]$ between subspaces $U,V \subseteq \mathbb{R}^n$ is defined in terms of its cosine as \cite{Deutsch}:
\begin{align}
\cos{\theta_F} := \max\left\{ \langle u, v \rangle: u \in U \cap (U \cap V)^\perp \cap \mathcal{B}, \, v \in V \cap (U \cap V)^\perp \cap \mathcal{B}, \right\}.
\end{align}
%
where $\mathcal{B} = \{ x: |x|_2 \leq 1\}$ is the unit $\ell_2$-ball in $\mathbb{R}^n$.
\end{definition}
For example, for $n=3$ and $k=1$ this is simply the angle between vectors, and for $k=2$ it is the angle between the normal vectors of two planes. In higher dimensions, the Friedrichs angle is one out of a set of \textit{principal} (or \textit{canonical} or \textit{Jordan}) angles between subspaces which are invariant to orthogonal transformations. These angles and are all zero if and only if one subspace is a subset of the other; otherwise, the Friedrichs angle is the smallest nonzero such angle. 

The next definition we need is based on a quantity derived in \cite{Deutsch} to describe the convergence of the alternating projections algorithm for projecting a point onto the intersection of a set of subspaces. We use it to bound the distance between a point and the intersection of a set of subspaces given an upper bound on the distance from that point to each individual subspace.

\begin{definition}\label{SpecialSupportSet}
Fix $A \in \mathbb{R}^{n \times m}$ and $k < m$. Setting $\phi_1(A) := 1$, define for $k \geq 2$:
\begin{align}\label{rho}
\phi_k(A) := \min_{ S_1,\ldots,S_k \in {[m] \choose k} } 1 - \xi( \text{\rm Span}\{A_{S_1}\}, \ldots,  \text{\rm Span}\{A_{S_k}\}),
\end{align}
where for any set $\mathcal{V} = \{V_1, \ldots, V_k\}$ of closed subspaces of $\mathbb{R}^m$, 
\begin{align}
\xi(\mathcal{V}) := \min_{\sigma \in \frak{S}_k} \left(1 - \prod_{i=1}^{k-1} \sin^2  \theta_F \left(V_{\sigma(i)}, V_{\sigma(i+1)} \cap \cdots \cap V_{\sigma(k)}\right)  \right)^{1/2},
\end{align}
%
and $\frak{S}_{k}$ is the set of permutations (i.e. bijections) on $k$ elements. 
\end{definition}

%=== SPECIFICS OF DETERMINISTIC THEOREM ===%
We are now in a position to state explicitly the constant $C$ referred to in Theorem \ref{DeterministicUniquenessTheorem}. Letting $X$ be the $m \times N$ matrix with columns $\mathbf{a}_i$, $I(S) := \{i : \text{supp}(\mathbf{a}_i) = S\}$, and $T$ be the set of intervals of length $k$ in a cyclic order of $[m]$, we have:
\begin{align}\label{Cdef}
C = \left( \frac{ \sqrt{k^3}}{ \phi_k(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in T} L_k(AX_{I(S)})}, 
\end{align}

\begin{remark}\label{nonzero}
We can be sure that $C$ is well-defined provided $\min_{S \in T} L_k(AX_{I(S)}) > 0$, since $\phi_k(A) = 0$ only when $\text{Span}(A_{S_1}) \supseteq \text{Span}(A_{S_2}) \cap \ldots, \cap \text{Span}(A_{S_k})$ for some $S_1, \ldots, S_k \in {[m] \choose k}$, which violates the spark condition on $A$.
\end{remark}

\begin{definition}\label{GapMetricDef}
Let $U, V$ be subspaces of $\mathbb{R}^m$ and let $d(u,V) := \inf\{|u-v|_2: v \in W\} = |u - \Pi_V u|_2$ where $\Pi_V$ is the orthogonal projection operator onto subspace $V$. The \textbf{gap metric} $\Theta$ on subspaces of $\mathbb{R}^{m}$ is \cite{akhiezer2013theory}:
\begin{equation}\label{SubspaceMetric}
\Theta(U,V) := \max\left( \sup_{\substack{u \in U, \, |u|_2 = 1}} d(u,V), \sup_{\substack{v \in V, \, |v|_2 = 1}} d(v,U) \right).
\end{equation}
\end{definition}

\begin{remark}
It can be shown that $\Theta(U,V)$ is the sine of the largest Jordan angle between $U$ and $V$.
\end{remark}

We are now state our main result from combinatorial matrix theory, generalizing \cite[Lemma 1]{Hillar15} to the noisy case.

%===========          MAIN LEMMA (K > 1)             =================
\begin{lemma}[Main Lemma]\label{MainLemma}
Fix positive integers $n, m$, $k < m$, and let $T$ be the set of intervals of length $k$ in some cyclic ordering of $[m]$. Let $A, B \in \mathbb{R}^{n \times m}$ and suppose that $A$ satisfies the spark condition \eqref{SparkCondition} with maximum column $\ell_2$-norm $\rho$.  If there exists a map $\pi: T \to {[m] \choose k}$ and some $\delta < \frac{L_{2}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound}
\Theta(\text{\rm Span}\{A_{S}\}, \text{\rm Span}\{B_{\pi(S)}\}) \leq \frac{ \phi_k(A) }{\rho k} \delta, \indent \text{for all } S \in T,
\end{equation}
%
then there exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and an invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ with
\begin{align}
|A_j - (BPD)_j|_2 \leq \delta, \indent \text{for all } j \in [m].
\end{align}
\end{lemma}
We defer the (technical) proof of this lemma to Appendix \ref{appendixA}. In words, the result says that the vectors forming the columns of $A$ are nearly identical to those forming the columns of $B$ (up to symmetry) provided that for a special set $T$ of $m$ subspaces, each spanned by $k$ columns of $A$, there exist $k$ columns of $B$ which span a nearby subspace with respect to the gap metric.

In our proof of robust identifiability, we will also use the following useful facts about the distance $d$ from Def.~\ref{GapMetricDef}. The first, 
\begin{equation}\label{SubspaceMetricSameDim}
\dim(W) = \dim(V) \implies \sup_{\substack{v \in V, |v|_2 = 1}}  d(v,W)  = \sup_{\substack{w \in W, |w|_2 = 1}} d(w,V),
\end{equation}
can be found in \cite[Lemma 3.3]{Morris10}. The second is:
\begin{lemma}\label{MinDimLemma}
If $U, V$ are subspaces of $\mathbb{R}^{m}$, then
\begin{equation}\label{MinDim}
d(u,V) < |u|_2 \ \ \text{\rm for all } u \in U \setminus{\{0\}} \implies \dim(U) \leq \dim(V).
\end{equation}
\end{lemma}

\begin{proof}
We prove the contrapositive.  If $\dim(U) > \dim(V)$, then a dimension argument ($\dim U + \dim V^\perp > m$) gives a nonzero $u \in U \cap V^\perp$.  In particular, we have $|u - v|_2^2 = |u|_2^2 + |v|_2^2 \geq |u|_2^2$ for all $v \in V$, and thus $d(u,V) \geq |u|_2$.
\end{proof}

%===================================
% 			Deterministic Identifiability
%===================================
\section{Deterministic Identifiability}\label{DUT}

%======== CASE K = 1 ============
Let $\mathbf{e}_1, \ldots, \mathbf{e}_m$ be the standard basis vectors in $\mathbb R^m$.
Before proving Theorem \ref{DeterministicUniquenessTheorem} in full generality, consider when $k=1$. Suppose we have $N = m$ $1$-sparse vectors $\mathbf{a}_j = c_j \mathbf{e}_j$ for $c_j \in \mathbb{R} \setminus \{0\}$, $j \in [m]$. Fix some $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} and let $\rho = \max_{i \in [m]} |A\mathbf{e}_i|_2$. By \eqref{Cdef} we have:
\begin{align}\label{C1}
C = \left( \frac{ \sqrt{k^3}}{ \phi_1(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{i \in [m]} \ell_1(c_iA_i) }
= \sqrt{k^3} \left( \frac{\max_{j \in [m]} |A_j|_2}{\min_{i \in [m]}|c_iA_i|_2} \right)
\geq \max_{i \in [m]}|c_i|^{-1}, 
\end{align}

Suppose that for some $B \in \mathbb{R}^{n \times m}$ and 1-sparse $\mathbf{b}_i \in \mathbb{R}^m$ we have  $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ for all $i \in [m]$. Since the $\mathbf{b}_i$ are 1-sparse, there must exist $c'_1, \ldots, c'_m \in \mathbb{R}$ and some map $\pi: [m] \to [m]$ such that 
\begin{align}\label{1D}
|c_iA_i - c'_iB_{\pi(i)}|_2 \leq \varepsilon \ \ \text{for all} \  i \in [m].
\end{align} 
Note that $c'_i \neq 0$ for all $i$ since then otherwise (by definition of $L_2(A)$) we reach the contradiction $|c_iA_i|_2 < \min_{i \in [m]}|c_iA_i|_2$. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m]$. Then, $|c_iA_i - c'_iB_{\ell}|_2  \leq \varepsilon$ and $|c_jA_j - c'_jB_{\ell}|_2 \leq \varepsilon$. Using the triangle inequality followed by application of the definition of $L_2$, we have:
\begin{align*}
(|c'_i| + |c'_j|) \varepsilon
&\geq |c'_j||c_iA_i - c'_iB_{\ell}|_2 + |c'_i||c'_jB_{\ell} - c_jA_j|_2 \\
&\geq |c'_jc_iA_i - c'_ic_jA_j|_2 \\
&\geq L_2(A)|c'_jc_i\mathbf{e}_i - c'_ic_j\mathbf{e}_j|_2 \\
&\geq \frac{L_2(A)}{\sqrt{2}}|c'_jc_i\mathbf{e}_i - c'_ic_j\mathbf{e}_j|_1 \\
&\geq \frac{L_2(A)}{\sqrt{2}} \left( |c'_j| + |c'_i| \right) \min_{\ell \in [m]} |c_\ell | \\
\end{align*}
%
which is in contradiction with \eqref{C1} and our upper bound on $\varepsilon$. Hence, $\pi$ is injective. Letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{c'_1}{c_1},\ldots,\frac{c'_m}{c_m})$, we see that \eqref{1D} becomes 
\begin{align}
|(A - BPD)_i|_2 = |A_i - \frac{c'_i}{c_i}B_{\pi(i)}|_2 \leq \frac{\varepsilon}{|c_i|} \leq C\varepsilon \ \ \text{for all } i \in [m].
\end{align}

\begin{remark}
We demonstrate with the following counter-example that \eqref{Cstable} with $C$ as defined in \eqref{Cdef} is not necessarily implied if $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [m]$ only holds for $\varepsilon \geq \frac{L_2(A)}{\sqrt{2}}C^{-1}$. Consider the dataset $\mathbf{a}_i = \mathbf{e}_i$ for $i = 1, \ldots, m$ and let $A$ be the identity matrix in $\mathbb{R}^{m \times m}$. Then $\varepsilon \geq 1/\sqrt{2}$ since $L_2(A) = 1$ (we have $|A\mathbf{x}|_2 = |\mathbf{x}|_2$ for all $\mathbf{x} \in \mathbb{R}^m$) and $C = 1$. Consider the alternate dictionary $B = \left(\mathbf{0}, \frac{1}{2}(\mathbf{e}_1 + \mathbf{e}_2), \mathbf{e}_3, \ldots, \mathbf{e}_{m} \right)$ and let $\mathbf{b}_i = \mathbf{e}_2$ for $i = 1, 2$ and $\mathbf{b}_i = \mathbf{e}_i$ for $i = 3, \ldots, m$. Then $|A\mathbf{a}_i - B\mathbf{b}_i| = 1/\sqrt{2}$ for $i = 1, 2$ and $0$ otherwise, i.e. the smallest $\varepsilon$ for which $|A\mathbf{a}_i - B\mathbf{b}_i| \leq \varepsilon$ for all $i \in [m]$ is $\varepsilon = 1/\sqrt{2}$. If there were permutation and diagonal matrices $P \in \mathbb{R}^{m \times m}$ and $D \in \mathbb{R}^{m \times m}$ such that $|(A-BPD)\mathbf{e}_i| \leq C\varepsilon$ for all $i \in [m]$, then we would reach the contradiction $1 = |P^{-1}\mathbf{e}_1|_2 = |(A-BPD)P^{-1}\mathbf{e}_1|_2 \leq 1/\sqrt{2}$.
\end{remark}

% ======== b - PDa =========
\begin{remark}\label{b-PDaProof}
From this result we can, in general, bound $|\mathbf{a}_i - D^{-1}P^{-1}\mathbf{b}_i|_1$ as well. Specifically, we will show that \eqref{b-PDa} always follows from \eqref{Cstable} when $\varepsilon < \varepsilon_0$, with $\varepsilon_0 = \frac{L_{2k}}{\sqrt{2k}}C^{-1}$. Note that for all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$ we have by the triangle inequality:
\begin{align*}
|(A-BPD)\mathbf{x}|_2 
\leq C\varepsilon|\mathbf{x}|_1
\leq C \varepsilon \sqrt{2k}  |\mathbf{x}|_2.
\end{align*}

Thus,
\begin{align*}
|BPD\mathbf{x}|_2 
\geq | |A\mathbf{x}|_2 - |(A-BPD)\mathbf{x}|_2 |
\geq (L_{2k}(A) - \sqrt{2k}C\varepsilon ) |\mathbf{x}|_2,
\end{align*}
%
where for the last inequality it was admissible to drop the absolute value since $\varepsilon < \varepsilon_0$. Hence, $L_{2k}(BPD) \geq L_{2k}(A) - C\varepsilon \sqrt{2k} > 0$ and it follows that:
\begin{align*}
|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i|_1
&\leq \sqrt{2k} |\mathbf{a}_i - D^{-1}P^{-1}\mathbf{b}_i|_2 \\
&\leq \frac{\sqrt{2k}}{L_{2k}(BPD)}|BPD(\mathbf{a}_i - D^{-1}P^{-1}\mathbf{b}_i)|_2 \\
&\leq \frac{\sqrt{2k}}{L_{2k}(BPD)} (|B\mathbf{b}_i - A\mathbf{a}_i|_2 + |(A - BPD)\mathbf{a}_i|_2) \\
&\leq \frac{\varepsilon\sqrt{2k}}{L_{2k}(BPD)}(1+C|\mathbf{a}_i|_1) \\
&\leq \frac{\varepsilon\sqrt{2k}(1+C|\mathbf{a}_i|_1)}{L_{2k}(A) - C\varepsilon\sqrt{2k}} \\
&= \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right).
\end{align*}
\end{remark}

% ========== DEFINITIONS FOR MAIN LEMMA (K>1) ================
It remains to show that \eqref{Cstable} with $C$ given in \eqref{Cdef} follows from $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ for $k > 1$. Our main tool for the proof is Lemma \ref{MainLemma}.

%========          PROOF OF THEOREM 1        ============
\begin{proof}[Proof of Theorem \ref{DeterministicUniquenessTheorem} and Corollary \ref{DeterministicUniquenessCorollary}]
Let $T$ be the set of intervals of length $k$ in the given cyclic order of $[m]$.  From above, we may assume that $k > 1$. The first step is to produce a set of $N = m(k-1){m \choose k}+m$ vectors in $\mathbb{R}^k$ in general linear position (i.e., any $k$ of them are linearly independent). Specifically, let $\gamma_1, \ldots, \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Next, form the $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with supports $S \in T$ (partitioning the $a_i$ evenly among these supports so that each support contains $(k-1){m \choose k}+1$ vectors $a_i$) by setting the nonzero values of vector $\mathbf{a}_i$ to be those contained in the $i$th column of $V$.

Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. We claim that $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$. Suppose that for some $B \in \mathbb{R}^{n \times m}$ there exist $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ such that $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [N]$. Since there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ with a given support $S \in T$, the pigeon-hole principle implies that there exists some $S' \in {[m] \choose k}$ and some set of indices $I(S)$ of cardinality $k$ such that all $\mathbf{a}_i$ and $\mathbf{b}_i$ with $i \in J(S)$ [*** @chris: keep this as J(S) not I(S). We use I(S) later ***] have supports $S$ and $S'$, respectively.

Let $X$ and $\tilde{X}$ be the $m \times N$ matrices with columns $\mathbf{a}_i$ and $\mathbf{b}_i$, respectively. It follows from the general linear position of the $\mathbf{a}_i$ and the linear independence of every $k$ columns of $A$ that the columns of the $n \times k$ matrix $AX_{J(S)}$ are linearly independent, i.e. $L(AX_{J(S)}) > 0$, and form a basis for $\text{Span}\{A_{S}\}$. Fixing $\mathbf{z} \in \text{Span}\{A_{S}\}$, there then exists a unique $\mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{z} = AX_{J(S)}\mathbf{c}$. Letting $\mathbf{z'} = BX'_{J(S)}\mathbf{c}$, which is in $\text{Span}\{B_{S'}\}$, we have:
\begin{align*}
|\mathbf{z} - \mathbf{z'}|_2 = |\sum_{j=1}^N c_i(AX_{J(S)} - BX'_{J(S)})\mathbf{e}_j|_2 
\leq \varepsilon \sum_{j=1}^N |c_j| 
\leq \varepsilon |\mathbf{c}|_2 \sqrt{k}  
\leq \frac{\varepsilon \sqrt{k}}{L(AX_{JS)})} |AX_{J(S)}\mathbf{c}|_2
= \frac{\varepsilon \sqrt{k}}{L(AX_{J(S)})} |\mathbf{z}|_2.
\end{align*}

Hence,
\begin{align}\label{ABSubspaceDistance}
\sup_{ \substack{ \mathbf{z} \in \text{Span}\{A_{S}\} \\ |\mathbf{z}|_2 = 1} } d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \frac{\varepsilon\sqrt{k}}{L(AX_{J(S)})}.
\end{align}

We now show that \eqref{Cstable} follows if $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$, with $C$ as defined in \eqref{Cdef}. In this case we can bound the RHS of \eqref {ABSubspaceDistance} as follows: 
\begin{align}\label{rhs}
\frac{\varepsilon\sqrt{k}}{L(AX_{J(S)})} 
<  \frac{\phi_k(A) L_2(A)}{\rho k \sqrt{2}} \left( \frac{\min_{S \in T}L_k(AX_{I(S)})}{L(AX_{J(S)})} \right)
\leq \frac{\phi_k(A)}{\rho k} \left( \frac{L_2(A)}{\sqrt{2}} \right),
\end{align}
%
where $I(S) = \{i: \text{supp}(\mathbf{a}_i)=S\}$. Since $L_2(A) \leq \rho \sqrt{2}$ and $\phi_k(A) \in [0,1]$, we have that the RHS of \eqref{ABSubspaceDistance} is strictly less than one. It follows by Lemma \ref{MinDimLemma} that $\dim(\text{Span}\{B_{S'}\}) \geq \dim(\text{Span}\{A_{S}\}) = k$ (since every $k$ columns of $A$ are linearly independent). Since $|S'| = k$, we have $\dim(\text{Span}\{B_{S'}\}) \leq k$, hence $\dim(\text{Span}\{B_{S'}\}) = \dim(\text{Span}\{A_{S}\})$. Recalling \eqref{SubspaceMetricSameDim},  we see the association $S \mapsto S'$ thus defines a map $\pi: T \to {[m] \choose k}$ satisfying
\begin{align}\label{yeyeye}
\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{\varepsilon\sqrt{k}}{L(AX_{J(S)})} \indent \text{for all } S \in T.
\end{align}

From \eqref{rhs} and \eqref{yeyeye} we see that the inequality $\Theta(\text{Span}\{A_{S}\}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{ \phi_k(A) }{\rho k} \delta$ is satisfied for this support $S$ and $\delta < \frac{L_2(A)}{\sqrt{2}}$ by setting $\delta = \frac{ \rho k}{ \phi_k(A) } \left(  \frac{\varepsilon \sqrt{k}}{L(AX_{J(S)})} \right)$ (see Remark \ref{nonzero} for why $\phi_k(A) \neq 0$). We therefore satisfy \eqref{GapUpperBound} for 
\begin{align}
\delta = \left( \frac{ \varepsilon \sqrt{k^3}}{ \phi_k(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in T} L_k(AX_{I(S)})}
= C\varepsilon
\end{align}

It follows by Lemma \ref{MainLemma} that there exists a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that for all $j \in [m]$,
$|A_j - (BPD)_j|_2 \leq C\varepsilon$. The proof is completed by the arguments in Remark \ref{b-PDaProof}, which show how \eqref{b-PDa} follows from this result.
\end{proof}

%===================================
% DIFFERENT CODING DIMENSIONS
%===================================
\section{Unknown representation dimension}\label{mleqm}

In this section we state a version of Theorem \ref{DeterministicUniquenessTheorem} and Lemma \ref{MainLemma} assuming that $B$ also satisfies the spark condition (in addition to $A$ satisfying the spark condition). With this additional assumption, we can address the issue of recovering $A \in \mathbb{R}^{n \times m}$ and the $\mathbf{a}_i \in \mathbb{R}^m$ when only an upper bound $m'$ on the number $m$ of sparse sources is known. 
\begin{theorem}\label{DeterministicUniquenessTheorem2}
Fix positive integers $n, m, m'$, and $k$ with $k < m \leq m'$. Fix $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ and let $X$ be the $m \times N$ matrix with columns given by $\mathbf{a}_i$. Let $I(S) = \{i : \text{supp}(\mathbf{a}_i) = S\}$ and let
\begin{align}\label{Cdef'}
C' = \left( \frac{ \sqrt{k^3}}{ \min(\phi_k(A), \phi_k(B)) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in T} L_k(AX_{I(S)})}.
\end{align}

Let $T$ be the set of all length $k$ intervals in some cyclic ordering of $[m]$. If the $\mathbf{a}_i$ are such that for every $S \in T$ there are $(k-1){m' \choose k}+1$ vectors $\mathbf{a}_i$ in general linear position supported on $S$ then the following holds. Every pair of matrices $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfying spark condition \eqref{SparkCondition} are such that if $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon < \frac{L_2(A)}{\sqrt{2}}C'^{-1}$ for all $i \in [N]$ for some $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^{m'}$ then $|A_j-(BPD)_j|_2 \leq C'\varepsilon$ for all $j \in [m]$ for some partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ (there is at most one nonzero entry in each row and column and these nonzero entries are all one) and diagonal matrix $D \in \mathbb{R}^{m' \times m}$ ($D_{ij} = 0$ whenever $i \neq j$).
\end{theorem}

The proof of Theorem \ref{DeterministicUniquenessTheorem2} is very similar to the proof of Theorem \ref{DeterministicUniquenessTheorem}, the difference being that now we establish a map $\pi: [m] \to [m']$ satisfying the requirements of Lemma \ref{MainLemma2}, which we state next, by pigeonholing $(k-1){m' \choose k} + 1$ vectors with respect to holes $[m']$. This insures that we can establish a one-to-one correspondence between subspaces spanned by the $m'$ columns of $B$ and nearby subspaces spanned by the $m$ columns of $A$ despite the fact that $m < m'$. By requiring $B$ to also satisfy the spark condition, we remove the dependency of Lemma \ref{MainLemma} on Lemma \ref{NonEmptyLemma} (which requires that $m = m'$), resulting in Lemma \ref{MainLemma2}.

\begin{lemma}[Main Lemma for $m < m'$]\label{MainLemma2}
Fix positive integers $n, m, m'$, and $k$ where $k < m < m'$, and let $T$ be the set of intervals of length $k$ in some cyclic ordering of $[m]$. Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfy spark condition \eqref{SparkCondition} with $A$ having maximum column $\ell_2$-norm $\rho$. If there exists a map $\pi: T \to {[m'] \choose k}$ and some $\delta < \frac{L_{2}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound2}
\Theta(\text{\rm Span}\{A_{S}\}, \text{\rm Span}\{B_{\pi(S)}\}) \leq \frac{ \delta }{\rho k} \min(\phi_k(A), \phi_k(B)), \indent \text{for all } S \in T,
\end{equation}
%
then there exist a partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and a diagonal matrix $D \in \mathbb{R}^{m' \times m}$ such that
\begin{align}
|A_j - (BPD)_j|_2 \leq \delta, \indent \text{for all } j \in [m].
\end{align}
\end{lemma}

We again defer the proof of this lemma to Appendix \ref{mleqmAppendix}.


%======================================
% PROBABILISTIC THEOREMS
%======================================
\section{Probabilistic Identifiability}\label{PUT}

We next give precise statements of our probabilistic versions of Theorem \ref{DeterministicUniquenessTheorem}, which apply to random sparse vectors as defined in Definition \ref{RandomDraw}. Our brief proofs rely largely on the following lemma, the proof of which can be found in \cite[Lemma 3]{Hillar15}:
\begin{lemma}\label{Hillar15lemma2}
Fix positive integers $n, m$, $k < m$, and a matrix $M \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. With probability one, $M\mathbf{a}_1, \ldots, M\mathbf{a}_k$ are linearly independent whenever the $\mathbf{a}_i$ are random $k$-sparse vectors.
\end{lemma}

\begin{theorem}\label{Theorem2}
Fix positive integers $n, m$, $k < m$, and $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. If a set of $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for each interval of length $k$ in some cyclic order on $[m]$ there are $k{m \choose k}$ vectors $\mathbf{a}_i$ supported on that interval then $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robustly identifiable $k$-sparse representation with probability one.
\end{theorem}

\begin{proof}
By Lemma \ref{Hillar15lemma2} (setting $M$ to be the identity matrix), with probability one the $\mathbf{a}_i$ are in general linear position. Apply Theorem \ref{DeterministicUniquenessTheorem}.
\end{proof} 

Note that an \emph{algebraic set} is a solution to a finite set of polynomial equations. 

\begin{theorem}\label{Theorem3}
Fix positive integers $k < m$ and $n$ . If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval, then with probability one the following holds. There is an algebraic set $Z \subset \mathbb{R}^{n \times m}$ of Lebesgue measure zero with the following property: if $A \notin Z$ then $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
\end{theorem}

\begin{proof}
By Lemma \ref{Hillar15lemma2} (setting $M$ to be the identity matrix), with probability one the $\mathbf{a}_i$ are in general linear position. By the same arguments made in the proof of Theorem 3 in \cite{Hillar15}, the set of matrices $A$ that fail to satisfy \eqref{SparkCondition} form an algebraic set of measure zero. Apply Theorem \ref{DeterministicUniquenessTheorem}.
\end{proof}

We note that the formula for $C$ derived in the deterministic case was derived for the \emph{worst case} noise, which (depending on the noise distribution) may have very small probability of occurrence. In particular, for fixed $k$, the larger the ambient dimension of $\mathbf{y}$, the smaller the probability that randomly generated noise points in a direction which conflates signals lying near distinct $k$-dimensional subspaces. 

%===================================
% 			DISCUSSION
%===================================
\section{Discussion}

In this note, we generalize uniqueness of SCA to the case of deterministic noise.  Somewhat surprisingly, almost every dictionary and sufficient quantity of sparse codes are uniquely determined up to the error in sampling and inherent symmetries (uniform relabeling and scaling).  To convince the reader of the general usefulness of this result, we elaborate briefly on the following five application areas.

\textbf{Sparse Coding Theory}.  
Sparse coding or sparse component analysis is a common tool for uncovering structure in complex datasets. 
%However, the literature contains a plethora of different approaches for implementing it, each one with its own proof of convergence to a local minimum.
While certain algorithms have been shown to have local converge to a global minimum or global convergence to a local minimum of approximations to \eqref{DictionaryLearning}, to our knowledge there exist no algorithms with guaranteed global convergence to a global solution. Our conditions apply regardless of whichever algorithm is used to solve \eqref{DictionaryLearning} since they are derived from the underlying geometry of the dictionary learning problem, providing for a universal check when any dictionary learning algorithm has converged to (near) a global minimum of \eqref{DictionaryLearning}.

Theorem \eqref{DeterministicUniquenessTheorem} is a stability result akin to that in \cite{donoho2006stable, candes2006stable}, only now for the case when the dictionary matrix is unknown.

Another implication of our theorem is the determination of an overcompleteness regime dimensionality of any dataset with respect to $\varepsilon$. If you coded very well some data sparsely then you can do a check and if that check holds up then anyone else who codes the data sparsely too has learned the same dictionary. 

The theory of CS informs also informs another practical consequence of our result. Since our derived sample complexity is independent of the ambient dimension of the data, $n$, given a lower bound on the sparsity of the latent variables $\mathbf{a}_i$ we can generate a random matrix to compress the data to a dimension in the regime of \eqref{CScondition} before applying a dictionary learning. This could significantly reduce the computational cost of dictionary learning when the sparsity is high by reducing the number of parameters required to define each dictionary element. Such improvements are crucial to scaling up dictionary learning to larger datasets. \textbf{[Is this actually a significant computational boost..?]}  

\textbf{Mathematics of Computation}.
Theorem \eqref{DeterministicUniquenessTheorem} derives from the underlying geometry of the dictionary learning problem, and therefore provides a universal check for when any dictionary learning algorithm has converged to (near) a global minimum of \eqref{DictionaryLearning}. This is similar in spirit to the test provided in \cite{gribonval2006simple} for the case where the dictionary matrix is known a priori. 
	
[********** Discuss that it would be awesome to find the best dependence of $\varepsilon$ on $\delta$ and best constants and number of samples for this problem **********]
Decidability (deterministic checks),  

Another field of computation for which our main results have consequences is that of smoothed analysis \cite{spielman2004smoothed}.  The main idea there is that certain algorithms having bad worst case behavior, nonetheless, are efficient if certain (typically Lebesgue measure zero) pathological input sets are avoided.  In the case of SCA, our results imply that if there is an efficient ``smoothed" algorithm, then only for a measure zero set of inputs will this algorithm fail to determine the unique original solution. 

\textbf{Signal Processing and Data Analysis}.

Uniqueness in data analysis.  \cite{agarwal2014spatially}
Uniqueness of receptive fields.

\textbf{Brain Communication Theory}.

Theory works in the noisy case.
\cite{coulter2010adaptive, isely2010deciphering}

\textbf{Applied Physics and Engineering}.
As a final application domain, we consider engineering applications.  Several groups have found ways to utilized compressed sensing for signal processing tasks, such as digital image compression \cite{SinglePixelCamera} (the ``single-pixel camera") and, more recently, the design of an ultrafast camera \cite{gao2014single} capable of capturing one hundred billion frames per second. Given such effective uses of classical CS, it is only a matter of time before these systems utilize learning in SCA to code and process data.  In this case, guarantees such as the ones offered by our main theorems allow any such device to be compared any other (having different initial parameters and data samples) as long as the data originates from the same system.

%========================================
%      	APPENDIX: COMBINATORICS
%========================================
\appendices
\section{Combinatorial Matrix Theory}\label{appendixA}

In this section, we prove Lemma \ref{MainLemma}, which is the main ingredient in our proof of Theorem \ref{DeterministicUniquenessTheorem}. For readers willing to assume a priori that the spark condition holds for $B$ as well as for $A$, a shorter proof of this case (Lemma \ref{MainLemma2} from Section \ref{mleqm}) is provided in Appendix \ref{mleqmAppendix}. This additional assumption simplifies the argument and allows us to extend robust identifiability conditions to the case where only an upper bound on $m$ is known. 

We now prove some auxiliary lemmas before deriving Lemma \ref{MainLemma}.  Given a collection of sets $\mathcal{T}$, we let $\cap \mathcal{T}$ denote their intersection.

%===== SPAN INTERSECTION LEMMA =====
\begin{lemma}\label{SpanIntersectionLemma}
Let $M \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $M$ are linearly independent, then for any $\mathcal{T} \subseteq \bigcup_{\ell \leq k} {[m] \choose \ell}$,
\begin{align}
%y \in \text{Span}\{M_{\cap \mathcal{T}}\}  \Longleftrightarrow y \in \bigcap_{S \in \mathcal{T}} \text{Span}\{M_S\}.
\text{\rm Span}\{M_{\cap \mathcal{T}}\}  = \bigcap_{S \in \mathcal{T}} \text{\rm Span}\{M_S\}.
\end{align}
\end{lemma}

\begin{proof}By induction, it is enough to prove the lemma when $|\mathcal{T}| = 2$. The proof now follows directly from the assumption.
% The forward direction is trivial; we prove the reverse direction. Enumerate $\mathcal{T} = (S_1, \ldots, S_{|\mathcal{T}|})$ and let $\mathbf{y} \in \text{Span}\{M_{S_1}\} \cap \text{Span}\{M_{S_2}\}$. Then there exists some $\mathbf{x}_1$ with support contained in $S_1$ such that $\mathbf{y} = M\mathbf{x}_1$ and some $\mathbf{x}_2$ with support contained in $S_2$ such that $\mathbf{y} = M\mathbf{x}_2$. We therefore have $M(\mathbf{x}_1 - \mathbf{x}_2) = 0$, which implies that $\mathbf{x}_1 = \mathbf{x}_2$ by the spark condition. Hence $\mathbf{x}_1$ and $\mathbf{x}_2$ have the same support contained in both $S_1$ and $S_2$, i.e. $\mathbf{y} \in \text{Span}\{M_{S_1 \cap S_2}\}$. This carries over by induction to the entire sequence of supports in $\mathcal{T}$. 
\end{proof}

%===== DISTANCE TO INTERSECTION LEMMA =====

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $\mathcal{V} = \{V_1, \ldots, V_k\}$ be subspaces of $\mathbb{R}^m$ and let $V = \bigcap \mathcal{V}$. For every $\mathbf{x} \in \mathbb{R}^m$, we have
\begin{align}\label{DTILeq}
|\mathbf{x} - \Pi_V \mathbf{x}|_2 \leq \frac{1}{1 - \xi(\mathcal{V})} \sum_{i=1}^k |x - \Pi_{V_i} x|_2,
\end{align}
provided $\xi(\mathcal{V}) \neq 1$, where the expression for $\xi$ is given in Def.~\ref{SpecialSupportSet}.
\end{lemma}
\begin{proof} 
Fix $\mathbf{x} \in \mathbb{R}^m$ and $k \geq 2$. The proof consists of two parts. First, we shall show that 
\begin{equation}\label{induction}
|\mathbf{x} - \Pi_V\mathbf{x}|_2 \leq \sum_{\ell=1}^k |\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}|_2 + |\Pi_{V_{k}}\Pi_{V_{k-1}}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}|_2.
\end{equation}
For each $\ell \in \{2, \ldots, k+1\}$ (when $\ell = k+1$, the product $\Pi_{V_k} \cdots \Pi_{V_{\ell}}$ is set to $I$), we have by the triangle inequality and the fact that $\|\Pi_{V_{\ell}}\|_2 \leq 1$ (as $\Pi_{V_{\ell}}$ are projections):
\begin{equation}
|\Pi_{V_k} \cdots \Pi_{V_{\ell}}\mathbf{x} - \Pi_V \mathbf{x}|  \leq  |\Pi_{V_k} \cdots \Pi_{V_{\ell-1}}\mathbf{x} - \Pi_V \mathbf{x}| + 
|\mathbf{x} - \Pi_{V_{\ell-1}}\mathbf{x}|.
%\begin{split}
%|\Pi_{V_k} \cdots \Pi_{V_{\ell}}\mathbf{x} - \Pi_V \mathbf{x}| & \ \leq |\Pi_{V_k} \cdots \Pi_{V_{\ell-1}}\mathbf{x} - \Pi_V \mathbf{x}| + 
%|\Pi_{V_k} \cdots \Pi_{V_{\ell}}(I - \Pi_{V_{\ell-1}}) \mathbf{x}|\\
%& \ \leq  |\Pi_{V_k} \cdots \Pi_{V_{\ell-1}}\mathbf{x} - \Pi_V \mathbf{x}| + 
%|\mathbf{x} - \Pi_{V_{\ell-1}}\mathbf{x}|.
%\end{split}
\end{equation}
Summing these inequalities over $\ell$ gives (\ref{induction}).
%will show that for any $\sigma \in \frak{S}_k$,
%\begin{align}\label{induction}
%|\mathbf{x} - \Pi_V\mathbf{x}|_2 \leq \sum_{i=1}^k |\mathbf{x} - \Pi_{V_i} \mathbf{x}|_2 + |\Pi_{V_{\sigma(k)}}\Pi_{V_{\sigma(k-1)}}\cdots\Pi_{V_{\sigma(1)}} \mathbf{x} - \Pi_V \mathbf{x}|_2.
%\end{align}
%%
%Assume without loss of generality that $\sigma(i) = i$ for all $i \in [m]$. We have by the triangle inequality that
%\begin{align*}
%|\mathbf{x} - \Pi_V\mathbf{x}|_2 &= |\mathbf{x} - \Pi_{V_k} \mathbf{x}|_2 + |\Pi_{V_k}(I - \Pi_{V_{k-1}}) \mathbf{x}|_2 + |\Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x} - \Pi_V\mathbf{x}|_2 \\
%&\leq \sum_{i=k-1}^k|\mathbf{x} - \Pi_{V_i} \mathbf{x}|_2 + |\Pi_{V_k}\Pi_{V_{k-1}} \mathbf{x} - \Pi_V \mathbf{x}|_2,
%\end{align*}
%%
%where in the second line we have used the fact that $\|\Pi_{V_k}\|_2 \leq 1$. If $k=2$ then we are done; otherwise, we may repeat this manipulation another $k-2$ times until we arrive at \eqref{induction}. 

Next, we show how the result \eqref{DTILeq} follows from \eqref{induction} from the following result of \cite[Theorem 9.33]{Deutsch}:
\begin{align}\label{dti2}
|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}|_2 \leq z |\mathbf{x}|_2 \indent \text{for all} \indent \mathbf{x} \in \mathbb{R}^m,
\end{align}
where $z= \left[1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)\right]^{1/2}$ and $z_{\ell} = \cos\theta_F\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$. To see this, note that
\begin{align}\label{dti1}
|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}(\mathbf{x} - \Pi_V\mathbf{x}) - \Pi_V(\mathbf{x} - \Pi_V\mathbf{x})|_2 
&= |\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x} |_2,
\end{align}
since $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$.
Therefore by \eqref{dti2} and \eqref{dti1}, it follows that
%\begin{align*}
\[|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x} |_2
= |\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}(\mathbf{x} - \Pi_V\mathbf{x}) - \Pi_V(\mathbf{x} - \Pi_V\mathbf{x})|_2  \leq z |\mathbf{x} - \Pi_V\mathbf{x}|_2.\]
%\end{align*}
Combining this last inequality with \eqref{induction} and rearranging, we arrive at
%\begin{align*}
%|\mathbf{x} - \Pi_V\mathbf{x}|_2 \leq \sum_{i=1}^k |\mathbf{x} - \Pi_{V_i} \mathbf{x}|_2 + z |\mathbf{x} - \Pi_V\mathbf{x}|_2,
%\end{align*}
%from which it follows that
%=======
%%
%We therefore have by \eqref{dti2} and \eqref{dti1} that
%\begin{align*}
%|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x} |_2
%&= |(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(\mathbf{x} - \Pi_V\mathbf{x}) - \Pi_V(\mathbf{x} - \Pi_V\mathbf{x})|_2 \\
%&\leq c |\mathbf{x} - \Pi_V\mathbf{x}|_2.
%\end{align*}
%
%Substituting the left-hand side into \eqref{induction}, we get
%\begin{align*}
%|\mathbf{x} - \Pi_V\mathbf{x}|_2 \leq \sum_{i=1}^k |\mathbf{x} - \Pi_{V_i} \mathbf{x}|_2 + c |\mathbf{x} - \Pi_V\mathbf{x}|_2,
%\end{align*}
%%
%from which it follows (provided $c \neq 1$) that
%>>>>>>> 8b98d274b6212fdd622059a4b4eb6c256d2a8ad9
\begin{align}\label{ceq}
|\mathbf{x} - \Pi_V \mathbf{x}|_2 \leq \frac{1}{1 - z} \sum_{i=1}^k |\mathbf{x} - \Pi_{V_i} \mathbf{x}|_2.
\end{align}
Finally, since the ordering of the subspaces is arbitrary, we can replace $z$ in \eqref{ceq} with $\xi(\mathcal{V})$ to obtain \eqref{DTILeq}.
\end{proof}

%Given positive integers $k < m$ and a permutation $\sigma \in \frak{S}_m$ (where $\frak{S}_m$ denotes the \textit{symmetric group} of bijections on $m$ elements), let
%\begin{align}
%S_\sigma(i) := \{1+\sigma(i), \ldots, 1+\sigma(i + (k-1)) \} \indent \text{for} \indent i = 0, \ldots, m-1
%\end{align}
%%
%with addition modulo $m$. In words, the $S_\sigma(i)$ are the intervals of length $k$ within the set of elements of $\{1, \ldots, m\}$ arranged in the cyclic order $\sigma(1), \ldots, \sigma(m)$.


%======= GRAPH THEORY LEMMA =======

\begin{lemma}\label{NonEmptyLemma} Fix positive integers $k < m$, and let $S_1, \ldots, S_m$ be the set of contiguous length $k$ intervals in some cyclic order of $[m]$. Suppose there exists a map $\pi: T \to {[m] \choose k}$ such that
\begin{align}\label{NonEmpty}
|\bigcap_{i \in J} \pi(S_i)| \leq |\bigcap_{i \in J} S_i | \ \ \text{for all} \ J \in {[m] \choose k}.
\end{align}
%
Then, $|\pi(S_{j_1}) \cap \cdots \cap \pi(S_{j_k})| = 1$ for $j_1,\ldots,j_k$ consecutive modulo $m$.
\end{lemma}

\begin{proof} Consider the set $Q_m = \{ (r,t) : r \in \pi(S_t), t \in [m] \}$, which has $mk$ elements. By the pigeon-hole principle, there is some $q \in [m]$ and $J \in {[m] \choose k}$ such that $(q, j) \in Q_m$ for all $j \in J$. In particular, we have $q \in \cap_{j \in J} \pi(S_j)$ so that from \eqref{NonEmpty} there must be some $p \in [m]$ with $p \in \cap_{j \in J} S_j$. Since $|J| = k$, this is only possible if the elements of $J = \{j_1, \ldots, j_k\}$ are consecutive modulo $m$, in which case $|\cap_{j \in J} S_j| = 1$. Hence $|\cap_{j \in J} \pi(S_j)| = 1$ as well.
% i.e. $J = \{v - (k-1), \ldots, v\}$. (The only elements in $T$ that contain $\sigma(v)$ are $S_\sigma(v-(k-1)), \ldots, S_\sigma(v)$.)

We next consider if any other $t \notin J$ is such that $q \in \pi(S_t)$. Suppose there were such a $t$; then, we would have $q \in \pi(S_t) \cap \pi(S_{j_1}) \cap \cdots \cap \pi(S_{j_k})$ and \eqref{NonEmpty} would imply that the intersection of every $k$-element subset of $\{S_t\} \cup \{S_j: j \in J\}$ is nonempty. This would only be possible if $\{t\} \cup J = [m]$, in which case the result then trivially holds since then $q \in \pi(S_j)$ for all $j \in [m]$.  Suppose now there exists no such $t$; then letting $Q_{m-1} \subset Q_m$ be the set of elements of $Q_m$ not having $q$ as a first coordinate, we have $|Q_{m-1}| = (m-1)k$. 

By iterating the above arguments we arrive at a partitioning of $Q_m$ into sets $R_i = Q_i \setminus Q_{i-1}$ for $i = 1, \ldots, m$, each having a unique element of $[m]$ as a first coordinate common to all $k$ elements while having second coordinates which form a consecutive set modulo $m$. In fact, every set of $k$ consecutive integers modulo $m$ is the set of second coordinates of some $R_i$. This must be the case because for every consecutive set $J$ we have $|\cap_{j \in J} S_j| = 1$, whereas if $J$ is the set of second coordinates for two distinct sets $R_i$ we would have $|\cap_{j \in J} \pi(S_j)| \geq 2$, which violates \eqref{NonEmpty}. 
\end{proof}
%==== PROOF OF MAIN LEMMA =======

\begin{proof}[Proof of Lemma \ref{MainLemma} (Main Lemma)]
We assume $k \geq 2$ since the case $k = 1$ was proven at the beginning of Section \ref{DUT}. Let $S_1, \ldots, S_m$ be the set of contiguous length $k$ intervals in some cyclic ordering of $[m]$. We begin by proving that $\dim(\text{Span}\{B_{\pi(S_i)}\}) = k$ for all $i \in [m]$. 
%[*** We already proved this for $S' = \pi(S)$ in the proof of Theorem 1 -- should we remove this redundancy? We have to prove it again here though if we want the lemma to stand on its own. ***] 
Fix $i \in [m]$ and note that by \eqref{GapUpperBound} we have for all unit vectors $\mathbf{u} \in \text{Span}\{A_{S_i}\}$ that $d(u, \text{Span}\{B_{\pi(S_i)}\}) \leq \frac{\phi_k(A)}{\rho k} \delta$ for $\delta < \frac{L_2(A)}{ \sqrt{2}}$. By definition of $L_2(A)$ we have for all $2$-sparse $\mathbf{x} \in \mathbb{R}^m$:
\begin{align}
L_2(A) \leq \frac{|A\mathbf{x}|_2}{|\mathbf{x}|_2} \leq \rho \frac{|\mathbf{x}|_1}{|\mathbf{x}|_2} \leq \rho \sqrt{2}
\end{align}

Hence $\delta < \rho$. Since $\phi_k \in [0,1]$ we have $d(u, \text{Span}\{B_{\pi(S_i)}\}) < 1$ and it follows by Lemma \ref{MinDimLemma} that $\dim(\text{Span}\{B_{\pi(S_i)}\}) \geq \dim(\text{Span}\{A_{S_i}\}) = k$. Since $|\pi(S_i)| = k$, we in fact have $\dim(\text{Span}\{B_{\pi(S_i)}\}) = k$. %, i.e. the columns of $B_{\pi(S_\sigma(i))}$ are linearly independent. 

We will now show that
\begin{align}\label{fact2}
|\bigcap_{i \in J} \pi(S_i)| \leq |\bigcap_{i \in J} S_i | \ \ \text{for all} \ J \in {[m] \choose k}.
\end{align}

Fix $J \in {[m] \choose k}$. By \eqref{GapUpperBound} we have for all unit vectors $\mathbf{u} \in \cap_{i \in J} \text{Span}\{B_{\pi(S_i)}\}$ that $d(\mathbf{u}, \text{Span}\{A_{S_i}\}) \leq \frac{\phi_k(A)}{\rho k} \delta$ for all $j \in J$, where $\delta < \frac{L_2(A)}{\sqrt{2}}$. It follows by Lemma \ref{DistanceToIntersectionLemma} that
\begin{align*}
d\left( \mathbf{u}, \bigcap_{i \in J} \text{Span}\{A_{S_j}\} \right) 
\leq \frac{\delta}{\rho} \left( \frac{ \phi_k(A) }{1 - \xi( \{ \text{Span}\{A_{S_i}\}: i \in J\} ) } \right) \leq \frac{\delta}{\rho},
\end{align*}
%
where the second inequality follows immediately from the definition of $\phi_k(A)$. 

Now, since \mbox{$\text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_i)}\}$} and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_i}\} = \text{Span}\{A_{\cap_{i \in J}  S_i}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\cap_{i \in J} S_i}\} \right) \leq \frac{\delta}{\rho} \indent \text{for all unit vectors } \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma} (since $\delta/\rho < 1$) that $\dim(\text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\}) \leq \dim(\text{Span}\{A_{\cap_{i \in J} S_i}\})$ and \eqref{fact2} follows by the linear independence of the columns of $A_{S_i}$ and $B_{\pi(S_i)}$ for all $i \in [m]$.

Suppose now that $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_i = i$. By \eqref{fact2} we have that $\cap_{i \in J} \pi(S_i)$ is either empty or it contains a single element. Lemma \ref{NonEmptyLemma} ensures that the latter case is the only possibility. Thus, the association $i \mapsto \cap_{i \in J} \pi(S_i)$ defines a map $\hat \pi: [m] \to [m]$. Recalling \eqref{SubspaceMetricSameDim}, it follows from \eqref{fact1} that for all unit vectors $\mathbf{u} \in \text{Span}\{A_i\}$ we have $d\left( \mathbf{u}, \text{Span}\{B_{\hat \pi(i)}\}\right) \leq \delta/\rho$ also. Since $i$ is arbitrary, it follows that for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, letting $c_i = |A\mathbf{e}_i|_2^{-1}$ and $\varepsilon = \delta/\rho$, there exists some $c'_i \in \mathbb{R}$ such that $|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\hat \pi(i)}|_2 \leq \varepsilon$ where $\varepsilon < \frac{L_2(A)}{\sqrt{2}} \min_{j \in [m]} c_i$. This is exactly the supposition in \eqref{1D} and the result follows from the subsequent arguments of Section \ref{DUT}. 
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_{i - (k-1)} \cap S_i = \{i\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}

%================================
% APPENDIX: ADAPTED MAIN LEMMA
%================================

\section{Unknown $m$}\label{mleqmAppendix}

In this section we prove Lemma \ref{MainLemma2} from Section \ref{mleqm}. We consider the case when $k=1$ before showing how the case for $k>1$ reduces to this. Suppose we have $N = m$ $1$-sparse vectors $\mathbf{a}_i = c_i\mathbf{e}_i$ $(i = 1, \ldots, m)$ for $c_1, \ldots, c_m \in \mathbb{R}^m/\{0\}$. Fix some $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} and let $\rho = \max_{i \in [m]} |A\mathbf{e}_i|_2$. By \eqref{Cdef'} we have:
\begin{align}
\varepsilon_0' 
= \frac{ L_2(A) \min(\phi_1(A), \phi_1(B))}{\rho \sqrt{2} } \min_{S \in T} L_1(AX_{I(S)})
= \frac{ L_2(A) }{\rho \sqrt{2} } \min_{j \in [m]} L_1(c_j A_j) 
= \frac{L_2(A)}{ \rho \sqrt{2}} \min_{j \in [m]}|c_jA_j|_2
\leq \frac{L_2(A)}{ \sqrt{2}} \min_{j \in [m]}|c_j|
\end{align}
%
and by \eqref{Cdef} we have
\begin{align}
C = \frac{L_{2}(A)}{ \varepsilon'_0 \sqrt{2}} \geq (\min_{j \in [m]} |c_j|)^{-1}. 
\end{align} 

Suppose that for some $B \in \mathbb{R}^{n \times m'}$ and 1-sparse $\mathbf{b}_i \in \mathbb{R}^{m'}$ we have  $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon < \varepsilon'_0$ for all $i \in [m]$. Since the $\mathbf{b}_i$ are 1-sparse, there must exist $c'_1, \ldots, c'_m \in \mathbb{R}$ and some map $\pi: [m] \to [m']$ such that 
\begin{align}\label{1D2}
|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\pi(i)}|_2 \leq \varepsilon \ \ \text{for all} \  j \in [m].
\end{align} 
Note that $c'_i \neq 0$ for all $i$ since then otherwise we reach the contradiction $|c_iA\mathbf{e}_i|_2 < \varepsilon'_0 \leq (L_2(A)/\sqrt{2}) \min_{i \in [m]}|c_i| \leq \min_{i\in [m]}|c_iA\mathbf{e}_i|_2$ (by definition of $L_2(A)$). We will now show that $\pi$ is necessarily injective (and thus defines a permutation on $m$ elements). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m']$. Then, $|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\ell}|_2  \leq \varepsilon$ and $|c_jA\mathbf{e}_j - c'_jB\mathbf{e}_{\ell}|_2 \leq \varepsilon$. Using the triangle inequality followed by application of the definition of $L$, we have:
%Summing and scaling these two inequalities by $|c'_j|$ and $|c'_i|$, respectively, we have:
\begin{align*}
(|c'_i| + |c'_j|) \varepsilon
&\geq |c'_j||c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\ell}|_2 + |c'_i||c'_jB\mathbf{e}_{\ell} - c_jA\mathbf{e}_j|_2 \\
&\geq |c'_jc_iAe_i - c'_ic_jAe_j|_2 \\
&\geq L_2(A)|c'_jc_ie_i - c'_ic_je_j|_2 \\
&\geq \frac{L_2(A)}{\sqrt{2}}|c'_jc_ie_i - c'_ic_je_j|_1 \\
&\geq \frac{L_2(A)}{\sqrt{2}} \left( |c'_j| + |c'_i| \right) \min_{\ell \in [m]} |c_\ell | \\
&\geq \varepsilon'_0 \left( |c'_j| + |c'_i| \right),
\end{align*}
%
which contradicts $\varepsilon < \varepsilon'_0$. Hence, $\pi$ is injective. Letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}, \mathbf{0}, \ldots, \mathbf{0} \right) \in \mathbb{R}^{m' \times m'}$ and $D = \text{diag}(\frac{c'_1}{c_1},\ldots,\frac{c'_m}{c_m}) \in \mathbb{R}^{m' \times m}$, we see that \eqref{1D} becomes 
\begin{align}
|(A - BPD)\mathbf{e}_i|_2 = |A\mathbf{e}_i - \frac{c'_i}{c_i}B\mathbf{e}_{\pi(i)}|_2 \leq \frac{\varepsilon}{|c_i|} \leq (\min_{i \in [m]} |c_i|)^{-1} \varepsilon \leq C\varepsilon \ \ \text{for all } i \in [m].
\end{align}


%=== PROOF OF ADAPTED MAIN LEMMA ===
Now suppose $k \geq 2$ and let $S_1, \ldots, S_m$ be the set of contiguous length $k$ intervals in some cyclic ordering of $[m]$. Fix $i \in [m]$ and let $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_i = i$. Assumption \eqref{GapUpperBound2} implies that for all unit vectors $\mathbf{u} \in \cap_{j \in J} \text{Span}\{B_{\pi(S_j)}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_j}\}) \leq \frac{\phi_k(A)}{\rho k} \delta$ for all $j \in J$, with $\delta < \frac{L_2(A)}{\sqrt{2}}$. By Lemma \ref{DistanceToIntersectionLemma} we have that:
\begin{align}\label{sym2}
d\left( \mathbf{u}, \bigcap_{j \in J} \text{Span}\{A_{S_j}\} \right) 
\leq \frac{\delta}{\rho} \left( \frac{\phi_k(A)}{1 - \xi(\{ \text{Span}\{A_{S_j}\} : j \in J \})} \right) \leq \frac{\delta}{\rho},
\end{align}
%
where the second inequality is by definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_i)}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_i}\} = \text{Span}\{A_i\}$, we have
\begin{align}\label{fact12}
d\left( \mathbf{u}, \text{Span}\{A_i\} \right) \leq \frac{\delta}{\rho} \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\})$ and $W = \text{Span}\{A_i\}$, that $\dim(V) \leq \dim(W)$. The same arguments can be used to prove \eqref{fact12} with the roles of $A$ and $B$ reversed. Hence, $\dim(V) = \dim(W) = 1$ and it follows that $|\cap_{i \in J} \pi(S_i)| = 1$ since the columns of $B_{\cap_{i \in J} \pi(S_i)}$ are linearly independent by the spark condition. Thus, the association $i \mapsto \cap_{i \in J} \pi(S_i)$ defines a map $\pi: [m] \to [m']$ such that $d\left( \mathbf{u}, \text{Span}\{B_{ \pi(S_i)}\}\right) \leq \delta/\rho$ for all $i \in [m]$.
Since $i$ is arbitrary, it follows that for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, letting $c_i = |A\mathbf{e}_i|_2^{-1}$ and $\varepsilon = \delta/\rho$, there exists some $c'_i \in \mathbb{R}$ such that $|c_iA\mathbf{e}_i - c'_iB\mathbf{e}_{\hat \pi(i)}|_2 \leq \varepsilon$ where $\varepsilon < \frac{L_2(A)}{\sqrt{2}} \min_{i \in [m]} |c_i|$. This is exactly the supposition in \eqref{1D2}.

%===================================
% 		ACKNOWLEDGEMENT
%===================================
\section*{Acknowledgment}
We would like to thank Fritz Sommer for turning our attention to the dictionary learning problem. We also thank Bizzyskillet on SoundCloud for "The No-Exam Jams", which played on repeat during many long hours of designing proofs. Finally, we thank Ian Morris for posting \eqref{SubspaceMetricSameDim} and a reference to his proof of it on the internet (``Stack Exchange").
Thank german dude who visited Redwood.  Thank Darren Rhea for early explorations in this problem.

%===================================
% 			REFERENCES
%===================================
\bibliographystyle{IEEEtran}
\bibliography{RobustIdentifiability}

%===================================
% 			BIOGRAPHY
%===================================
\begin{IEEEbiographynophoto}{Charles J. Garfinkle}
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Christopher J. Hillar}
completed a B.S. in Mathematics and a B.S. in Computer Science at Yale University.  Supported by an NSF Graduate Research Fellowship, he received his Ph.D. in Mathematics from the University of California (UC), Berkeley in 2005. From 2005-2008, he was a Visiting Assistant Professor and NSF Postdoctoral Fellow at Texas A\&M University. From 2008-2010, he was an NSF Mathematical Sciences Research Institutes Postdoctoral Fellow at the Mathematical Sciences Research Institute (MSRI) in Berkeley, CA.  In 2010, he joined the Redwood Center for Theoretical Neuroscience at UC Berkeley, and in 2011, he  became a research specialist in the Tecott mouse behavioral neuroscience lab at UC San Francisco.
\end{IEEEbiographynophoto}

\end{document}
