% REFERENCE NOTES
% 'Spark' of a matrix coined and defined in "Optimally  sparse representation in  general  (non-orthogonal)
% dictionaries via L1 minimization" and applied to the study of uniqueness in "Sparse signal reconstruction % from limited data using FOCUSS:   A   re-weighted   norm   minimization   algorithm"

% QUESTIONS
% 1) Are necessary conditions for uniqueness in the case where A (or sources) is known still necessary
% conditions for when A (or sources) is unknown? Permutation scaling ambiguity...
% 2) If 1) is true, can we derive the conditions on A and the data for SCA from the necessary conditions
% for these simpler constrained problems?

\documentclass[journal, onecolumn]{IEEEtran}

% *** MATH PACKAGES ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Robust Identifiability in Sparse Dictionary Learning}

\author{Charles~J.~Garfinkle,  Christopher~J.~Hillar%
\thanks{The research of Garfinkle and Hillar was conducted while at the Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA; e-mails: cjg@berkeley.edu, chillar@msri.org.}}%

\maketitle

\begin{abstract}
We study uniqueness in sparse dictionary learning when reconstruction of data is approximate.
\end{abstract}

\begin{IEEEkeywords}
bilinear inverse problem, identifiability, dictionary learning, sparse coding, matrix factorization, compressed sensing, combinatorial matrix theory, blind source separation
\end{IEEEkeywords}

%===================================
% 			INTRODUCTION
%===================================

\section{Introduction}

\IEEEPARstart{O}{ne} of the fundamental questions in data analysis is how to represent the data in a way that reveals structure. Recently, algorithms have been developed for uncovering \emph{sparse} structure in a given dataset $\mathbf{y}_1, \ldots, \mathbf{y}_N \in \mathbb{R}^n$ by approximately solving the constrained optimization problem:
\begin{align}\label{DictionaryLearning}
\min_{A, \mathbf{a}_i} \sum_{i=1}^N \|\mathbf{y}_i - A\mathbf{a}_i\|_2 \indent \text{subject to } \|\mathbf{a}_i\|_0 \leq k 
\end{align}
%
for unknown $A \in \mathbb{R}^{n \times m}$ and $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with $k < m \ll N$. Known variously as dictionary learning, sparse coding, or sparse matrix factorization, this framework emphasizes parsimony in representation by approximating each $\mathbf{y}_i$ as some linear combination of at most $k$ columns of some matrix $A$, typically with $k \ll m$. 

It is natural to wonder when the solution to \eqref{DictionaryLearning} is in some sense unique, or at least approximately so. This is a question of fundamental concern to \emph{sparse component analysis} (SCA) \cite{Georgiev05}, wherein one assumes the linear model:
\begin{align}\label{LinearModel}
\mathbf{y}_i = A\mathbf{a}_i + \mathbf{n}_i 
\end{align}
%
with the vector $\mathbf{n}_i \in \mathbb{R}^n$ accounting for both noise in the measurements and the degree to which the model deviates from reality (e.g. the true $\mathbf{a}_i$ may only be approximately $k$-sparse). The goal in SCA is to estimate the sparse \emph{sources} $\mathbf{a}_i$ from the observed \emph{mixtures} $\mathbf{y}_i$ by approximately solving \eqref{DictionaryLearning}.

Even when $\mathbf{n}_i = 0$, however, there infinitely many solutions to \eqref{DictionaryLearning}. This is perhaps best seen from the perspective of sparse matrix factorization. Letting $Y$ and $X$ be the matrices with columns $\mathbf{y}_i$ and $\mathbf{a}_i$, respectively, we see that if $Y = AX$ for some $X$ with $k$-sparse columns, then also $Y = (AD^{-1}P^{-1})(PDX)$ for any permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. Moreover the columns of $PDX$ are also $k$-sparse. The "uniqueness" of any solution to \eqref{DictionaryLearning} can therefore only be considered up to this inherent ambiguity, known as the \emph{ambiguity transform group} associated to this particular form of bilinear inverse problem. This motivates the following definition.

\begin{definition}\label{Uniqueness}
A dataset $\{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ has a \textbf{$k$-sparse representation} in $\mathbb{R}^m$ when for some $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ we have $\mathbf{y}_i = A\mathbf{a}_i$ for all $i = 1, \ldots, N$. We say that $A$ and the $\mathbf{a}_i$ forming this representation are \textbf{identifiable} (up to permutation and scaling ambiguity) given the data when every other $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ for which $\mathbf{y}_i = B\mathbf{b}_i$ for all $i = 1, \ldots, N$ necessarily satisfy $A = BPD$ and $\mathbf{b}_i = PD\mathbf{a}_i$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. The representation is \textbf{stable} when there exists some $\varepsilon > 0$ such that if $B$ and the $\mathbf{b}_i$ only satisfy $\|\mathbf{y}_i - B\mathbf{b}_i\|_2 < \varepsilon$ for all $i = 1, \ldots, N$, then
\begin{align}\label{def1}
\|A - BPD\|_2 \leq C_1 \max_i \|A\mathbf{a}_i - B\mathbf{b}_i\|_2 
\indent \text{and} \indent
\|\mathbf{b}_i - PD\mathbf{a}_i\|_2 \leq C_2 \max_i \|A\mathbf{a}_i - B\mathbf{b}_i\|_2
\end{align}
for some $C_1, C_2>0$. We call $\varepsilon$ the \textbf{error threshold} and $C_1, C_2$ the \textbf{stability constants} associated with the $k$-sparse representation formed by $A$ and the $\mathbf{a}_i$.
\end{definition}

In reality, we never obtain data without some contamination with noise, and our models are rarely (if ever) perfect. Suppose then that $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$ is a dataset for which for some $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ we have $\|\mathbf{y}_i - A\mathbf{a}_i\|_2 < \varepsilon$ for $i = 1, \ldots, N$ for some $\varepsilon > 0$. By the triangle inequality, any alternate $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ satisfying $\|\mathbf{y}_i - B\mathbf{b}_i\| < \varepsilon$ for $i = 1, \ldots, N$ is such that $\|A\mathbf{a}_i - B\mathbf{b}_i\| < 2\varepsilon$. If the set $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with error threshold $2\varepsilon$ then the alternate representation must also satisfy \eqref{def1}. We say such datasets $Y$ have stable $k$-sparse \emph{approximate representations} in $\mathbb{R}^m$. Some readers may find the following alternate definition more appealing:

\begin{definition}\label{Uniqueness}
We say a dataset $Y \subset \mathbb{R}^n$ has a stable $k$-sparse approximate representation in $\mathbb{R}^m$ when:
\begin{enumerate}
\item There exists some $A \in \mathbb{R}^{n \times m}$ and $\varepsilon > 0$ such that for every $\mathbf{y} \in Y$, the $\varepsilon$-ball $\mathcal{B}_2(\mathbf{y}, \varepsilon)$ centered at $\mathbf{y}$ intersects exactly one subspace spanned by $k$ columns of $A$.
\item Any matrix $B \in \mathbb{R}^{n \times m}$ for which every such ball $\mathcal{B}_2(\mathbf{y}, \varepsilon)$ intersects a subspace spanned by $k$ columns of $B$ necessarily satisfies $\|A - BPD\|_2 \leq C_1\varepsilon$ and $\|\mathbf{b}_i - PD\mathbf{a}_i\| \leq C_2\varepsilon$ for some permutation matrix $P \in \mathbb{R}^{m \times m}$, invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ and $C_1, C_2 > 0$.
\end{enumerate}
\end{definition}

We are now in a position to phrase the central problem addressed in this paper:

\begin{problem}\label{DUTproblem}
Let $Y = \{\mathbf{y}_1, \ldots, \mathbf{y}_N \} \subset \mathbb{R}^n$ be generated as $\mathbf{y}_i = A\mathbf{a}_i  + \mathbf{n}_i$ for some matrix $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_i \in \mathbb{R}^m$ with bounded noise $\mathbf{n}_i \in \mathbb{R}^n$. When does $Y$ have a stable $k$-sparse approximate representation in $\mathbb{R}^m$?
\end{problem}

Note how by the comments preceding Definition \ref{Uniqueness}, the dataset $Y$ has a stable $k$-sparse approximate representation in $\mathbb{R}^m$ whenever $\{A\mathbf{a}_i\}_{i=1}^N$ has a stable $k$-sparse representation in $\mathbb{R}^m$, provided $\|\mathbf{n}_i\|$ is less than half the error threshold of this representation.

Conditions for the identifiability of the parameters $A$ and $\mathbf{a}_i$ of the SCA model were first provided for the noiseless case by Georgiev et. al. \cite{Georgiev05} and subsequently by Aharon et. al. \cite{Aharon06}. The most general conditions to date were recently proven in \cite{HS11}. We refine their proofs to account for noisy measurements (or inadequacy of the model) and we significantly reduce their theoretically required number of samples for the deterministic case from $N=k{m \choose k}^2$ to $N = mk{m \choose k}$. Our \emph{robust} identifiability conditions describe when the true sources $\mathbf{a}_i$ and mixing matrix $A$ can be "uniquely" determined in the sense of Definition \ref{Uniqueness}. Moreover, we provide guarantees for the case when only an upper bound on the dimensionality of the sources is known. Our conditions are derived from the underlying geometry of the dictionary learning problem, hence they apply regardless of whichever algorithm is used to solve \eqref{DictionaryLearning}. 

Before stating our first theorem, we introduce the necessary constraint on the matrix $A$ by considering the simpler case where we wish to identify the sources $\mathbf{a}_i$ from the noiseless mixture $\mathbf{y}_i = A\mathbf{a}_i$ when $A$ is already known. Evidently, $A$ must satisfy
\begin{align}\label{SparkCondition}
A\mathbf{a}_1 = A\mathbf{a}_2 \implies \mathbf{a}_1 = \mathbf{a}_2 \indent \text{for all $k$-sparse } \mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^m
\end{align}
%
for such recovery to be possible. This condition, known in the literature as the \emph{spark condition} \cite{ref?}, ensures that distinct sparse sources $\mathbf{a}_i$ are distinguishable in their measurements $\mathbf{y}_i$. As it turns out, given enough samples, condition \eqref{SparkCondition} is also enough to guarantee the identifiability of $A$ in the case where it is not known a priori, as our main result states:

%=== STATEMENT OF DETERMINISTIC UNIQUENESS THEOREM ===%

\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix positive integers $n, m$ and $k < m$ and a cyclic order on $\{1, \ldots, m\}$. If $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in the cyclic order there are $k{m \choose k}$ vectors $\mathbf{a}_i$ in general linear position supported on that interval then every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates a set of vectors $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{theorem}

\begin{corollary}\label{DeterministicUniquenessTheorem}
Given positive integers $n, m$ and $k < m$, there exist $N =  mk{m \choose k}$ $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} generates a set of vectors $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

In fact, there are many such sets of deterministically produced $\mathbf{a}_i$; we give a parametrized family in Section \ref{DUT}. The generality of the construction allows us to easily extend the theorem to cases where the $A$ and $\mathbf{a}_i$ are randomly generated.

Before we bound the error threshold and stability constants associated to a given stable $k$-sparse representation we must first introduce some notation. We denote by $[m]$ the set $\{1, ..., m\}$ and by ${[m] \choose k}$ the set of subsets of $[m]$ of cardinality $k$. Recall that $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ for real vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$ is the vector space consisting of their $\mathbb{R}$-linear span:
%
\[ \text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\} = \left\{ \sum_{i=1}^\ell t_i\mathbf{v}_i : t_1, \ldots, t_\ell \in \mathbb{R}\right\}. \]
%
For a subset $S \subseteq [m]$ and matrix $M \in \mathbb{R}^{n \times m}$ with columns $\{M_1,...,M_m\}$ we define
%
\[ \text{Span}\{M_S\} = \text{Span}\{M_s: s \in S\}. \]

The \emph{lower bound} of a matrix $M \in \mathbb{R}^{n \times m}$ \cite{refs?} is 
\begin{align*}
\ell(M) := \sup \{ \alpha : \|M\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all } \mathbf{x} \in \mathbb{R}^m\}
\end{align*}
%
and we introduce 
\[
 \ell_k(M) =
  \begin{cases} 
      \hfill \min \left\{ \ell(M_S) : S \in {[m] \choose k} \right\} \hfill & \text{ if $k < m$} \\
      \hfill \ell(M) \hfill & \text{ if $k \geq m$} \\
  \end{cases}
\]

The reader can convince themselves that if $k < k'$ then $\ell_{k}(M) \geq \ell_{k'}(M) \geq \ell(M) > 0$ for any matrix $M$ with full column rank. We note that if $M$ is a real matrix then it also has bounded matrix norm $\max_{\mathbf{x} \in \mathbb{R}^m}\|M\mathbf{x}\|_2 / \|\mathbf{x}\|_2$; hence $\ell_{2k}(M) > 0$ is equivalent to $M$ having the \emph{restricted isometry property} \cite{CandesTao05} familiar from work in the field of compressed sensing.

Whenever $A$ satisfies \eqref{SparkCondition}, there is at least one nonzero principal (or canonical) angle between all pairs of subspaces spanned by the columns of $A$ of dimensionality less than or equal to $k$. (To see why, note that if the largest principal angle between any two of these subspaces is zero, then one subspace is a subset of the other which violates the spark condition.) The smallest nonzero principal angle between a pair of subspaces, known also as the Friedrichs angle, can be defined in terms of its cosine as follows:

\begin{definition}\label{FriedrichsDefinition}
The \emph{Friedrichs angle} $\theta_F(V,W) \in [0,\frac{\pi}{2}]$ between subspaces $V,W \subseteq \mathbb{R}^n$ is the minimal angle formed between unit vectors in $V \cap (V \cap W)^\perp$ and $W \cap (W \cap V)^\perp$. That is,
\begin{align}
\cos\left[\theta_F(V,W)\right] := \max\left\{ \frac{ \langle v, w \rangle }{\|v\|\|w\|}: v \in V \cap (V \cap W)^\perp, w \in W \cap (V \cap W)^\perp \right\}.
\end{align}
\end{definition}

To gain intuition for this quantity, if $k=2$ and $n=3$ it is the angle between the normal vectors of the two planes. (In this case there is only one nonzero principal angle between non-intersecting planes.) We also define the following quantities for notational convenience:

\begin{definition}\label{SpecialSupportSet}
Given $A \in \mathbb{R}^{n \times m}$ and $k < m$, we define for $k \geq 2$:
\begin{align}\label{rho}
\phi_k(A) := \min_{ \mathcal{F} \in {[m] \choose k} } \left(1 - \xi( \{\text{Span}\{A_S\} : S \in \mathcal{F} \} ) \right),
\end{align}
%
where for any set $\mathcal{V} = \{V_1, \ldots, V_p\}$ of closed subspaces of $\mathbb{R}^m$, 
\begin{align}
\xi(\mathcal{V}) := \min_{\sigma \in \Sigma_p} \left(1 - \prod_{i=1}^{p-1} \sin^2\left[ \theta_F(V_{\sigma(i)}, \cap_{j=i+1}^p V_{\sigma(j)}) \right]  \right)^{1/2}.
\end{align}
%
and we set $\phi_1(A) := 1$.
\end{definition}

We will not attempt to provide intuition for this quantity, other than to say it is based on a value derived in \cite{Deutsch} to describe the convergence of the alternating projections algorithm for projecting a point onto the intersection of a set of subspaces. We use it to bound the distance between a point and the interesection of a set of subspaces given an upper bound on the distance from that point to each individual subspace. 

Given positive integers $k < m$ and some permutation $\sigma \in \Sigma_m$ (where $\Sigma_m$ denotes the symmetric group on $m$ elements), let
\begin{align}
S_\sigma(i) := \{1+\sigma(i), \ldots, 1+\sigma(i + (k-1)) \} \indent \text{for} \indent i = 0, \ldots, m-1
\end{align}
%
with addition modulo $m$. In words, the $S_\sigma(i)$ are the intervals of length $k$ within the set of elements of $\{1, \ldots, m\}$ arranged in the cyclic order $\sigma(1), \ldots, \sigma(m)$.

%=== SPECIFICS OF DETERMINISTIC THEOREM ===%

\begin{remark}\label{thm1specs}
We can calculate the error threshold and stability constants for the stable $k$-sparse representation referred to in Theorem \ref{DeterministicUniquenessTheorem} as follows. Letting $X  = (\mathbf{a}_1 \cdots \mathbf{a}_N) \in \mathbb{R}^{n \times N}$, the $\mathbf{a}_i$ are constructed to have supports $S_{\sigma(1)}, \ldots, S_{\sigma(m)}$ for some $\sigma \in \Sigma_m$ and so that $L_k(AX;\sigma) > 0$, where
\begin{align}\label{Ldef}
L_k(AX;\sigma) := \min_{i \in [m]} \ell_k(AX_{J(S_\sigma(i))}) \indent \text{where } J(S) = \{j : \text{supp}(\mathbf{a}_j) = S\}.
\end{align}
If $A$ has unit norm columns, then for any matrix $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ for which
\begin{align}\label{ErrorBound}
\|\mathbf{y}_i - B\mathbf{b}_i\| < \frac{ \phi_k(A) \ell_{k}(A) L_k(AX;\sigma) }{\sqrt{2k^3} } \indent \text{for all } i = 1, \ldots, N
\end{align}
%
we have
\begin{align}
C_1 = \frac{\sqrt{k^3}}{\phi_k(A)L_k(AX;\sigma)} 
\indent \text{and} \indent 
C_2 = \frac{ \|D\|_2}{\ell_{2k}(A)}  \left( 1 + C_1\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right).
\end{align}
\end{remark}

It is important for us to note where this result fits in the field of \emph{theoretical dictionary learning}. While a collection of greedy optimization algorithms have been proposed for solving \eqref{DictionaryLearning}, as yet there exist no global convergence guarantees for these techniques. Local identifiability analyses have demonstrated that the global minimum is a local minimum of cost functions of L0 and L1 cost functions, (local) convergence guarantees of greedy and convex algorithms. A key corollary to our theorem is a closed form expression from which one can check when any dictionary learning algorithm has converged to a global minimum of \eqref{DictionaryLearning} by asserting whether or not the reconstruction error of the proposed solution is indeed small enough. 

It is also informative to elaborate on the relationship of our results to the field of compressive sensing. [ref?] CS theory provides conditions under which it is possible to recover data vectors $\mathbf{x}$ with sparse structure after they have been linearly subsampled as $\mathbf{y} = \Phi \mathbf{x}$ by a known compression matrix $\Phi$. The sparsity usually enforced is that $\mathbf{x} = \Psi\mathbf{a}$ for some known $\Psi$ and $k$-sparse $\mathbf{a}$, where $k \ll m$. As explained previously, a necessary condition for the unique recovery of $\mathbf{a}$ given $\mathbf{y}$ is that the generation matrix $A = \Phi\Psi$ satisfy the spark condition \eqref{SparkCondition}. Provided the dimension $n$ of $\mathbf{y}$ satisfies
\begin{align}\label{CScondition}
n \geq Ck\log\left(\frac{m}{k}\right),
\end{align}
%
the theory guarantees that with high probability a randomly generated $\Phi$ will yield an $A$ satisfying \eqref{SparkCondition}. In contrast to CS, the goal of SCA is to recover both the code vectors \emph{and} the generation matrix from measurements. We show that the same uniqueness conditions required by CS also guarantee uniqueness in SCA (up to inherent permutation and scaling ambiguities) given enough samples distributed over suitable supports.

The organization of the rest of this paper is as follows. In Section \ref{DUT} we state our main tool from combinatorial matrix theory and then derive from it Theorem \ref{DeterministicUniquenessTheorem}. In Section \ref{ProbabilisticTheorems} we state probabilistic versions of Theorem \ref{DeterministicUniquenessTheorem} and give brief proofs based largely on arguments outlined in \cite{HS11}. In Section \ref{mleqm} we describe how to extend all of these results to the case where we know only an upper bound on the dimensionality of the sparse sources. The final section is a discussion, and an appendix contains the proof of Lemma \ref{MainLemma}.

\textbf{[Have the cool small dimensional example here. Data in $3D$-space coded by planes. $n=3, k=2$.]}


 
%============================================
% PROOF OF DETERMINISTIC UNIQUENESS THEOREM
%============================================

\section{Deterministic Stability Theorem}\label{DUT}

%======== CASE K = 1 ============

Before proving Theorem \ref{DeterministicUniquenessTheorem} in full generality it is illustrative to consider the case when $k=1$. In this simple case, we only need $N = m$ samples to guarantee the stability of the sparse representation. Set $\mathbf{a}_i = \mathbf{e}_i$ $(i = 1, \ldots, m)$ to be the standard basis vectors in $\mathbb{R}^m$ and fix some $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition} with unit norm columns. Suppose now that for some $B \in \mathbb{R}^{n \times m}$ and 1-sparse $\mathbf{b}_i \in \mathbb{R}^m$ we have  $\|A\mathbf{a}_i - B\mathbf{b}_i\|_2 \leq \varepsilon < \frac{\ell_2(A)}{\sqrt{2}}$ for all $i \in [m]$. Since the $\mathbf{b}_i$ are 1-sparse, there must exist $c_1, \ldots, c_m \in \mathbb{R}$ such that 
\begin{align}\label{1D}
\|A\mathbf{e}_i - c_iB\mathbf{e}_{\pi(i)}\|_2 \leq \varepsilon \indent \text{for all} \indent i \in [m].
\end{align}
for some map $\pi: [m] \to [m]$. 
Note that if $c_i = 0$ for some $i$ then $\|A\mathbf{e}_i\| < 1$ (since $\ell_2(A) \leq 1$) which contradicts that $A$ has unit norm columns. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell \in [m]$. Then $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2  \leq \varepsilon$ and $\|A\mathbf{e}_j - c_jB\mathbf{e}_{\ell}\|_2 \leq \varepsilon$. Summing and scaling these two inequalities by $|c_j|$ and $|c_i|$, respectively, we apply the triangle inequality and then \eqref{RIP} to yield
\begin{align*}
(|c_i| + |c_j|) \varepsilon
&\geq |c_j|\|A\mathbf{e}_i - c_iB\mathbf{e}_{\ell}\|_2 + |c_i|\|c_jB\mathbf{e}_{\ell} - A\mathbf{e}_j\|_2 \\
&\geq \|c_jAe_i + c_iAe_j\|_2 \\
&\geq \ell_2(A)\|c_je_i + c_ie_j\|_2
\end{align*}
%
which is in contradiction with the fact that $\|x\|_1 \leq \sqrt{2}\|x\|_2$ for all $x \in \mathbb{R}^2$ and $\varepsilon < \frac{\ell_2(A)}{\sqrt{2}}$. Hence, $\pi$ is injective. Letting $P$ and $D$ denote the following permutation and diagonal matrices, respectively:
\begin{equation}\label{PandD}
P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right), \ \ D = \left(\begin{array}{ccc}c_1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & c_m\end{array}\right),
\end{equation}
%
we see that \eqref{1D} becomes $\|(A - BPD)\mathbf{e}_i\|_2 \leq C\varepsilon$ for all $i \in [m]$, where $C = 1$. 

% ======== b - PDa =========
We note that from this result we can, in general, derive an upper bound on $\|\mathbf{b}_i - PD\mathbf{a}_i\|$ as well. By the triangle inequality, we have that $\|(A-BPD)\mathbf{x}\| \leq \|\mathbf{x}\|_1C\varepsilon$ for all $\mathbf{x} \in \mathbb{R}^m$. Hence,

\begin{align*}
\|\mathbf{b}_i - PD\mathbf{a}_i\| 
&\leq \frac{1}{\ell_{2k}(B)}\|B(\mathbf{b}_i - PD\mathbf{a}_i)\| \\
&\leq \frac{1}{\ell_{2k}(B)} (\|B\mathbf{b}_i - A\mathbf{a}_i\| + \|(A - BPD)\mathbf{a}_i\|) \\
&\leq \frac{\varepsilon}{\ell_{2k}(B)}(1+C\|\mathbf{a}_i\|_1).
\end{align*}

We have by the triangle inequality that for all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$,
\begin{align}
\|BPD\mathbf{x}\| \geq \|A\mathbf{x}\| - \|(A-BPD)\mathbf{x}\| \geq (\ell_{2k}(A) - C\varepsilon)\|\mathbf{x}\|_1.
\end{align}

Hence $\ell_{2k}(B) \geq \ell_{2k}(A) - C\varepsilon \geq \ell_{2k}(A) - \ell_k(A)/\sqrt{2}$ (see Remark \ref{thm1specs}). \textbf{Can we show that $\ell_{2k}(A) \geq \ell_k(A)/\sqrt{2}$?}

Alternatively, 
\begin{align*}
\|\mathbf{b}_i - PD\mathbf{a}_i\|_2 &=  \|PD(D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i)\|_2 \\
&\leq \|D\|_2 \|D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i\|_2 \\
&\leq \frac{\|D\|_2}{\ell_{2k}(A)} \|A(D^{-1}P^{-1}\mathbf{b}_i - \mathbf{a}_i)\|_2 \\
&\leq \frac{ \|D\|_2}{\ell_{2k}(A)} \left(\|(A - BPD)D^{-1}P^{-1}\mathbf{b}_i\|_2 + \|B\mathbf{b}_i - A\mathbf{a}_i\|_2 \right) \\
&\leq \frac{\varepsilon \|D\|_2}{\ell_{2k}(A)} \left( 1 + C\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right)
\end{align*}


% ========== DEFINITIONS FOR MAIN LEMMA (K>1) ================

Unfortunately, the proof for larger $k$ is more challenging since in general it is nontrivial to produce $P$ and $D$ as in \eqref{PandD}. Our main tool for the proof is a result in combinatorial linear algebra, which we state after making a couple more necessary definitions.

\begin{definition}
Let $V, W$ be subspaces of $\mathbb{R}^m$ and let $d(v,W) := \inf\{\|v-w\|_2: w \in W\} = \|v - \Pi_W v\|_2$ where $\Pi_W$ is the orthogonal projection operator onto subspace $W$. The \emph{gap} metric $\Theta$ on subspaces of $\mathbb{R}^{m}$ is \cite{TheoryOfLinearOperatorsPage69}:
\begin{equation}\label{SubspaceMetric}
\Theta(V,W) := \max\left( \sup_{\substack{v \in V \\ \|v\| = 1}} d(v,W), \sup_{\substack{w \in W \\ \|w\| = 1}} d(w,V) \right).
\end{equation}
\end{definition}
%
In our proof we will make use of the following useful facts about $d$. The first, 
\begin{equation}\label{SubspaceMetricSameDim}
\dim(W) = \dim(V) \implies \sup_{\substack{v \in V \\ \|v\| = 1}}  d(v,W)  = \sup_{\substack{w \in W \\ \|w\| = 1}} d(w,V),
\end{equation}
%
is proven in \cite{Morris10} (Lemma 3.3). The second is:
\begin{lemma}\label{MinDimLemma}
Let $V, W$ be subspaces of $\mathbb{R}^{m}$. Then
\begin{equation}\label{MinDim}
d(v,W) < \|v\|_2 \indent \forall v \in V \implies \dim(V) \leq \dim(W).
\end{equation}
\end{lemma}

\begin{proof}
If $\dim(V) > \dim(W)$ then there exists some $v' \in V \cap W^\perp$ and by Pythagoras' Theorem, $\|v' - w\|_2^2 = \|v'\|_2^2 + \|w\|_2^2 \geq \|v\|_2^2$ for all $w \in W$. This is in contradiction with the LHS of \eqref{MinDim}, which states that for all $v \in V$ there exists some $w \in W$ such that $\|v - w\|_2 < \|v\|_2$.
\end{proof}

We are now in a position to state our main result from combinatorial matrix theory.

%===========          MAIN LEMMA (K > 1)             =================

\begin{lemma}[Main Lemma]\label{MainLemma}
Fix positive integers $n, m$ and $k$ such that $k < m$. Let $A, B \in \mathbb{R}^{n \times m}$ and suppose that $A$ satisfies the spark condition \eqref{SparkCondition} with unit norm columns. If there exists for some permutation $\sigma \in \Sigma_m$ a map $\pi: \{S_{\sigma}(1), \ldots, S_{\sigma}(m)\} \to {[m] \choose k}$ and some $\Delta < \frac{\ell_{2k}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound}
\Theta(\text{Span}\{A_{S_{\sigma}(i)}\}, \text{Span}\{B_{\pi(S_{\sigma}(i))}\}) \leq \frac{ \phi_k(A) }{k} \Delta \indent \text{for all} \indent i \in [m]
\end{equation}
%
then there exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that
\begin{align}
\|A - BPD\|_2 \leq \Delta.
\end{align}
\end{lemma}

We defer the proof of this lemma to the Appendix. 

%========          PROOF OF THEOREM 1        ============

\begin{proof}[Proof of Theorem \ref{DeterministicUniquenessTheorem}]
Fix positive integers $n, m$ and $k < m$. We assume $k > 1$ (the $k=1$ case was proven for $N=m$ at the beginning of this section). First, we produce a set of $N = mk{m \choose k}$ vectors in $\mathbb{R}^k$ in general linear position (i.e. any set of $k$ of them are linearly independent). Specifically, let $\gamma_1, ..., \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ "Vandermonde" sub-determinant is nonzero). Next, fix some $\sigma \in \Sigma_m$ and form the $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with supports $S_\sigma(j)$ for $j \in [m]$ (partitioning the $a_i$ evenly among these supports, i.e. for each support there are $k{m \choose k}$ vectors $a_i$ with that support) by setting the nonzero values of vector $\mathbf{a}_i$ to be those contained in the $i$th column of $V$.

We claim that any $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} is such that $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$. Fix $A$ satisfying \eqref{SparkCondition} and let it have unit norm columns (it is not hard to show that this can be done without loss of generality). Suppose that for some $B \in \mathbb{R}^{n \times m}$ there exist $k$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ such that $\|A\mathbf{a}_i - B\mathbf{b}_i\| \leq \varepsilon$ for all $i \in \{1, \ldots, N\}$. Since there are $k{m \choose k}$ vectors $\mathbf{a}_i$ with a given support $S_\sigma(j)$, $j \in [m]$, the pigeon-hole principle implies that there is some set of indices $J$ of cardinality (at least) $k$ such that all $\mathbf{a}_i$ and $\mathbf{b}_i$ with $i \in J$ have supports $S_\sigma(j)$ and $S' \in {[m] \choose k}$, respectively.

Let $X = (\mathbf{a}_1 \cdots \mathbf{a}_N)$ and $X' = (\mathbf{b}_1 \cdots \mathbf{b}_N)$ be the matrices formed by horizontally stacking the column vectors $\mathbf{a}_i$ and, repsectively, the $\mathbf{b}_i$. It follows from the general linear position of the $\mathbf{a}_i$ and the linear independence of every $k$ columns of $A$ that the columns of $AX_J$ are linearly independent and form a basis for $\text{Span}\{A_{S_\sigma(j)}\}$. Hence, fixing $\mathbf{z} \in \text{Span}\{A_{S_\sigma(j)}\}$, there exists a unique $\mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{z} = \sum_i c_iAX_J\mathbf{e}_i$. Letting $\mathbf{z'} = \sum_i c_iBX'_J\mathbf{e}_i$, which is in $\text{Span}\{B_{S'}\}$, we have:
\begin{align*}
\|\mathbf{z} - \mathbf{z'}\|_2 = \|\sum_ic_i(AX - BX')_J\mathbf{e}_i\|_2 
\leq \sum_i |c_i| \|(AX - BX')_J\mathbf{e}_i\|_2  
\leq \varepsilon \sqrt{k} \|\mathbf{c}\|_2 
\leq \frac{\varepsilon \sqrt{k} \|\mathbf{z}\|_2}{\ell_k(AX_J)}.
\end{align*}

(We can be sure that $\ell_k(AX_J) > 0$ since the columns of $AX_J$ are linearly independent.) It follows that $d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \varepsilon\sqrt{k} / \ell_k(AX_J)$ for all unit vectors $\mathbf{z} \in \text{Span}\{A_{S_\sigma(j)}\}$. Hence,
\begin{align}\label{ABSubspaceDistance}
\sup_{ \substack{ \mathbf{z} \in \text{Span}\{A_{S_\sigma(j)}\} \\ \|\mathbf{z}\| = 1} } d(\mathbf{z}, \text{Span}\{B_{S'}\}) \leq \frac{\varepsilon\sqrt{k}}{\ell_k(AX_J)}.
\end{align}

Now, suppose that $\varepsilon <  \phi_k(A) \ell_{k}(A) L_k(AX;\sigma) / \sqrt{2k^3}$. Since $k \geq 1$ and $\ell_k(A), \phi_k(A) \leq 1$, it follows from the definition of $L_k$ in \eqref{Ldef} that $\varepsilon < \ell_k(AX_J)/\sqrt{k}$ and by Lemma \ref{MinDimLemma} applied to \eqref {ABSubspaceDistance} that $\dim(\text{Span}\{B_{S'}\}) \geq \dim(\text{Span}\{A_S\}) = k$ (since every $k$ columns of $A$ are linearly independent). In fact, since $|S'| = k$, we have $\dim(\text{Span}\{B_{S'}\}) \leq k$, hence $\dim(\text{Span}\{B_{S'}\}) = \dim(\text{Span}\{A_S\})$. Recalling \eqref{SubspaceMetricSameDim},  we see the association $S \mapsto S'$ thus defines a map $\pi: \{S_{\sigma}(1), \ldots, S_{\sigma}(m)\} \to {[m] \choose k}$ satisfying
\begin{align*}
\Theta(\text{Span}\{A_{S_\sigma(i)}\}, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{\varepsilon\sqrt{k}}{\ell_k(AX_J)} \indent \text{for all } i = 1, \ldots, m.
\end{align*}

It follows by Lemma \ref{MainLemma} (setting $\Delta =  \frac{\epsilon \sqrt{k^3} }{ \phi_k(A) L_k(AX;\sigma) })$ that there exists a permutation matrix $P \in \mathbb{R}^{m \times m}$ and a diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that for all $i \in \{1, \ldots, m\}$,
$\|A - BPD\|_2 \leq C\varepsilon$ for $C = \frac{\sqrt{k^3}}{ \phi_k(A)L_k(AX;\sigma) }$.
\end{proof}

%======================================
% PROBABILISTIC THEOREMS
%======================================

\section{Probabilistic Stability Theorems}\label{PUT}

We next give precise statements of our probabilistic versions of Theorem 1 along with brief proofs relying largely to the methods used in \cite{HS11}. Our statements are based upon the following construction of \emph{random sparse vectors}.

\begin{definition}[Random $k$-Sparse Vectors]\label{RandomDraw}
Given the support set for its $k$ nonzero entries, a random draw of $\mathbf{a}$ is the $k$-sparse vector with support entries chosen uniformly from the interval $[0, 1] \subset \mathbb{R}$, independently. When a support set is not specified, a random draw is a choice of one support set uniformly from all ${m \choose k}$ of them and then a random draw.
\end{definition}

Our proofs rely on the following lemma adapted from \cite{HS11}:
\begin{lemma}\label{HS11lemma2}
Fix positive integers $n, m$ and $k < m$ and a matrix $M \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. With probability one, $M\mathbf{a}_1, \ldots, M\mathbf{a}_k$ are linearly independent whenever the $\mathbf{a}_i$ are random $k$-sparse vectors.
\end{lemma}

\begin{theorem}\label{Theorem2}
Fix positive integers $k < m$ and $n$, and a generation matrix $A \in \mathbb{R}^{n \times m}$ satisfying spark condition \eqref{SparkCondition}. If $N = mk{m \choose k}$ $k$-sparse $\mathbf{a}_i$ are randomly drawn with supports equipartitioned between $S_\sigma(1), \ldots, S_\sigma(m)$ for some $\sigma \in \Sigma_m$ then $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{theorem}

\begin{proof}
Fix  $\sigma \in \Sigma_m$ and let $X = (\mathbf{a}_1 \cdots \mathbf{a}_N)$. By Lemma \ref{HS11lemma2}, with probability one every set of $k$ columns of $AX$ are linearly independent, i.e. $\ell_k(AX) > 0$. It follows by Theorem \ref{DeterministicUniquenessTheorem} that $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one. 
\end{proof} 

\begin{corollary}
Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random\footnote{Many ensembles of random matrices work, e.g. \cite{??}} $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing such an $A$, we have with probability one that a dataset $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N\}$ generated from $N = mk{m \choose k}$ $k$-sparse samples $\mathbf{a}_i$ equipartitioned over supports in $S_\sigma(1), \ldots, S_\sigma(m)$ for some $\sigma \in \Sigma_m$ has a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}
We now state our third theorem. Note that an \emph{algebraic set} is a solution to a finite set of polynomial equations. 

\begin{theorem}\label{Theorem3}
Fix positive integers $k < m$ and $n$. If $k{m \choose k}$ $k$-sparse $\mathbf{a}_i$ are randomly drawn from each support set in $S_\sigma(1), \ldots, S_\sigma(m)$ for some $\sigma \in \Sigma_m$, then with probability one the following holds. There is an algebraic set $Z \subset \mathbb{R}^{n \times m}$ of Lebesgue measure zero with the following property: if $A \notin Z$ then $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ has a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{theorem}

\begin{proof}
By Lemma \ref{HS11lemma2} (setting $M$ to be the identity matrix) we have that with probability one the $\mathbf{a}_i$ are in general linear position. Moreover, by the same arguments made in the proof of Theorem 3 in \cite{HS11}, the set of matrices $A$ that fail to satisfy \eqref{SparkCondition} form an algebraic set of measure zero. Letting $X = (\mathbf{a}_1 \cdots \mathbf{a}_N)$, we therefore have with probability one that $\ell_k(AX) > 0$ for all but an algebraic set of matrices $A$ of measure zero. The result then follows by Theorem \ref{DeterministicUniquenessTheorem}.
\end{proof}

\begin{corollary}
Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}. With probability one, a random draw of $N = mk{m \choose k}$ $k$-sparse samples equipartitioned between $S_\sigma(1), \ldots, S_\sigma(m)$ for some $\sigma \in \Sigma_m$ satisfies: almost every matrix $A$ gives $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

%===================================
% DIFFERENT CODING DIMENSIONS
%===================================

\section{What if we don't know $m$?}\label{mleqm}

In this section we state a version of Theorem \ref{DeterministicUniquenessTheorem} and Lemma \ref{MainLemma} assuming that $B$ also satisfies the spark condition (in addition to $A$ satisfying the spark condition). With this additional assumption, we can address the issue of recovering $A \in \mathbb{R}^{n \times m}$ and the $\mathbf{a}_i \in \mathbb{R}^m$ when only an upper bound $m'$ on the number $m$ of sparse sources is known.

\begin{theorem}\label{DeterministicUniquenessTheorem2}
Given positive integers $n, m, m'$ and $k$ with $k < m \leq m'$, there exist $N =  mk{m' \choose k}$ $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with the following property: every pair of matrices $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfying spark condition \eqref{SparkCondition} are such that if $\| A\mathbf{a}_i - B\mathbf{b}_i \|_2 \leq \varepsilon$ for all $i = 1, \ldots, N$ for some $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^{m'}$ then 
\begin{align}
&\|A - BPD\|_2 \leq C_1 \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|_2 \\
\indent \text{and} \indent
&\|\mathbf{b}_i - PD\mathbf{a}_i\|_2 \leq C_2 \max_i \|\mathbf{y}_i - B\mathbf{b}_i\|_2
\end{align}
%
for some $C_1, C_2 > 0$, partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and diagonal matrix $D \in \mathbb{R}^{m' \times m}$, provided $\varepsilon$ is small enough.

Specifically, letting $X  = (\mathbf{a}_1 \cdots \mathbf{a}_N) \in \mathbb{R}^{n \times N}$, the $\mathbf{a}_i$ are constructed to have supports $S_\sigma(1), \ldots, S_\sigma(m)$ for some $\sigma \in \Sigma_m$ and so that $L_k(X;\sigma)$ as defined in \eqref{Ldef} is positive. If $A$ has unit norm columns and
\begin{align*}
\|\mathbf{y}_i - B\mathbf{b}_i\| < \left( \frac{ \ell_{k}(A) L_k(AX;\sigma) }{\sqrt{2k^3} } \right) \min(\phi_k(A), \phi_k(B)) \indent \text{for all} \indent i = 1, \ldots, N
\end{align*}
%
then
\begin{align}
C_1 = \frac{\sqrt{k^3}}{\phi_k(A) L_k(AX;\sigma)} 
\indent \text{and} \indent 
C_2 = \frac{ \|D\|_2}{\ell_{2k}(A)}  \left( 1 + C_1\|D^{-1}P^{-1}\mathbf{b}_i\|_1 \right).
\end{align}
\end{theorem}

The proof of Theorem \ref{DeterministicUniquenessTheorem2} is very similar to the proof of Theorem \ref{DeterministicUniquenessTheorem}, the difference being that now we establish a map $\pi: [m] \to [m']$ satisfying the requirements of Lemma \ref{MainLemma2}, the statement of which follows, by pigeonholing $k{m' \choose k}$ vectors with respect to $[m']$ holes. The assumption that $B$ also satisfy the spark condition allows us to overcome the complication due to $m < m'$ to prove Lemma \ref{MainLemma2}, since the proof of Lemma \ref{NonEmptyLemma} relies on the fact that $m = m'$. 

\begin{lemma}[Main Lemma for $m \leq m'$]\label{MainLemma2}
Fix positive integers $n, m, m'$ and $k$ with $k < m \leq m'$. Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfy spark condition \eqref{SparkCondition}, with $A$ having unit norm columns. If there exists for some permutation $\sigma \in \Sigma_m$ a map $\pi: \{S_{\sigma}(i), \ldots, S_{\sigma}(m-1)\} \to {[m'] \choose k}$ and some $\Delta < \frac{\ell_{2k}(A)}{\sqrt{2}}$ such that 
\begin{equation}\label{GapUpperBound2}
\Theta(\text{Span}\{A_{S_\sigma(i)}\}, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{ \min(\phi_k(A), \phi_k(B)) }{k} \Delta \indent \text{for all} \indent i \in [m]
\end{equation}
%
then there exist a partial permutation matrix $P \in \mathbb{R}^{m' \times m'}$ and a diagonal matrix $D \in \mathbb{R}^{m' \times m}$ (i.e. $D_{ij} = 0$ whenever $i \neq j$) such that
\begin{align}
\|A - BPD\|_2 \leq \Delta \indent \text{for all} \indent i \in [m].
\end{align}
\end{lemma}

We again defer the proof of this lemma to the Appendix, and note that if the reader is wiling to impose the additional constraint assumed in Section \ref{mleqm} then our probabilistic theorems can also be extended to the case where only an upper bound on $m$ is known.

%===================================
% 			DISCUSSION
%===================================

\section{Discussion}


Another implication of our theorem is the determination of an overcompleteness regime dimensionality of any dataset with respect to $\varepsilon$. If you coded very well some data sparsely then you can do a check and if that check holds up then anyone else who codes the data sparsely too has learned the same dictionary. 

Approximating the optimal solution to \eqref{DictionaryLearning} has been shown in general to be NP-hard \cite{Razaviyayn} \textbf{[Razaviyayn 2015, but they did it within an additive error. Tollman 2015 did it for multiplicative error for the dual problem. Should we say this isn't necessarily a big deal since there may be some \emph{particular} instances where NP-hard problems are solvable in poly time?}. 

The theory of CS informs also informs another practical consequence of our result. Since our derived sample complexity is independent of the ambient dimension of the data, $n$, given a lower bound on the sparsity of the latent variables $\mathbf{a}_i$ we can generate a random matrix to compress the data to a dimension in the regime of \eqref{CScondition} before applying a dictionary learning. This could significantly reduce the computational cost of dictionary learning when the sparsity is high by reducing the number of parameters required to define each dictionary element. Such improvements are crucial to scaling up dictionary learning to larger datasets. \textbf{[Is this actually a significant computational boost..?]}

Uniqueness in data analysis.


Identifying uniqueness: the bispectrum

%========================================
%      	APPENDIX: COMBINATORICS
%========================================

\appendices
\section{Combinatorial Matrix Theory}

In this section, we prove Lemma \ref{MainLemma}, which is the main ingredient in our proof of Theorem \ref{DeterministicUniquenessTheorem}. We suspect that there is an appropriate generalization to matroids. For readers willing to assume a priori that the spark condition holds for $B$ as well as for $A$, a proof of this case (Lemma \ref{MainLemma2} from section \ref{mleqm}) is provided in Appendix \ref{mleqmAppendix}. The additional assumption simplifies the proof and allows us to extend stability conditions to the case where only an upper bound on $m$ is known a priori. We now prove some auxiliary lemmas before deriving Lemma \ref{MainLemma}.

%===== SPAN INTERSECTION LEMMA =====

\begin{lemma}\label{SpanIntersectionLemma}
Let $M \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $M$ are linearly independent, then for any $\mathcal{T} \subseteq \bigcup_{\ell \leq k} {[m] \choose \ell}$,
\begin{align}
y \in \text{Span}\{M_{\cap \mathcal{T}}\}  \Longleftrightarrow y \in \bigcap_{S \in \mathcal{T}} \text{Span}\{M_S\}.
\end{align}
\end{lemma}

\begin{proof}The forward direction is trivial; we prove the reverse direction. Enumerate $\mathcal{T} = (S_1, \ldots, S_{|\mathcal{T}|})$ and let $\mathbf{y} \in \text{Span}\{M_{S_1}\} \cap \text{Span}\{M_{S_2}\}$. Then there exists some $\mathbf{x}_1$ with support contained in $S_1$ such that $\mathbf{y} = M\mathbf{x}_1$ and some $\mathbf{x}_2$ with support contained in $S_2$ such that $\mathbf{y} = M\mathbf{x}_2$. We therefore have $M(\mathbf{x}_1 - \mathbf{x}_2) = 0$, which implies that $\mathbf{x}_1 = \mathbf{x}_2$ by the spark condition. Hence $\mathbf{x}_1$ and $\mathbf{x}_2$ have the same support contained in both $S_1$ and $S_2$, i.e. $\mathbf{y} \in \text{Span}\{M_{S_1 \cap S_2}\}$. This carries over by induction to the entire sequences of supports in $\mathcal{T}$. 
\end{proof}

%===== DISTANCE TO INTERSECTION LEMMA =====

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $\mathcal{V} = \{V_1, \ldots, V_k\}$ be a set of closed subspaces of $\mathbb{R}^m$ and let $V = \cap \mathcal{V}$. For every $x \in \mathbb{R}^m$,
\begin{align}\label{DTILeq}
\|x - \Pi_V x\|_2 \leq \frac{1}{1 - \xi(\mathcal{V})} \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2.
\end{align}
\end{lemma}
%
where the expression for $\xi$ is given in Definition \ref{SpecialSupportSet}.

\begin{proof} 
Fix $x \in \mathbb{R}^m$ and $k \geq 2$. The proof can be subdivided into two steps. First, we will show that for any $\sigma \in \Sigma_k$,
\begin{align}\label{induction}
\|x - \Pi_Vx\|_2 \leq \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2 + \|\Pi_{V_{\sigma(k)}}\Pi_{V_{\sigma(k-1)}}\cdots\Pi_{V_{\sigma(1)}} x - \Pi_V x\|_2.
\end{align}
%
Assume without loss of generality that $\sigma(i) = i$ for all $i \in [m]$. We have by the triangle inequality that
\begin{align*}
\|x - \Pi_Vx\|_2 &= \|x - \Pi_{V_k} x\|_2 + \|\Pi_{V_k}(I - \Pi_{V_{k-1}}) x\|_2 + \|\Pi_{V_k}\Pi_{V_{k-1}}x - \Pi_Vx\|_2 \\
&\leq \sum_{i=k-1}^k\|x - \Pi_{V_i} x\|_2 + \|\Pi_{V_k}\Pi_{V_{k-1}} x - \Pi_V x\|_2,
\end{align*}
%
where in the second line we have used the fact that $\|\Pi_{V_k}\|_2 \leq 1$. If $k=2$ then we are done; otherwise, we may repeat this manipulation another $k-2$ times until we arrive at \eqref{induction}. Next, we show how the result \eqref{DTILeq} follows from \eqref{induction}. To do so, we make use of the following result from \cite{Deutsch} (Theorem 9.33):
\begin{align}\label{dti2}
\|\left(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}\right)x - \Pi_Vx\|_2 \leq c \|x\|_2 \indent \text{for all} \indent x \in \mathbb{R}^m.
\end{align}
%
where $c:= \left[1 - \prod_{i=1}^{k-1}(1-c_i^2)\right]^{1/2}$ and $c_i = \cos\theta_F\left(V_i, \cap_{j=i+1}^kV_j\right)$. Note that
\begin{align}\label{dti1}
\|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)\|_2 
&= \|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}) x - \Pi_V x \|_2,
\end{align}
%
since $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$.
%
We therefore have by \eqref{dti2} and \eqref{dti1} that
\begin{align*}
\|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1}) x - \Pi_V x \|_2
&= \|(\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1})(x - \Pi_Vx) - \Pi_V(x - \Pi_Vx)\|_2 \\
&\leq c \|x - \Pi_Vx\|_2.
\end{align*}

Substituting the LHS into \eqref{induction}, we get
\begin{align*}
\|x - \Pi_Vx\|_2 \leq \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2 + c \|x - \Pi_Vx\|_2,
\end{align*}
%
from which it follows that
\begin{align}\label{ceq}
\|x - \Pi_V x\|_2 \leq \frac{1}{1 - c} \sum_{i=1}^k \|x - \Pi_{V_i} x\|_2.
\end{align}
Since the permutation $\sigma$ we chose was arbitrary, we can replace $c$ in \eqref{ceq} with $\xi(\mathcal{V})$ to obtain the result \eqref{DTILeq}.
\end{proof}

%======= GRAPH THEORY LEMMA =======

\begin{lemma}\label{NonEmptyLemma} Fix positive integers $k < m$ and $\sigma \in \Sigma_m$ and let $\mathcal{T} = \{S_\sigma(1), \ldots, S_\sigma(m)\}$. Suppose there exists a map $\pi: \mathcal{T} \to {\mathbb{Z}/m\mathbb{Z} \choose k}$ such that for all $J \in {[m] \choose k}$,
\begin{align}\label{EmptyToEmpty}
 \bigcap_{i \in J} S_\sigma(i) = \emptyset \Longrightarrow \bigcap_{i \in J} \pi(S_\sigma(i)) = \emptyset.
\end{align}
%
Then  $\pi(S_\sigma(i)) \cap \cdots \cap \pi(S_\sigma(i+(k-1))) \neq \emptyset$ for all $i \in \mathbb{Z}/m\mathbb{Z}$.
\end{lemma}

\begin{proof} Consider the set $Q_m = \{ (i,j) : i \in \mathbb{Z}/m\mathbb{Z}, j \in \pi(S_\sigma(i)) \}$, which has $mk$ elements. By the pigeon-hole principle, there is some $p \in \mathbb{Z}/m\mathbb{Z}$ and $J \in {[m] \choose k}$ such that $(i, p) \in Q_m$ for all $i \in J$. Hence, $p \in \cap_{i \in J} \pi(S_\sigma(i))$ and by \eqref{EmptyToEmpty} there must be some $v \in \mathbb{Z}/m\mathbb{Z}$ such that $\sigma(v) \in \cap_{i \in J} S_\sigma(i)$. This is only possible (since $|J| = k$) if the elements of $J$ are consecutive in $\mathbb{Z}/m\mathbb{Z}$, i.e. $J = \{v - (k-1), \ldots, v\}$. (The only elements in $\mathcal{T}$ that contain $\sigma(v)$ are $S_\sigma(v-(k-1)), \ldots, S_\sigma(v)$.)

If there existed some $i \notin J$ such that $p \in \pi(S_\sigma(i))$ then we would have $p \in \pi(S_\sigma(i)) \cap \pi(S_\sigma(v - k+1)) \cap \cdots \cap \pi(S_\sigma(v))$ and \eqref{EmptyToEmpty} would imply that the intersection of every $k$-element subset of $\{S_\sigma(i)\} \cup \{S_\sigma(j): j \in J\}$ is non-empty. This would only be possible if $\{i\} \cup J = \mathbb{Z}/m\mathbb{Z}$, in which case the result then trivially holds since then $p \in \pi(S_\sigma(i))$ for all $i \in [m]$. If there exists no additional $i \notin J$ such that $p \in \pi(S_\sigma(i))$ then by letting $Q_{m-1} \subset Q_m$ be the set of elements of $Q_m$ not having $p$ as a second coordinate we have $|Q_{m-1}| = (m-1)k$ and the result follows by iterating the above arguments.
\end{proof}

%==== PROOF OF MAIN LEMMA =======

\begin{proof}[Proof of Lemma \ref{MainLemma} (Main Lemma)]
We assume $k \geq 2$ since the case $k = 1$ was proven at the beginning of Section \ref{DUT}. We begin by proving that $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$ for all $i \in [m]$. Fix $i \in [m]$ and note that by \eqref{GapUpperBound} we have for all unit vectors $u \in \text{Span}\{A_{S_\sigma(i)}\}$ that $d(u, \text{Span}\{B_{\pi(S_\sigma(i))}\}) \leq \frac{\phi_k(A)}{k} \Delta$. Note also that $\frac{\phi_k(A)}{k} \Delta < 1$; hence by Lemma \ref{MinDimLemma} we have $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) \geq \dim(\text{Span}\{A_{S_\sigma(i)}\}) = k$. Since $|\pi(S_\sigma(i))| = k$ we in fact have $\dim(\text{Span}\{B_{\pi(S_\sigma(i))}\}) = k$, i.e. the columns of $B_{\pi(S_\sigma(i))}$ are linearly independent. 

We will now show that
\begin{align}\label{fact2}
|\bigcap_{i \in J} \pi(S_\sigma(i))| \leq |\bigcap_{i \in J} S_\sigma(i) | \indent \text{for all} \indent J \in {[m] \choose k}.
\end{align}

Fix $J \in {[m] \choose k}$. Assumption \eqref{GapUpperBound} implies that for all unit vectors $\mathbf{u} \in \cap_{i \in J} \text{Span}\{B_{\pi(S_\sigma(i))}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(i)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $i \in J$. By Lemma \ref{DistanceToIntersectionLemma}, we have:
\begin{align*}
d\left( \mathbf{u}, \bigcap_{i \in J} \text{Span}\{A_{S_{\sigma}(i)}\} \right) 
\leq \Delta \left( \frac{\phi_k(A)}{1 - \xi( \{ \text{Span}\{A_{S_{\sigma}(i)}\}: i \in J\} ) } \right) \leq \Delta,
\end{align*}
%
where the second inequality follows immediately from the definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_\sigma(i))}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\cap_{i \in J}  S_\sigma(i)}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\cap_{i \in J} S_\sigma(i)}\} \right) \leq \Delta \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\})$ and $W = \text{Span}\{A_{\cap_{i \in J} S_\sigma(i)}\}$, that $\dim(V) \leq \dim(W)$ and \eqref{fact2} follows by the linear independence of the columns of $A_{S_\sigma(i)}$ and $B_{\pi(S_\sigma(i))}$ for all $i \in [m]$.

Suppose now that $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_\sigma(i) = \sigma(i)$. By \eqref{fact2} we have that $\cap_{i \in J} \pi(S_\sigma(i))$ is either empty or it contains a single element. Lemma \ref{NonEmptyLemma} ensures that the latter case is the only possibility. Thus, the association $\sigma(i) \mapsto \cap_{i \in J} \pi(S_\sigma(i))$ defines a map $\hat \pi: [m] \to [m]$. Recalling \eqref{SubspaceMetricSameDim}, it follows from \eqref{fact1} that for all unit vectors $\mathbf{u} \in \text{Span}\{A_{i}\}$ we have $d\left( \mathbf{u}, \text{Span}\{B_{\hat \pi(i)}\}\right) \leq \Delta$ also, i.e. for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, there exists some $c_i \in \mathbb{R}$ such that $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\hat \pi(i)}\| \leq \Delta$ where $\Delta < \frac{\ell_{2k}(A)}{\sqrt{2}} \leq \frac{\ell_2(A)}{\sqrt{2}}$. This is exactly the supposition in \eqref{1D} and the result follows from the subsequent arguments of Section \ref{DUT}. 
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}

%================================
% APPENDIX: ADAPTED MAIN LEMMA
%================================

\section{What if we don't know $m$?}\label{mleqmAppendix}

In this section we prove Lemma \ref{MainLemma2} from Section \ref{mleqm}.

%=== PROOF OF ADAPTED MAIN LEMMA ===

\begin{proof}
We consider the case when $k=1$ before showing how the case for $k>1$ reduces to this. Assumption \eqref{GapUpperBound2} implies that for all canonical basis vectors $\mathbf{e}_i \in \mathbb{R}^m$ there is some $c_i \in \mathbb{R}$ such that 
\begin{align}\label{yepyep}
\|A\mathbf{e}_i - c_iB\mathbf{e}_{\pi(i)}\|_2 \leq \Delta \indent \text{for all} \indent i \in [m].
\end{align}
Note that if $c_i = 0$ for some $i$, then $\|A\mathbf{e}_i\| < 1$ (since $\ell_2(A) \leq 1$), contradicting the fact that $A$ has unit norm columns. We will now show that $\pi$ is necessarily injective (and thus defines a permutation). Suppose that $\pi(i) = \pi(j) = p$ for some $i \neq j$ and $\ell \in [m']$. Then $\|A\mathbf{e}_i - c_iB\mathbf{e}_{p}\|_2  \leq \Delta$ and $\|A\mathbf{e}_j - c_jB\mathbf{e}_{p}\|_2 \leq \Delta$. Summing and scaling these two inequalities by $|c_j|$ and $|c_i|$, respectively, the triangle inequality yields
\begin{align*}
(|c_i| + |c_j|) 2\varepsilon
&\geq |c_j|\|A\mathbf{e}_i - c_iB\mathbf{e}_{p}\|_2 + |c_i|\|c_jB\mathbf{e}_{p} - A\mathbf{e}_j\|_2 \\
&\geq \|c_jAe_i + c_iAe_j\|_2 \\
&\geq \ell_2(A)\|c_je_i + c_ie_j\|_2
\end{align*}
%
which is in contradiction with the fact that $\|x\|_1 \leq \sqrt{2}\|x\|_2$ for all $x \in \mathbb{R}^2$ and $\Delta < \frac{\ell_2(A)}{\sqrt{2}}$. Hence, $\pi$ is injective. Letting $P$ and $D$ denote the following partial permutation and diagonal matrices, respectively:
\begin{equation}\label{PandD}
P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}, \mathbf{0}, \cdots, \mathbf{0} \right), \ \ D = \left(\begin{array}{ccc}c_1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & c_m \\ 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0
\end{array}\right),
\end{equation}
%
we see that \eqref{yepyep} becomes $\|(A - BPD)\mathbf{e}_i\|_2 \leq \Delta$ for all $i \in [m]$.

Now suppose $k \geq 2$ and for some fixed $i \in [m]$ let $J = \{i-k+1, \ldots, i\}$ so that $\cap_{i \in J} S_\sigma(i) = \sigma(i)$. Assumption \eqref{GapUpperBound2} implies that for all unit vectors $\mathbf{u} \in \cap_{j \in J} \text{Span}\{B_{\pi(S_\sigma(j))}\}$ we have $d(\mathbf{u}, \text{Span}\{A_{S_\sigma(j)}\}) \leq \frac{\phi_k(A)}{k} \Delta$ for all $j \in J$. By Lemma \ref{DistanceToIntersectionLemma} we have that:
\begin{align}\label{sym2}
d\left( \mathbf{u}, \bigcap_{j \in J} \text{Span}\{A_{S_{\sigma}(j)}\} \right) 
\leq \Delta \left( \frac{\phi_k(A)}{1 - \xi(\{ \text{Span}\{A_{S_{\sigma}(j)}\} : j \in J \})} \right) \leq \Delta,
\end{align}
%
where the second inequality is by definition of $\phi_k(A)$. Now, since $\text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\} \subseteq \cap_{i \in J} \text{Span}\{B_{\pi(S_i)}\}$ and (by Lemma \ref{SpanIntersectionLemma}) $\cap_{i \in J}  \text{Span}\{A_{S_\sigma(i)}\} = \text{Span}\{A_{\sigma(i)}\}$, we have
\begin{align}\label{fact1}
d\left( \mathbf{u}, \text{Span}\{A_{\sigma(i)}\} \right) \leq \Delta \indent \text{for all} \indent \mathbf{u} \in \text{Span}\{B_{\cap_{i \in J}\pi(S_i)}\}.
\end{align}
We therefore have by Lemma \ref{MinDimLemma}, setting $V = \text{Span}\{B_{\cap_{i \in J}\pi(S_\sigma(i))}\})$ and $W = \text{Span}\{A_{\sigma(i)}\}$, that $\dim(V) \leq \dim(W)$. The same arguments can be used to prove \eqref{fact1} with the roles of $A$ and $B$ reversed. Hence, $\dim(V) = \dim(W) = 1$ and it follows that $|\cap_{i \in J} \pi(S_\sigma(i))| = 1$ since the columns of $B_{\cap_{i \in J} \pi(S_\sigma(i))}$ are linearly independent by the spark condition. Thus, the association $\sigma(i) \mapsto \cap_{i \in J} \pi(S_\sigma(i))$ defines a map $\pi: [m] \to [m']$ such that $d\left( \mathbf{u}, \text{Span}\{B_{ \pi(S_\sigma(i))}\}\right) \leq \Delta$ for all $i \in [m]$, i.e. for every canonical basis vector $\mathbf{e}_i \in \mathbb{R}^m$, there exists some $c_i \in \mathbb{R}$ such that $\|A\mathbf{e}_i - c_iB\mathbf{e}_{\hat \pi(i)}\| \leq \Delta$ where $\Delta < \frac{\ell_{2k}(A)}{\sqrt{2}} \leq \frac{\ell_{2}(A)}{\sqrt{2}}$. This is exactly the supposition in \eqref{yepyep}.
\end{proof}

\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_\sigma(i - (k-1)) \cap S_\sigma(i) = \{\sigma(i)\}$. For brevity, we have considered a construction that is valid for any $k < m$.
\end{remark}


%===================================
% 		APPENDIX: CALCULATING C
%===================================

\section{Calculating C}

In this section we demonstrate how an upper bound on the constant $C$ can be derived for a particular $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$. \textbf{[Alternatively, see Lemma 2.2 in Grcar - A matrix lower bound, for a lower bound on rectangular matrices (though not k-restricted)]}

%=== MATRIX LOWER BOUND LEMMA ===

\begin{lemma}\label{MatrixLowerBoundLemma}
Let $\gamma_1 < ... < \gamma_N$ be any distinct numbers such that $\gamma_{i+1} = \gamma_i + \delta$ and form the $k \times N$ Vandermonde matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$. Then for all $S \in {[N] \choose k}$, 
\begin{align}
	\|V_S x\|_2 > \rho \|x\|_1 \indent \text{where} \indent \rho = \frac{\delta^k}{\sqrt{k}} \left( \frac{k-1}{k} \right)^\frac{k-1}{2} \prod_{i = 1}^k (\gamma_1 + (i-1)\delta) \indent \text{for all} \indent x \in \mathbb{R}^k
\end{align}
\end{lemma}

\begin{proof} 
The determinant of the Vandermonde matrix is
\begin{align}
	\det(V) = \prod_{1 \leq j \leq k} \gamma_j \prod_{1 \leq i \leq j \leq k} (\gamma_j - \gamma_i) \geq \delta^k \prod_{i = 1}^k (\gamma_1 + (i-1)\delta).
\end{align}	
Since the $\gamma_i$ are distinct, the determinant of any $k \times k$ submatrix of $V$ is nonzero; hence $V_S$ is nonsingular for all $S \in {[N] \choose k}$. Suppose $x \in \mathbb{R}^k$. Then $\|x\|_2 = \|V_S^{-1} V_S x\|_2 \leq \|V_S^{-1}\| \|V_S x\|_2$, implying $\|V_Sx\|_2 \geq \|V_S^{-1}\|^{-1}\|x\|_2 \geq \frac{1}{\sqrt{k}} \|V_S\|_2^{-1}\|x\|_1$. For the Euclidean norm we have $\|V_S^{-1}\|_2^{-1} = \sigma_{\min}(V_S)$, where $\sigma_{\min}$ is the smallest singular value of $V_S$. A lower bound for the smallest singular value of a nonsingular matrix $M \in \mathbb{R}^{k \times k}$ is given in [Hong and Pan]:
\begin{align}
	\sigma_{\min}(M) > \left( \frac{k-1}{k} \right)^\frac{k-1}{2} |\det M|
\end{align}
%
and the result follows. 
\end{proof}

%===================================
% 		ACKNOWLEDGEMENT
%===================================

\section*{Acknowledgment}
We would like to thank Fritz Sommer for turning our attention to the dictionary learning problem. We also thank Bizzyskillet on SoundCloud for "The No-Exam Jams", which played on repeat during many long hours of designing proofs. Finally, we thank Ian Morris for posting equation \eqref{SubspaceMetricSameDim} and a reference to his proof on Stack Exchange.

%===================================
% 			REFERENCES
%===================================

\bibliographystyle{IEEEtran}
\bibliography{RobustIdentifiability}

%===================================
% 			BIOGRAPHY
%===================================

\begin{IEEEbiographynophoto}{Charles J. Garfinkle}
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Christopher J. Hillar}
\end{IEEEbiographynophoto}

%===================================
% 			SCRAP PAPER
%===================================

\section*{SCRAP PAPER}

\begin{lemma}
If a matrix $M \in \mathbb{n \times m}$ has full column rank then $\ell(M) > 0$. Spark condition implies $\ell_k(A) > 0$. \textbf{[re-word this]}
\end{lemma}

\begin{proof}
Consider the compact set $\mathcal{C} = \{c \in \mathbb{R}^k: \|c\|_2 = 1\}$ and the continuous map
\begin{align*}
\phi: \mathcal{C} &\to \mathbb{R} \\
(c_1, ..., c_k) &\mapsto \|\sum_{j = 1}^k c_j \mathbf{a}_{i_j}\|_2.
\end{align*}

By general linear position of the $\mathbf{a}_i$, we know that $0 \notin \phi(\mathcal{C})$. Since $\mathcal{C}$ is compact, we have by continuity of $\phi$ that $\phi(\mathcal{C})$ is also compact; hence it is closed and bounded. Therefore $0$ can't be a limit point of $\phi(\mathcal{C})$ and there must be some $\rho > 0$ such that the neighbourhood $\{x: x < \rho\} \subseteq \mathbb{R} \setminus \phi(\mathcal{C})$. Hence $\phi(c) \geq \rho$ for all $c \in \mathcal{C}$. The result follows by the association $c \mapsto \frac{c}{\|c\|_2}$ and the fact that there are only finitely many subsets of $k$ vectors $\mathbf{a}_i$ (actually, for our purposes we need only consider those subsets of $k$ vectors $\mathbf{a}_i$ having the same support), hence there is some minimal $\rho$ satisfying \eqref{DataSpread} for all of them. (We refer the reader to the Appendix for a lower bound on $\rho$ given as a function of $k$ and an arithmetic sequence $\gamma_1, \ldots, \gamma_N$ used to generate the $a_i$.)
\end{proof}

%========================
%            INTRODUCTION
%========================

      
\subsection{Introduction}

We also require the data to satisfy certain properties. Consider the problem where we wish to identify the mixing matrix $A$ from the mixtures $\mathbf{y}_i$ when the sources $\mathbf{a}_i$ are known. In this case, a necessary condition for uniqueness of $A$ given $\mathbf{y}_i = A \mathbf{a}_i + \mathbf{\eta}_i$ (even when $\mathbf{\eta}_i=0$) is:
\begin{align}\label{SparkCondition2}
A^{(1)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N) = A^{(2)}_{i,:}(\mathbf{a}_1 \cdots \mathbf{a}_N)  \text{ for all } i \in [n] \implies A^{(1)}  = A^{(2)} \indent \text{for all } A^{(1)}, A^{(2)} \in \mathbb{R}^{n \times m}.
\end{align}

Otherwise, blah blah blah (figure this out). Trying to introduce the "spread" of the data as a necessary condition.

%===================================
% PROOFS OF PROBABILISTIC THEOREMS
%===================================

\subsection{Proofs of Probabilistic Theorems}\label{PUTproof}

Definition \ref{RandomDraw}: can I bound the probability of drawing from one of the $m!$ special support sets without explicitly making that restriction?

We first generalize Lemma 3 in \cite{HS11} to the noisy case:

\begin{lemma}
Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{RIP}. With probability one, if $(k+1)$ $k$-sparse vectors $\mathbf{a}_i \in \mathbb{R}^{n \times m}$ are such that $d(A\mathbf{a}_i,V) \leq (??)$ for some $k$-dimensional subspace $V \subset \mathbb{R}^m$ then all of the $(k+1)$ vectors $\mathbf{a}_i$ have the same supports.
\end{lemma}
\begin{proof}
We need only show that the $k+1$ vectors $A\mathbf{a}_i$ are linearly dependent; the rest follows by Lemma 3 from \cite{HS11}. Let these $k+1$ vectors $\mathbf{a}_i$ be indexed by $J$ and let $W = \text{Span}\{A\mathbf{a}_{i \in J}\}$. Then for all $w \in W$ we can write $w = \sum_{i \in J} c_iA\mathbf{a}_i$ for some set of $c_i \in \mathbb{R}$. Letting $v = \sum_{i \in J} c_iv_i$, it follows that
\[ \|w - v\| = \|\sum_{i \in J} c_i A\mathbf{a}_i - \sum_{i \in J} c_i v_i \| 
\leq \sum_{i \in J} |c_i| \|A\mathbf{a}_i - v_i\| \leq 2\varepsilon \sum_{i \in J}|c_i| \]

Need RHS less thatn $\|w\|$ to prove that $\dim(W) \leq \dim(V) = k$...
\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2} ($\varepsilon = 0$)]
Consider any alternate factorization $\mathbf{y}_i = B\mathbf{b}_i$ for $Y$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that those $\mathbf{y}$ indexed by $J(S)$ span at most a $k$-dimensional space. By Lemma 3 in \cite{HS11}, either $|J(S)| \leq k$ or with probability one all $\mathbf{a}_j$ with $j \in J(S)$ have the same support $S'$. Since there are only $(k+1)$ vectors $\mathbf{a}_i$ with a given support, the latter case actually implies (with probability one) that $|J(S)| = k+1$. If $N=(k+1){m \choose k}$, though, then with probability one we would reach a contradiction if $|J(S)| \leq k$ for any $S \in {[m] \choose k}$; hence with probability one we have $|J(S)| = k+1$. Can we reach the same conclusion when $N = m(k+1)$? (Perhaps by creating 'virtual data' spanning all supports by those data points with supports in $\mathcal{T}$)?

\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem2}]
Let $\mathbf{y}_i = A\mathbf{a}_i + \eta_i$ for all $i \in \{1, \ldots, N\}$ and suppose there is some $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ such that $\|\mathbf{y}_i - B\mathbf{b}_i\| \leq \varepsilon$ for all $i \in \{1, \ldots, N\}$. Then by the triangle inequality we have $\|A\mathbf{a}_i - B\mathbf{b}_i\| \leq 2\varepsilon$ for all $i \in \{1, \ldots, N\}$. Given a support set $S \in {[m]\choose k}$, let $J(S) = \{j: \text{supp}(\mathbf{b}_j) \subseteq S\}$ and note that for all $j \in J(S)$ there exists some $v \in \text{Span}\{B_S\}$ such that $\|A\mathbf{a}_i - v\| \leq 2\varepsilon$. By Lemma (??), with probability one, either $|J(S)| \leq k$ for all $\mathbf{a}_j$ with $j \in J(S)$ have the same support.
\end{proof}


\end{document}


