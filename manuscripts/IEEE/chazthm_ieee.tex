% Notes:
%
% Can we prove in a line or two that the inverse problem is stable by arguing that the function is bijective and closed therefore a homeomorphism?
% Should we mention the notion of Grassmannian manifolds and that Theta is a metric on this space?
% Change robustness to stability?
%
\documentclass[journal, twocolumn]{IEEEtran}

% *** MATH PACKAGES ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
\usepackage{cite}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

%\title{When can a generating dictionary and sparse codes be recovered from noisy data?}
%\title{When is the dictionary learning problem well-posed?}
\title{When is sparse dictionary learning well-posed?}
%\title{When is the dictionary learning problem for sparse coding well-posed?}
%\title{When is the sparsest representation unique?}
%\title{When are sparse codes identifiable from their unknown noisy compressive measurements?}
% FRITZ2: The title doesn't reflect the amount of new stuff, it seems incremental from the last one. It's not very meaningful.
% FRITZ: Not everyone knows what well-posed means. You should mention recovery from noisy data in the title.

\author{Charles~J.~Garfinkle and Christopher~J.~Hillar \\
Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA
%\thanks{%e-mails: cjg@berkeley.edu, chillar@msri.org.  
%CG and CH supported, in part, by NSF grant IIS-1219212 and the Statistical and Applied Mathematical Sciences Institute, under NSF DMS-1127914.}
}
\maketitle

\begin{abstract}
Dictionary learning methods for sparse coding have exposed underlying structure in many kinds of natural signals.  
However, universal theorems guaranteeing the statistical consistency of inference in this model are lacking.  
Here, we prove that for almost all diverse enough datasets generated from the model, latent dictionaries and sparse 
codes are uniquely identifiable up to an error commensurate with measurement noise. 
Applications are given to data analysis, neuroscience, and engineering.
\end{abstract}

%\begin{IEEEkeywords}
%Bilinear inverse problem, matrix factorization, identifiability, dictionary learning, sparse coding, compressed sensing, blind source separation, sparse component analysis
%\end{IEEEkeywords}

%===================================
% 			INTRODUCTION
%===================================]
%TODO: changed intro and 2nd paragraph. changed compressed to compressive.

%Sparse coding is a concept originally developed in neuroscience to account for sensory representations in the brain, which now sees widespread use in many image and signal processing and data analysis tasks. %copied word-for-word from the grant Bruno, Fritz, Yubei, Saeed and I submitted.
\section{Introduction}\label{Intro}
%\IEEEPARstart{A}{n} exciting discovery in computational neuroscience was that response properties of neurons in the mammalian visual cortex can be reproduced by algorithms designed to optimize dictionaries for sparse coding  of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}. 
\IEEEPARstart{T}{he} discovery that response properties of neurons in the mammalian visual cortex are reproducible by algorithms designed to optimize dictionaries for sparse coding of natural images marked an exciting development in computational neuroscience \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}. Since then, many dictionary learning algorithms have been developed and applied to a variety of problems in signal processing and machine learning (see \cite{Zhang15} for a comprehensive review). A popular formulation of the idea is to encode each datum (out of $N$) as a linear combination of at most $k$ $n$-dimensional vectors from an inferred dictionary of size $m$, where $k < m \ll N$. 
% 'encoded by' different from 'approximated by'. Encoded just means any map from data to representation, approximation means this map is determined by finding the closest point of the model to the data. 

Certain applications to data analysis call for a unique such ``sparse structure". For instance, detecting forgeries by analysis of local painting style \cite{hughes2010, Olshausen10} requires that all dictionaries consistent with training data do not differ appreciably in their ability to sparsely encode new samples. Recently, algorithms with proven convergence under certain conditions have been proposed  (see \cite[Sec.~I-E]{Sun16} for a brief summary of the state-of-the-art). It has remained unknown, however, when the problem is well-posed in general (as per Hadamard \cite{Hadamard1902}) -- that is, independent of constraints specific to a particular algorithm.
%rests on brush stroke primitives uniquely specifying an artist. 
% It could be that a forgery has sparse structure with respect to the dictionary but lies in some subspace where no training data lies. 
% A descriptive model would be able to sparse code all of the data, but drawing samples from it would yield nonsense unless the model was actually a good generative model of the data.

The main finding of this work is that any dictionary satisfying the spark condition \eqref{SparkCondition} from compressive sensing is identifiable from as few as $N = m + m(k-1){m \choose k}$ noisy sparse linear combinations of its elements up to an error linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $(n, m, k)$ satisfy the nearly-optimal compressive sensing inequality \eqref{CScondition}, in almost all cases the dictionary learning problem is well-posed given enough data (Cor.~\ref{ProbabilisticCor}). Moreover, these guarantees extend easily to the case when only an upper bound on $m$ is known (Thm.~\ref{DeterministicUniquenessTheorem2}). We hope the explicit, algorithm-independent criteria we provide here may serve as a useful tool in future analyses of dictionary learning procedures.

% FRITZ: Cite applications of algorithms (e.g. gautam) that use algorithms as blind source separation techniques. And then: what about theoretical justifications of this approach. There is a body of work on guarantees for zero error, but there is nothing for noisy case. Don't assume that everyone is assuming ground truth. Many algorithms have been designed, and yet in the presence of measurement errors there are no theoretical guarantees. Then set up problem, cite Aharon etc.
% CHRIS: Citing Gautam is ok! We give reason to believe it's unique. Alright, soften up 'ground truth' statement assumptions about people. Save Gautam for discussion? Intro: artist stroke design?

We state the dictionary learning problem considered here more precisely as follows. Fix a dictionary represented as the columns $A_j$ of a matrix $A \in \mathbb R^{n \times m}$ and suppose a dataset $Z$ consists of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = A\mathbf{a}_i + \mathbf{n}_i,\indent \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{a}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $| \mathbf{n}_i |_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $A\mathbf{a}_i$.

\begin{problem}[Dict. Learning]\label{InverseProblem}
Find $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ such that $|\mathbf{z}_i - B\mathbf{b}_i|_2 \leq \eta$ for $i = 1, \ldots, N$.
\end{problem}

%Note that from any particular solution to this problem we can generate infinitely many solutions $B = AD^{-1}P^{\top}$ and $\mathbf{b}_i = PD\mathbf{a}_i$, where $P \in \mathbb{R}^{m \times m}$ is any permutation matrix and $D \in \mathbb{R}^{m \times m}$ any invertible diagonal matrix, but that identifying the parameters of the sparse coding model amounts to solving it only up to this inherent ambiguity.
Note that any particular solution to this problem in fact represents a whole class of equivalent dictionaries $BPD$ and codes $D^{-1}P^{\top}\mathbf{b}_i$, where $P \in \mathbb{R}^{m \times m}$ is any permutation matrix and $D \in \mathbb{R}^{m \times m}$ any invertible diagonal matrix. Since  arbitrary scalings and ordering of dictionary elements represent the same underlying model, it is natural to ask whether solutions to Problem~\ref{InverseProblem} are unique up to this equivalence.

Previous work \cite{li2004analysis, Georgiev05, Aharon06, Hillar15} on the noiseless case $\eta = 0$ has shown that the solution (when it exists) is indeed unique in this sense provided the $\mathbf{a}_i$ are sufficiently diverse and the matrix $A$ satisfies the \textit{spark condition}:
\begin{align}\label{SparkCondition}
A\mathbf{x}_1 = A\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2,\indent \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
% [ *** Can we motivate more necessary conditions using theorems of Li15, too? Thm.~2.8. *** ] 
which is evidently a necessary condition given that the $\mathbf{a}_i$ are known only to be $k$-sparse.\footnote{For example, consider $A$ with collinear columns.} 
%TODO: Make PERFECT. Basically, we need this to ensure that any two a_i don't get mapped to the same thing. but what if our constraints on the a_i already ensure that? We don't 'only know the a_i are k-sparse'. We know they'r distributed over supports in a cyclic order and in general linear position over each support.
% MAYBE: If $A$ does not satisfy the spark condition, we can always add the two delinquent a_i to the dataset. 
Matrices of the form $PD$ thus form the \emph{ambiguity transformation group} inherent to the noiseless problem subject to these constraints \cite{Li15}. 

We introduce the following terminology to handle $\eta > 0$.

% no need for this in this paper -- a bit confusing to throw in 
%When they exist, solutions to Problem \ref{InverseProblem} will be found by solving:
%\begin{align*}
%\min_{B, \, \mathbf{b}_i} \  \max_{i \in [N]} |\mathbf{b}_i|_0 \text{ s.t. } |\mathbf{y}_i - B\mathbf{b}_i|_2 \leq \varepsilon \text{ for $i = 1, \ldots, N$}.
%\end{align*}

\begin{definition}\label{Uniqueness}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $\mathbb{R}^m$} if there exists an $A \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = A\mathbf{a}_i$ for all $i$. This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon = \varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if a matrix $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ have \mbox{$|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$} for all $i$, then there is a permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that for all $i = 1,\ldots,N$ and $j = 1,\ldots,m$:
\begin{align}\label{def1}
|A_j - (BPD)_j|_2 \leq \delta_1 \ \ \text{and} \ \ |\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i|_1 \leq \delta_2.
\end{align}
\end{definition}

We ask here: \emph{When does $Y \subset \mathbb{R}^n$ have a stable $k$-sparse representation in $\mathbb{R}^m$?} To see how an affirmative answer to this question informs the interpretation of solutions to Problem \ref{InverseProblem}, suppose that \mbox{$Y = \{A \mathbf{a}_1, \ldots, A\mathbf{a}_N\}$} has a stable $k$-sparse representation in $\mathbb R^m$, and fix $\delta_1, \delta_2$ to be the desired accuracy in recovery \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} that has $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then, any dictionary $B$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N$ solving Problem \ref{InverseProblem} are necessarily close to the original matrix $A$ and codes $\mathbf{a}_i$ (i.e., satisfy \eqref{def1}).  

%Our results, outlined in Sec.~\ref{Results}, include an explicit form for $\varepsilon(\delta_1, \delta_2)$ in Thm.~\ref{DeterministicUniquenessTheorem}. 
In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after listing some additional definitions and lemmas required for the proof, including our main tool from combinatorial matrix analysis (Lem.~\ref{MainLemma}). Our proof is a refinement of the arguments in \cite{Hillar15} to handle noise and to reduce the sufficient number of samples from $N=k{m \choose k}^2$ to $N = m + m(k-1){m \choose k}$. All other proofs are relegated to the appendices. Finally, several applications are considered in our discussion, Sec.~\ref{Discussion}.

\section{Results}\label{Results}

Before precisely stating our results, we explain how the spark condition \eqref{SparkCondition} relates to the \emph{lower bound} \cite{Grcar10} of $A$, written $L(A)$, which is the largest number $\alpha$ such that \mbox{$|A\mathbf{x}|_2 \geq \alpha|\mathbf{x}|_2$} for all $\mathbf{x} \in \mathbb{R}^m$. By compactness, every injective linear map has a nonzero lower bound; hence, if $A$ satisfies \eqref{SparkCondition}, then every submatrix formed from $2k$ of its columns or less has a nonzero lower bound. We therefore define the following domain-restricted lower bound of $A$:
\begin{align*}
L_k(A) := \max \{ \alpha : |A\mathbf{x}|_2 \geq \alpha|\mathbf{x}|_2 \text{ for all $k$-sparse } \mathbf{x} \in \mathbb{R}^m\}.
\end{align*} 
Clearly, $L_k(A) \geq L_{k'}(A)$ whenever $k < k'$, and for any $A$ satisfying \eqref{SparkCondition}, we have $L_{k'}(A) > 0$ for all $k' \leq 2k$. 

A \textit{cyclic order} on $[m] := \{1, \ldots,m\}$ is an arrangement of $[m]$ in a circular necklace, and an \textit{interval} in the order is any subset of contiguous elements. A vector $\mathbf{a} \in \mathbb{R}^m$ is said to be \emph{supported} on $S \subseteq [m]$ when $\mathbf{a} \in \text{Span}_{\mathbb R}\{ \{\mathbf{e}_j\}_{j\in S}\}$, where $\mathbf{e}_j$ are the standard basis vectors.  Also, recall that $M_j$ denotes the $j$th column of a matrix $M$. The following result gives an answer to our question from the introduction.

%=== STATEMENT OF DETERMINISTIC UNIQUENESS THEOREM ===%
% TODO: Is "at least" below redundant? if there are N+1, there still are N. We don't say "exactly". 
\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix $n, m$, and $k < m$. If $A \in \mathbb{R}^{n \times m}$ is injective on $k$-sparse vectors and $k$-sparse \mbox{$\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$} are such that for every interval of length $k$ in some cyclic order on $[m]$ there are at least \mbox{$(k-1){m \choose k}+1$} vectors $\mathbf{a}_i$ in general linear position (i.e., any $k$ of them are linearly independent) supported there, then $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$.

Specifically, there exists a constant $C > 0$ for which the following holds for all $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$.\footnote{The condition $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ is necessary; otherwise,  with \mbox{$A$ = $I$} and $\mathbf{a}_i = \mathbf{e}_i$, there is a $B$ and $1$-sparse $\mathbf{b}_i$ with $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ violating \eqref{Cstable}.} If any matrix $B \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^m$ are such that \mbox{$|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$} for all $i \in [N]$, then for all $j \in [m]$:
\begin{align}\label{Cstable}
|A_j-(BPD)_j|_2 \leq C\varepsilon,
\end{align}
%
for some permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$. Moreover, if $\varepsilon < \varepsilon_0 := \frac{L_{2k}(A)}{\sqrt{2k}}C^{-1}$, then $L_{2k}(BPD) \geq L_{2k}(A)\left( 1 - \varepsilon / \varepsilon_0 \right)$ and for all $i \in [N]$:
\begin{align}\label{b-PDa}
|\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i|_1 &\leq \frac{\varepsilon }{ \varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right).
\end{align}
\end{theorem} 
The constant $C$ is explicitly defined in \eqref{Cdef} below.

% NOTE: The upper bound on eps is due to the fact that any two vectors in the plane can be a permutation-scaling away from each other if this bound is exceeded.

%Note that it was not assumed as given that $B$ satisfy \eqref{SparkCondition}. Still, when $\varepsilon < \varepsilon_0$, we have \mbox{$L_{2k}(BPD) \geq L_{2k}(A)\left( 1 - \frac{\varepsilon}{\varepsilon_0} \right)$}.

% Can we get rid of the k-sparse assumption on b using results from CS?
%\begin{remark}[Support Properties]
%Note that the bound in \eqref{b-PDa} does not immediately entail that $\mathbf{a}_i$ and $D^{-1}P^{-1}\mathbf{b}_i$ share the same support unless $\varepsilon = 0$. If $\varepsilon$ is small enough, however, then it is indeed the case that $\text{supp}(D^{-1}P^{-1}\mathbf{b}_i) \subseteq \text{supp}(\mathbf{a}_i)$. [*** TODO: Is this right? See Donoho paper on L1 stability? ***]
% Kinf of like m' > m scenario
%\end{remark}
%[ *** So inference is stable for small enough error! There is no multiplicity of alternative models that also happen to work. Regression models have a fixed $A$ (i.e. the values of the variables we are regressing on). Here, in this model, we don't specify the $A$, instead we learn what the best regressor variables would be if they existed. And we want to combine regressors for different problems so as to save resources. Thought of all this reading section 8 of the Breiman paper on data models vs. algorithmic models.***]

%TODO: Add epsilon_0 into the max?

 
An important consequence of this result is that \eqref{def1} is guaranteed provided $\varepsilon$ does not exceed:
\begin{align*}
%\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C }, \frac{ \delta_2 \varepsilon_0}{\delta_2 + \max_{i \in [N]} |\mathbf{a}_i|_1 + C^{-1}} \right\}. \\
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C }, \frac{ \delta_2 L_{2k}(A) / \sqrt{2k}}{ 1 + C \left( \max_{i \in [N]} |\mathbf{a}_i|_1  + \delta_2 \right) } \right\}.
\end{align*}


%
%at least for $\delta_1, \delta_2 \geq 0$ small enough to satisfy $\varepsilon(\delta_1, \delta_2) < \varepsilon_0$. 
% The constant $C$ is explicitly defined in \eqref{Cdef}, below.

%\footnote{The reader can verify that in the case $k=1$, for $C$ as defined in \eqref{Cdef}, if \mbox{$A$ = $I$} and $\mathbf{a}_i = \mathbf{e}_i$, there is a $B$ and $1$-sparse $\mathbf{b}_i$ with $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ that do not have \eqref{Cstable} when $\varepsilon \geq \frac{L_2(A)}{\sqrt{2}}C^{-1}$}


\begin{corollary}\label{DeterministicUniquenessCorollary}
Given $n, m$, and $k < m$, there are $N =  m(k-1){m \choose k}+m$ vectors \mbox{$\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$} such that every matrix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} generates a set $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ with a stable $k$-sparse representation in $\mathbb R^m$.
\end{corollary}

It is straightforward to provide a probabilistic extension of Thm.~\ref{DeterministicUniquenessTheorem} using the following fact in random matrix theory.  The matrix $A \in \mathbb{R}^{n \times m}$ satisfies \eqref{SparkCondition} with probability one (or with ``high probability" for discrete variables) 
provided:
\begin{align}\label{CScondition}
n \geq \gamma k\log\left(\frac{m}{k}\right),
\end{align}
where $\gamma$ is a positive constant dependent on the particular distribution from which the entries of $A$ are sampled i.i.d. (many ensembles suffice, e.g. \cite[Sec.~4]{Baraniuk08}). 
%[*** mention algebraic goem problem --  ask Bernd ***]

In fact, the spark condition can be made explicit.  Let $A$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for all the $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(A) = \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det A_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $A_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$. 

Since $f$ is a real analytic function, it is enough to show that at least \emph{one} substitution of real numbers satisfies $f(A) \neq 0$ to conclude that its zeroes form a set with measure zero. Hence, \emph{every} $n \times m$ matrix $A$ outside a set of measure zero satisfies \eqref{SparkCondition} provided \eqref{CScondition} holds for a value of $\gamma$ for \emph{some} distribution. We remark that the precise relationship between $m$, $n$, and $k$ guaranteeing that $f$ is not identically zero is still an open problem in real algebraic geometry. In any case, we set $\gamma_0$ to be the smallest known such $\gamma$.

It so happens that a similar statement applies to sets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic" dataset $Y = \{A\mathbf{a}_1,\ldots,A \mathbf{a}_N\}$ generated by indeterminate $A$ and $k$-sparse $\mathbf{a}_1, \ldots, \mathbf{a}_N$. 
\begin{theorem}\label{robustPolythm} 
Fix $n, m$, $k < m$. There is a polynomial in the entries of $A$ and the $\mathbf{a}_i$ with the following property:  if the polynomial evaluates to a nonzero number and for every interval of length $k$ in some cyclic order on $[m]$ at least \mbox{$(k-1){m \choose k}+1$} of the resulting vectors $\mathbf{a}_i$ are supported on that interval, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ (Def.~\ref{Uniqueness}). In particular, either no substitutions impart to $Y$ this property or all but a Borel set of measure zero do. 
\end{theorem}

% this is cool, but let's just save it for later...i can ask around.  -cjh
%An interesting open question in real algebraic geometry is which collections of $(m,n,k)$ determine this last ``or".

\begin{corollary}\label{ProbabilisticCor}
Fix $n, m$, and $k$ satisfying \eqref{CScondition} for $\gamma = \gamma_0$, and let the entries of the matrix $A \in \mathbb{R}^{n \times m}$ and $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If at least $(k-1){m \choose k} + 1$ of the vectors $\mathbf{a}_i$ are supported on each interval of length $k$ in some cyclic order on $[m]$, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{corollary}

%The argument in \cite{Hillar15} shows that if $k+1$ random $\mathbf{a}_i$ are drawn for \emph{each} support in ${[m] \choose k}$, then $Y$ has a unique $k$-sparse representation in $\mathbb{R}^m$ (up to symmetries) with probability one. This representation can now be said to be stable as well. %[ ** TODO: Double check this...see last steps of proof of Theorem 2 in \cite{Hillar15} ** ] 

%CUT: OK? 
%We note furthermore that our result in the deterministic case (Thm.~\ref{DeterministicUniquenessTheorem}) accounts for \emph{worst-case} noise. However, for fixed sparsity $k$, the larger the ambient dimension $n$ of the data, the smaller the probability that the noise points in a direction  confusing signals generated by $k$ columns of $A$.  Therefore, for a given distribution, the ``effective'' noise might be much smaller, with the original dictionary and sparse codes being identifiable for better constants with high probability. 

Next, we address when only an upper bound $m'$ on the latent dimension $m$ is known (assuming that $B$ satisfies (\ref{SparkCondition})). 

\begin{theorem}\label{DeterministicUniquenessTheorem2}
Let $Y$ be defined as in Thm.~\ref{DeterministicUniquenessTheorem}. There exists $C > 0$ for which the following holds for all $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ and any $m' > m$. If a matrix $B \in \mathbb{R}^{n \times m'}$ satisfies \eqref{SparkCondition} and $k$-sparse $\mathbf{b}_1, \ldots, \mathbf{b}_N \in \mathbb{R}^{m'}$ are such that \mbox{$|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$} for all $i \in [N]$, then \eqref{Cstable} and \eqref{b-PDa} hold for some $n \times m$ submatrix of $B$ and corresponding subvectors of the $\mathbf{b}_i$, respectively. 
\end{theorem}

In other words, the columns of $B$ contain (up to noise, after appropriate scaling) the columns of the original dictionary $A$. Similarly, the $\mathbf{b}_i$ contain the original codes $\mathbf{a}_i$. The constant $C$ here is expression (\ref{Cdef2}) from the proof of Thm.~\ref{DeterministicUniquenessTheorem2}. 


\vspace{-.08 cm}

\section{Proof of Theorem~\ref{DeterministicUniquenessTheorem}}\label{DUT}
%===================================
% 			Preliminaries
%===================================
We first briefly outline our main tools, which include general notions of angle (Def.~\ref{FriedrichsDefinition}) and distance (Def.~\ref{GapMetricDef}) between subspaces as well as a (stable) uniqueness result in matrix analysis (Lem.~\ref{MainLemma}).
Given a set $\mathcal{T}$, let ${\mathcal{T} \choose k}$ denote the set of all subsets of $\mathcal{T}$ of size $k$. Denote by $\mathbf{e}_i$ for $i \in [m]$ the canonical basis vectors in $\mathbb{R}^m$ and let $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ be the $\mathbb{R}$-linear span of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$. Given $S \subseteq [m]$ and $M \in \mathbb{R}^{n \times m}$, let $M_S$ be the submatrix with columns $M_j$ for $j \in S$, which will also denote its column space when appropriate. Let $\frak{S}_{k}$ denote the set of permutations (bijections) on $k$ elements.
%Note: the gap is not a metric
% we did before earlier. no we didn't....

\begin{definition}\label{FriedrichsDefinition}
The \textbf{angle} between subspaces $U,V \subseteq \mathbb{R}^n$ is the angle $\theta(U,V)$ in $(0,\frac{\pi}{2}]$ whose cosine is \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,V)} := \max\left\{ |\langle \mathbf{u}, \mathbf{v} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap V)^\perp \cap \mathcal{B} \\ \mathbf{v} \in V \cap (U \cap V)^\perp \cap \mathcal{B} } \right\},
\end{align*}
% max replaces sup since we are in a real vector space
where $\mathcal{B} = \{ \mathbf{x} \in \mathbb{R}^n: |\mathbf{x}|_2 \leq 1\}$.
%\footnote{ $\theta$ is one out of a set of \textit{principal} (or \textit{canonical} or \textit{Jordan}) angles between subspaces that are invariant to orthogonal transformations \cite{?}.} 
\end{definition}

 % See Theorem 9.35(1) in Deutsch
%TODO: mention that we have subbed max for sup because real subspaces are closed?
% The concept of principal angles between subspaces was first introduced by Jordan in 1875 \cite{?}. The definition of $\theta$ is attributed to Friedrichs \cite{Friedrichs1937OnCertainInequalitiesAnd}}
%For example, when $n=3$ and $k=1$, this is simply the angle between vectors; and for $k=2$, it is the angle between the normal vectors of two planes. In higher dimensions, the Friedrichs angle is one out of a set of \textit{principal} (or \textit{canonical} or \textit{Jordan}) angles between subspaces that are invariant to orthogonal transformations. These angles are all zero if and only if one subspace is a subset of the other; otherwise, the Friedrichs angle is the smallest nonzero such angle. 

The next quantity is based on one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm for projecting a point onto the intersection of a set of subspaces.
% We use it to bound the distance between a point and the intersection of a set of subspaces given an upper bound on the distance from that point to each individual subspace. 

%The minimization over permutations in \eqref{xidef} below is done only to remove the dependence the convergence result has on the order in which subspaces are inputted to the alternating projections algorithm.
\begin{definition}\label{SpecialSupportSet}
Fix $A \in \mathbb{R}^{n \times m}$ and $k < m$ and let $\mathcal{T} \subseteq {[m] \choose k}$ be of size at least $k$. For $k > 1$, define:
\begin{align*}
%\phi_k(A) := \min_{ S_1 \neq \ldots \neq S_k \in {[m] \choose k} } 1 - \xi( A_{S_1}, \ldots, A_{S_k}),
\phi_\mathcal{T}(A) :=  \min_{T \in {\mathcal{T} \choose k}} 1 - \xi( \{ A_{S}: S \in T\}),
\end{align*}
where for any set $\mathcal{V} = \{V_1, \ldots, V_k\}$ of subspaces of $\mathbb{R}^m$:
\begin{align*}
\xi(\mathcal{V}) := \min_{\sigma \in \frak{S}_k} \left(1 -  \prod_{i=1}^{k-1} \sin^2  \theta \left(V_{\sigma(i)}, \cap_{j>i} V_{\sigma(j)} \right)  \right)^{1/2},
\end{align*} 
%
and we set $\phi_\mathcal{T}(A) := 1$ for $k=1$.
\end{definition}
Note that $\xi < 1$ and  $\phi_\mathcal{T}(A) > 0$ always, since it is clear from Def.~\ref{FriedrichsDefinition} that $\cos\theta(U,V) < 1$ for all subspaces $U,V \in \mathbb{R}^m$. 

%=== SPECIFICS OF DETERMINISTIC THEOREM ===%
We now state the constant $C > 0$ referred to in Thm. ~\ref{DeterministicUniquenessTheorem}:
\begin{align}\label{Cdef}
C = \left( \frac{ \sqrt{k^3}}{ \phi_\mathcal{T}(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in \mathcal{T}} L_k(AX_{I(S)})},
\end{align}
%
where $\mathcal{T}$ is the set intervals in the cyclic order, $X$ is the $m \times N$ matrix with columns $\mathbf{a}_i$, and $I(S) := \{i : \text{supp}(\mathbf{a}_i) = S\}$.\footnote{That $C > 0$ is well-defined follows from $L_k(A) > 0$ and the general linear position of the $\mathbf{a}_i$, since then $|A_j|_2 > 0$ for all $j$ and $\min_{S \in \mathcal{T}} L_k(AX_{I(S)}) > 0$.}


%OLD:
%implies that $\theta_F(A_{S_1}, A_{S_2}) = 0$ for some $S_1 \in {[m] \choose k}$ and $S_2 \in {[m] \choose \ell}, \ \ \ell \leq k$, which would contradict \eqref{SparkCondition}.
%$\text{Span}(A_{S_1}) \supseteq \cap_{j=2}^k \text{Span}(A_{S_j})$ for some $S_1, \ldots, S_k \in {[m] \choose k}$, which would contradict \eqref{SparkCondition}.

\begin{definition}\label{GapMetricDef}
The \textbf{gap} (or \textbf{aperture}) $\Theta(U,V)$ between subspaces $U, V \subseteq \mathbb{R}^m$ is given by \cite[Sec.~2]{Kato2013}: 
\begin{equation*}
\Theta(U,V) := \max \big\{ \sup_{\mathbf{u} \in U, |\mathbf{u}|_2 = 1} d(\mathbf{u},V), \sup_{\mathbf{v} \in V, |\mathbf{v}|_2 = 1} d(\mathbf{v},U) \big\},
\end{equation*}
%
where $d(\mathbf{u},V) := \inf\{|\mathbf{u}-\mathbf{v}|_2: \mathbf{v} \in V\}$.
%TODO: Include this footnote? \footnote{In fact, $\Theta(U,V)$ is equal to the sine of the largest Jordan angle between $U$ and $V$ \cite{?} and also $\Theta(U,V) = \|\Pi_V - \Pi_U\|$ \cite{?}.} 
\end{definition}

We now state our uniqueness result in matrix analysis, generalizing \cite[Lem.~1]{Hillar15} to the noisy case.

%===========          MAIN LEMMA (K > 1)             =================
\begin{lemma}[Main Lemma]\label{MainLemma}
Fix $n, m$, $k < m$, and let $\mathcal{T}$ be the set of intervals of length $k$ in some cyclic order on $[m]$. Let $A, B \in \mathbb{R}^{n \times m}$ and suppose that $A$ satisfies the spark condition \eqref{SparkCondition} and has maximum column $\ell_2$-norm $\rho$.  If there exists a map $\pi: \mathcal{T} \to {[m] \choose k}$ and some $\delta < \frac{L_{2}(A)}{\sqrt{2}}$ such that: 
\begin{equation}\label{GapUpperBound}
\Theta(A_{S}, B_{\pi(S)}) \leq \frac{ \phi_\mathcal{T}(A) }{\rho k} \delta, \indent \text{for all } S \in \mathcal{T},
\end{equation}
%
then there exist a permutation matrix $P \in \mathbb{R}^{m \times m}$ and an invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ with:
\begin{align}\label{MainLemmaBPD}
|A_j - (BPD)_j|_2 \leq \delta, \indent \text{for } j \in [m].
\end{align}
\end{lemma}

We will also use a few facts about $d$ in Def.~\ref{GapMetricDef}. The first, proven in \cite[Lem.~3.2]{Morris10}, is that if $\dim(W) = \dim(V)$ then:
\begin{equation}\label{SubspaceMetricSameDim}
\sup_{\mathbf{v} \in V, |\mathbf{v}|_2 = 1}  d(\mathbf{v},W)  = \sup_{\mathbf{w} \in W, |\mathbf{w}|_2 = 1} d(\mathbf{w},V).
\end{equation}
% Actually, in Morris' paper it is |v| <= 1 and |u| <= 1. Cool for us to just make this change, right?
The second is:
\begin{lemma}\label{MinDimLemma}
If $U, V$ are subspaces of $\mathbb{R}^{m}$, then:
\begin{equation*}
d(\mathbf{u},V) < |\mathbf{u}|_2 \ \text{and} \  \mathbf{u} \in U \setminus{\{\mathbf{0}\}} \implies \dim(U) \leq \dim(V).
\end{equation*}
\end{lemma}
%
Finally, we often use $|\mathbf{x}|_1 \leq \sqrt{k} |\mathbf{x}|_2$ for $k$-sparse $\mathbf{x} \in \mathbb{R}^m$.

%===================================
% 			Deterministic Identifiability
%===================================
%======== CASE K = 1 ============
%Let $\mathbf{e}_1, \ldots, \mathbf{e}_m$ be the standard basis vectors in $\mathbb R^m$.
Let us now prove Thm.~\ref{DeterministicUniquenessTheorem} for the simple case when $k=1$. Fix $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}, and let $\mathbf{a}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. By \eqref{Cdef}, we have:
\begin{align}\label{C1}
C 
%&= \left( \frac{ \sqrt{k^3}}{ \phi_1(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{i \in [m]} L_1(c_iA_i) } \\
&= \sqrt{k^3} \left( \frac{\max_{j \in [m]} |A_j|_2}{\min_{j \in [m]}|c_jA_j|_2} \right)
\geq \left( \min_{\ell \in [m]} |c_{\ell}| \right)^{-1}.
\end{align}

Suppose that for some $B \in \mathbb{R}^{n \times m}$ and $1$-sparse $\mathbf{b}_i \in \mathbb{R}^m$ we have  $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ for $i \in [m]$. Then there are $c'_1, \ldots, c'_m \in \mathbb{R}$ and $\pi: [m] \to [m]$ with:
\begin{align}\label{1D}
|c_jA_j - c'_jB_{\pi(j)}|_2 \leq \varepsilon, \ \ \text{for } \  j \in [m].
\end{align} 
Note that $c'_j \neq 0$ for all $j$ since otherwise (by definition of $L_2(A)$), we would have $|c_jA_j|_2 < \min_{\ell \in [m]}|c_{\ell}A_{\ell}|_2$. 

We  now show that $\pi$ is injective (and thus is a permutation).
Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $|c_{j}A_{j} - c'_{j}B_{\ell}|_2 \leq \varepsilon$ and $|c_{i}A_{i} - c'_{i} B_{\ell}|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|c'_{i}|$ and $|c'_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align}\label{contra}
(|c'_{i}| + |c'_{j}|) \varepsilon
&\geq |A(c'_{i}c_{j} \mathbf{e}_{j} - c'_{j}c_{i}\mathbf{e}_{i})|_2 \nonumber \\ 
&\geq \frac{L_2(A)}{\sqrt{2}} \left( |c'_{i}| + |c'_{j}| \right) \min_{\ell \in [m]} |c_\ell |.
\end{align}
%
%where the last inequality follows from the definition of $L_2(A)$ and (\ref{sqrtk}). 

Since \eqref{contra} contradicts \eqref{C1} and our upper bound on $\varepsilon$, the map $\pi$ is injective. Letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{c'_1}{c_1},\ldots,\frac{c'_m}{c_m})$, we see that \eqref{1D} becomes \eqref{Cstable} for $j \in [m]$:
\begin{align*}%\label{k=1result}
|A_j - (BPD)_j|_2 = |A_j - \frac{c'_j}{c_j}B_{\pi(j)}|_2 \leq \frac{\varepsilon}{|c_j|} \leq C\varepsilon.
\end{align*}

% ======== b - PDa =========

It turns out that \eqref{Cstable} already implies in general the recovery result \eqref{b-PDa} for $k \geq 1$ when $\varepsilon < \varepsilon_0 := \frac{L_2(A)}{2k}C^{-1}$. To see why, note that for all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$|(A-BPD)\mathbf{x}|_2  \leq C\varepsilon|\mathbf{x}|_1 \leq C \varepsilon \sqrt{2k}  |\mathbf{x}|_2$}. Thus, $|BPD\mathbf{x}|_2 \geq | |A\mathbf{x}|_2 - |(A-BPD)\mathbf{x}|_2 | \geq (L_{2k}(A) - \sqrt{2k}C\varepsilon ) |\mathbf{x}|_2$, where we drop the absolute value since $\varepsilon < \varepsilon_0$. Hence, $L_{2k}(BPD) \geq L_{2k}(A)\left( 1 - \varepsilon/\varepsilon_0 \right) > 0$ and \eqref{b-PDa} then follows from:
\begin{align*}
|D^{-1}P^{\top}\mathbf{b}_i - \mathbf{a}_i|_1
%&\leq \sqrt{2k} |\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i|_2 \\
&\leq \frac{\sqrt{2k}}{L_{2k}(BPD)}|BPD(\mathbf{a}_i - D^{-1}P^{\top}\mathbf{b}_i)|_2 \\
%&\leq \frac{\sqrt{2k}}{L_{2k}(BPD)} (|B\mathbf{b}_i - A\mathbf{a}_i|_2 + |(A - BPD)\mathbf{a}_i|_2) \\
&\leq \frac{\varepsilon\sqrt{2k}}{L_{2k}(BPD)}(1+C|\mathbf{a}_i|_1).
%\leq \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C^{-1}+|\mathbf{a}_i|_1 \right).
\end{align*}

% ========== DEFINITIONS FOR MAIN LEMMA (K>1) ================
It remains to show that \eqref{Cstable} with $C$ given in \eqref{Cdef} follows from $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ for $k > 1$. Our main tool is Lem.~\ref{MainLemma}.

%========          PROOF OF THEOREM 1        ============
% TODO: Add "Since there are *at least* N = ... vectors for each support. Because the theorem statement uses the words "at least". Fix N \geq ...? Then C needs to be defined with respect to the N data points that we use. 
\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessTheorem}]
From above, we may assume that $k > 1$. Let $\mathcal{T}$ be the set of intervals of length $k$ in the given cyclic order on $[m]$, and fix $N = m(k-1){m \choose k}+m$ vectors in $\mathbb{R}^m$ and $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} as in the statement of the theorem. %We claim $Y = \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$. 

Suppose that for some $B \in \mathbb{R}^{n \times m}$ there exist $k$-sparse \mbox{$\mathbf{b}_i \in \mathbb{R}^m$} such that $|A\mathbf{a}_i - B\mathbf{b}_i|_2 \leq \varepsilon$ for all $i \in [N]$. Since there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ with a given support $S \in \mathcal{T}$, the pigeon-hole principle implies that there exists some $S' \in {[m] \choose k}$ and some set of $k$ indices $J(S)$ such that all $\mathbf{a}_i$ and $\mathbf{b}_i$ with $i \in J(S)$ have supports $S$ and $S'$, respectively.

Let $X$ and $X'$ be the $m \times N$ matrices with columns $\mathbf{a}_i$ and $\mathbf{b}_i$, respectively. It follows from the general linear position of the $\mathbf{a}_i$ and the linear independence of every $k$ columns of $A$ that $L(AX_{J(S)}) > 0$; that is, the columns of the $n \times k$ matrix $AX_{J(S)}$ are linearly independent and thus form a basis for $\text{Span}\{A_{S}\}$. Fixing $\mathbf{y} \in \text{Span}\{A_{S}\}$, there then exists a unique $\mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = AX_{J(S)}\mathbf{c}$. Letting \mbox{$\mathbf{y'} = BX'_{J(S)}\mathbf{c}$}, which is in $\text{Span}\{B_{S'}\}$, we have:
\begin{align*}
|\mathbf{y} - \mathbf{y'}|_2 
= |\sum_{i=1}^k c_i(AX_{J(S)} - BX'_{J(S)})_i|_2 
\leq \varepsilon \sum_{i=1}^k |c_i| \\
\leq \varepsilon \sqrt{k}  |\mathbf{c}|_2 
\leq \frac{\varepsilon \sqrt{k}}{L(AX_{J(S)})} |AX_{J(S)}\mathbf{c}|_2 
= \frac{\varepsilon \sqrt{k}}{L(AX_{J(S)})} |\mathbf{y}|_2.
\end{align*}
In particular, the inequality $d(\mathbf{y}, B_{S'}) \leq \varepsilon\sqrt{k} / L(AX_{J(S)})$ holds for all $\mathbf{y} \in \text{Span}\{A_{S}\}$ having unit $\ell_2$-norm.
%\begin{align}\label{ABSubspaceDistance}
%\sup_{ \substack{ \mathbf{y} \in \text{Span}\{A_{S}\} \\ |\mathbf{y}|_2 = 1} } d(\mathbf{y}, B_{S'}) \leq \frac{\varepsilon\sqrt{k}}{L(AX_{J(S)})}.
%\end{align}

We now show that \eqref{Cstable} follows if $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$, for $C$ as defined in \eqref{Cdef}. Letting $\rho = \max_{j \in [m]} |A_j|_2$, we have:
\begin{align}\label{rhs}
\sup_{ \substack{ \mathbf{y} \in \text{Span}\{A_{S}\} \\ |\mathbf{y}|_2 = 1} } d(\mathbf{y}, B_{S'}) \leq \frac{\varepsilon\sqrt{k}}{L(AX_{J(S)})}
%&<  \frac{\phi_k(A) L_2(A)}{\rho k \sqrt{2}} \left( \frac{\min_{S \in \mathcal{T}}L_k(AX_{I(S)})}{L(AX_{J(S)})} \right) \nonumber \\
&< \frac{\phi_\mathcal{T}(A) L_2(A)}{\rho k \sqrt{2}}.
\end{align}
Since $L_2(A) \leq \rho \sqrt{2}$ and $\phi_\mathcal{T}(A) \leq 1$, the RHS of \eqref{rhs} is strictly less than one. It follows by Lem.~\ref{MinDimLemma} that $\dim(\text{Span}\{B_{S'}\}) \geq \dim(\text{Span}\{A_{S}\}) = k$. % (since every $k$ columns of $A$ are linearly independent). 
Since $|S'| = k$, we also have $\dim(\text{Span}\{B_{S'}\}) \leq k$; hence, $\dim(\text{Span}\{B_{S'}\}) = \dim(\text{Span}\{A_{S}\})$. Recalling \eqref{SubspaceMetricSameDim},  we see that the association $S \mapsto S'$ therefore defines a map $\pi: \mathcal{T} \to {[m] \choose k}$ satisfying:
\begin{align}\label{yeyeye}
%\Theta(A_{S}, B_{\pi(S)}) \leq \frac{\phi_k(A) L_2(A)}{\rho k \sqrt{2}} \indent \text{for } S \in \mathcal{T}.
\Theta(A_{S}, B_{\pi(S)}) \leq \frac{\varepsilon\sqrt{k}}{ L(AX_{J(S)})}, \indent \text{for } S \in \mathcal{T}.
\end{align}

% TODO: I changed what's below:
Thus, from \eqref{rhs} and \eqref{yeyeye} it follows that, for any particular $S \in \mathcal{T}$, the inequality in \eqref{GapUpperBound} is satisfied for:
\[ \delta = \frac{ \rho k}{ \phi_\mathcal{T}(A) } \left(  \frac{\varepsilon \sqrt{k}}{L(AX_{J(S)})} \right) < \frac{L_2(A)}{\sqrt{2}}. \] %TODO: tell chris. took out Ce because that's for all S, not particular S
Hence, \eqref{GapUpperBound} holds (for all $S \in \mathcal{T}$) when $\delta = C\varepsilon$. %, with $C$ as defined in \eqref{Cdef}.
%\begin{align*}
%\delta = \left( \frac{ \varepsilon \sqrt{k^3}}{ \phi_k(A) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in \mathcal{T}} L_k(AX_{I(S)})}
%= C\varepsilon.
%\end{align*}
The result \eqref{Cstable} follows by application of Lem.~\ref{MainLemma} and, as demonstrated previously, \eqref{Cstable} implies \eqref{b-PDa}.
%It follows by Lem.~\ref{MainLemma} that there exists a permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ such that $|A_j - (BPD)_j|_2 \leq C\varepsilon$ for all $j \in [m]$. As demonstrated in the proof of the $k=1$ case, this implies \eqref{b-PDa}.
\end{proof}

%===================================
% 			DISCUSSION
%===================================
\section{Discussion}\label{Discussion}

In this note, we generalize recent results \cite{Hillar15} on the uniqueness of solutions to Problem~\ref{InverseProblem} to the case of noisy measurements while also significantly reducing the number of required samples.
% from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$. 
% CUT OK? Surprisingly, almost all $n \times m$ dictionaries satisfying the standard assumption \eqref{CScondition} from compressed sensing (CS) are identifiable from enough generic noisy $k$-sparse linear combinations of their elements, up to an error linear in the noise. Moreover, if solutions are constrained to satisfy \eqref{SparkCondition}, then only an upper bound on the number of dictionary elements need be taken as given. 
We remark that our result in the deterministic case (Thm.~\ref{DeterministicUniquenessTheorem}) accounts for \emph{worst-case} noise, whereas the ``effective'' noise might be much smaller when it is sampled from a given distribution; in such cases, the constants $C$ in Thms.~\ref{DeterministicUniquenessTheorem},~\ref{DeterministicUniquenessTheorem2} may be much smaller with high probability.
We note also that these results extend trivially to when point-wise injective nonlinearities are applied to the data. We close by outlining  four diverse application areas.
% TODO: to when? Not, to the case when?

%\textbf{Compressed Sensing}.
%The goal of compressed sensing is to recover a signal $\mathbf{x} \in \mathbb{R}^n$ that is sparse enough in some known basis (i.e., $\mathbf{a} = \Psi^{-1} \mathbf{x}$ is $k$-sparse for some invertible $\Psi$) via a stable and efficient reconstruction process after it has been linearly subsampled as $\mathbf{y} = \Phi \mathbf{x} + \mathbf{n}$ by a known compression matrix $\Phi \in \mathbb{R}^{n \times m}$ and noise $\mathbf{n}$. If $A = \Phi\Psi$ satisfies the spark condition \eqref{SparkCondition}, then $\mathbf{a}$ is identifiable given $\mathbf{y}$ and the signal $\mathbf{x}$ can then be reconstructed as $\Psi \mathbf{a}$. But what if $\Phi$ is unknown? Our theorems show that the matrix $A$ and sparse codes can still be estimated up to noise and an inherent permutation-scaling ambiguity if samples $\mathbf{y}_1, \ldots, \mathbf{y}_N$ are sufficiently diverse. 

% FRITZ: Inverse problems instead of Data Analysis. Don't assume that everyone is assuming they are recovering ground truth. Cite results again and state implications! Focus on them (probabilistic ones too).
\textbf{Inverse Problems}.  
Our results provide theoretical grounding for the use of sparse dictionary learning in blind source separation, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements generated as in \eqref{LinearModel} (e.g., recovering a rat's position on a linear track from local field potentials in Hippocampus \cite{Agarwal14}). It would be of practical utility therefore to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ (see Thm.~\ref{DeterministicUniquenessTheorem}) as well as the minimal requirements on the number and diversity of generating codes. %We encourage researchers to extend our results and tighten these parameters.

\textbf{Theoretical Neuroscience}.
Sparse dictionary learning and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12}, reproducing response properties of cortical neurons. Our theorems suggest that this correspondence could be due to the uniqueness of sparse representations. Furthermore, our guarantees justify the hypothesis of \cite{Coulter10} and \cite{Isely10} that sparse codes passed through a communication bottleneck in the brain can be recovered from random projections via (unsupervised) biologically plausible sparse dictionary learning (e.g., \cite{rozell2007neurally, hu2014hebbian}).   
%TODO: add reproduction of response propeties in olfactory cortex?

%A necessary condition for this theory to work is that the sparse coding problem has a unique solution stable with respect to noise, which our work verifies. 

% Reiterate probabilistic result in the smooth analysis part?
\textbf{Smoothed Analysis}.
The main concept in smoothed analysis \cite{Spielman04} is that certain algorithms having exponential worst-case behavior are, nonetheless, efficient if certain (typically, measure zero in the continuous case and with ``low probability" in the discrete case) pathological input sets are avoided. Our results imply that if there is an efficient ``smoothed" algorithm for solving Problem \ref{InverseProblem} given enough samples, then for generic inputs this algorithm determines the unique original solution. We note that avoiding ``bad" (NP-hard) sets of inputs is a necessary technicality for dictionary learning \cite{Razaviyayn15, Tillmann15}.

%\textbf{Neural Communication Theory}.
%In \cite{Coulter10} and \cite{Isely10}, it was posited that sparse features of natural data passed through a communication bottleneck in the brain using random projections could be decoded, unsupervised, via sparse coding.  Necessary for this theory to work is that sparse coding has a unique solution stable with respect to noise, our main result. 
%This was already verified in the case of data sampled without noise in \eqref{Hillar15}.  Our work extends this theory to the more realistic case of sampling error.

\textbf{Engineering}.
Several groups utilize compressive sensing for signal processing tasks: MRI analysis \cite{lustig2008compressed},  image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of compressive sensing, it is only a matter of time before these systems incorporate sparse dictionary learning to encode and process data. Guarantees such as those offered by our theorems allow any such device to be equivalent to any other (having different initial parameters and data samples) as long as enough data originate from a statistically identical system.
% MENTION THE PEOPLE AT CARNEGIE MELON WHO CONTACTED US?

% FRITZ: Cite Mickey Lustig

%===================================
% 		ACKNOWLEDGEMENT
%===================================
%\section*{Acknowledgment}
We thank Fritz Sommer for turning our attention to the dictionary learning problem, Darren Rhea for sharing early explorations, and Ian Morris for posting identity \eqref{SubspaceMetricSameDim} online.  
%Support for CG and CH provided, in part, by National Science Foundation grant IIS-1219212 and the Statistical and Applied Mathematical Sciences Institute, under NSF DMS-1127914.

%a reference to his proof on the internet (www.stackexchange.com). %Finally, we thank Bizzyskillet of Soudcloud.com for the ``No Exam Jams'', which played on repeat during many long hours of designing proofs.


%\vspace{-.5 cm} 

%===================================
% 			REFERENCES
%===================================

% IF WE NEED ROOM: remove folland
\bibliographystyle{IEEEtran}
\bibliography{chazthm_ieee}

%===================================
% 			BIOGRAPHY
%===================================
%\vspace{-1.07 cm}
%\begin{IEEEbiographynophoto}{Charles J. Garfinkle}: B.S. Physics, B.S. Chemistry, McGill University, 2010. Currently, Ph.D. candidate, Neuroscience, University of California, Berkeley.
%\end{IEEEbiographynophoto}
%\vspace{-1.1 cm} 
%\begin{IEEEbiographynophoto}{Christopher J. Hillar:} B.S. Mathematics, B.S. Computer Science, Yale University, 2000; Ph.D., Mathematics, University of California, Berkeley, CA 2005.  Currently, Redwood Center for Theoretical Neuroscience, UCB.
%%; NSF Postdoctoral Fellow, Texas A\&M University, College Station, TX, 2005--2008; NSF All Institutes Postdoctoral Fellow, Mathematical Sciences Research Institute, Berkeley, CA, 2008--2010; Redwood Center for Theoretical Neuroscience, UCB, 2010--.  % , and in 2011, began working part time in psychiatry department at UC San Francisco.
%\end{IEEEbiographynophoto}


 \clearpage

\appendices
\section{Combinatorial Matrix Analysis}\label{appendixA}

Here, we prove Lem.~\ref{MainLemma}, which is the main ingredient in our proof of Thm.~\ref{DeterministicUniquenessTheorem}. We then outline how additionally assuming the spark condition \eqref{SparkCondition} for $B$ simplifies the proof and also allows for its extension to the case where only an upper bound on the number of columns $m$ of $A$ is known (Thm.~\ref{DeterministicUniquenessTheorem2}). 
%This extension is applied to the proof of  Thm.~\ref{DeterministicUniquenessTheorem2} in Appendix~\ref{AppendixB}. 

We first prove some auxiliary lemmas, starting with a proof of Lem.~\ref{MinDimLemma}, which was stated in Section \ref{DUT}. 
\begin{proof}[Proof of Lemma \ref{MinDimLemma}]
We prove the contrapositive.  If $\dim(U) > \dim(V)$, then a dimension argument ($\dim U + \dim V^\perp > m$) gives a nonzero $\mathbf{u} \in U \cap V^\perp$.  In particular, we have $|\mathbf{u} - \mathbf{v}|_2^2 = |\mathbf{u}|_2^2 + |\mathbf{v}|_2^2 \geq |\mathbf{u}|_2^2$ for $\mathbf{v} \in V$, and thus $d(\mathbf{u},V) \geq |\mathbf{u}|_2$.
\end{proof}

Given sets $\mathcal{T}$, let $\cap \mathcal{T}$ denote their intersection.
%===== SPAN INTERSECTION LEMMA =====
\begin{lemma}\label{SpanIntersectionLemma}
Let $M \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $M$ are linearly independent, then for any $\mathcal{T} \subseteq \bigcup_{\ell \leq k} {[m] \choose \ell}$, we have:
\begin{align*}
\text{\rm Span}\{M_{\cap \mathcal{T}}\}  = \bigcap_{S \in \mathcal{T}} \text{\rm Span}\{M_S\}.
\end{align*}
\end{lemma}

\begin{proof}By induction, it is enough to prove the lemma when $|\mathcal{T}| = 2$. The proof now follows directly from the assumption.
\end{proof}


%===== DISTANCE TO INTERSECTION LEMMA =====

% This can be made tighter by using Pythagoras' Thm for first projection instead of triangle inequality. 
\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $\mathcal{V} = \{V_1, \ldots, V_k\}$ be subspaces of $\mathbb{R}^m$ and set $V = \cap \mathcal{V}$. For every $\mathbf{x} \in \mathbb{R}^m$, we have:
\begin{align}\label{DTILeq}
d(\mathbf{x}, V) \leq \frac{1}{1 - \xi(\mathcal{V})} \sum_{i=1}^k d(\mathbf{x}, V_i),
\end{align}
where $\xi$ is given in Def.~\ref{SpecialSupportSet}.
\end{lemma}
\begin{proof} 
Fix $\mathbf{x} \in \mathbb{R}^m$ and $k \geq 2$. Recall that the orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V$ from $\mathbb{R}^m$ to $V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$:
\begin{align*}
|\mathbf{x} - \Pi_V\mathbf{x}|_2 = d(\mathbf{x}, V) := \min\{|\mathbf{x}-\mathbf{v}|_2: \mathbf{v} \in V\}.
\end{align*}
% Since subspaces of real vector spaces are closed, we can replace inf with min in def. of orthogonal projection

We begin the proof by observing that:
%Use Pythagoras' Theorem first?
\begin{align*}
|\mathbf{x} - \Pi_V\mathbf{x}|_2 &\leq |\mathbf{x} - \Pi_{V_k} \mathbf{x}|_2 + |\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}|_2 \nonumber \\
&\ \ \ + \cdots + |\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}|_2 \nonumber \\
&\leq \sum_{\ell=1}^k |\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}|_2 + |\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}|_2,
\end{align*}
%
by the triangle inequality and the fact that the spectral norm $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result \eqref{DTILeq} now follows by bounding the second term on the RHS using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}|_2 \leq z |\mathbf{x}|_2, %  \indent \text{for } \mathbf{x} \in \mathbb{R}^m,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$, this yields:
\begin{align*}
|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} |_2 
&= |\left( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V \right) (\mathbf{x} - \Pi_V\mathbf{x})|_2 \\
&\leq z|\mathbf{x} - \Pi_V\mathbf{x}|_2.
\end{align*}

Since $z$ may depend on the arbitrary labelling of the $V_i \in \mathcal{V}$, we substitute $\xi(\mathcal{V})$ for $z$ to obtain \eqref{DTILeq}.
\end{proof}

%======= GRAPH THEORY LEMMA =======

\begin{lemma}\label{NonEmptyLemma} Fix integers $k < m$, and let $\mathcal{T} = \{S_1, \ldots, S_m\}$ be the set of intervals of length $k$ in some cyclic order on $[m]$. Suppose there exists a map $\pi: \mathcal{T} \to {[m] \choose k}$ such that:
\begin{align}\label{NonEmpty}
|\bigcap_{j \in J} \pi(S_j)| \leq |\bigcap_{j \in J} S_j | \ \ \text{for } \ J \in {[m] \choose k}.
\end{align}
%
Then, $|\pi(S_{j_1}) \cap \cdots \cap \pi(S_{j_k})| = 1$ for all consecutive indices $j_1,\ldots,j_k$ in the order on $[m]$.
\end{lemma}

\begin{proof} Consider the set $Q_m = \{ (r,\ell) : r \in \pi(S_{\ell}), \ell \in [m] \}$, which has $mk$ elements. By the pigeon-hole principle, there is some $q \in [m]$ and $J \in {[m] \choose k}$ such that $(q, j) \in Q_m$ for all $j \in J$. In particular, we have $q \in \cap_{j \in J} \pi(S_j)$, so that from \eqref{NonEmpty} there must be some $p \in [m]$ with $p \in \cap_{j \in J} S_j$. Since $|J| = k$, this is only possible if the elements of $J = \{j_1, \ldots, j_k\}$ are consecutive modulo $m$, in which case $|\cap_{j \in J} S_j| = 1$. Hence, it follows that $|\cap_{j \in J} \pi(S_j)| = 1$ as well.

We next consider if any other $\ell \notin J$ is such that $q \in \pi(S_\mathcal{T})$. Suppose there were such a $\mathcal{T}$; then, we have \mbox{$q \in \pi(S_\mathcal{T}) \cap \pi(S_{j_1}) \cap \cdots \cap \pi(S_{j_k})$} and \eqref{NonEmpty} would imply that the intersection of every $k$-element subset of $\{S_\mathcal{T}\} \cup \{S_j: j \in J\}$ is nonempty. This would only be possible if $\{\ell\} \cup J = [m]$, in which case the result then trivially holds since then $q \in \pi(S_j)$ for all $j \in [m]$.  Suppose now there exists no such $\mathcal{T}$; then, letting $Q_{m-1} \subset Q_m$ be the set of elements of $Q_m$ not having $q$ as a first coordinate, we have $|Q_{m-1}| = (m-1)k$. 

By iterating the above arguments, we arrive at a partitioning of $Q_m$ into sets $R_i = Q_i \setminus Q_{i-1}$ for $i = 1, \ldots, m$, each having a unique element of $[m]$ as a first coordinate common to all $k$ elements while having second coordinates which form a consecutive set. In fact, every set of $k$ consecutive integers in the order is the set of second coordinates of some $R_i$. This must be the case because for every consecutive set $J$ we have $|\cap_{j \in J} S_j| = 1$, whereas if $J$ is the set of second coordinates for two distinct sets $R_i$, we would have \mbox{$|\cap_{j \in J} \pi(S_j)| > 1$}, violating \eqref{NonEmpty}. 
\end{proof}

%==== PROOF OF MAIN LEMMA =======
\begin{proof}[Proof of Lem.~\ref{MainLemma} (Main Lemma)]
We assume $k \geq 2$ since the case $k = 1$ was proven in Sec.~\ref{DUT}. For simplicity, we also assume without loss of generality that the cyclic order on $[m]$ is the trivial one so that $\mathcal{T} = \{S_1, \ldots, S_m\}$ has $S_i = \{i, \ldots, i+k-1\}$ for $i \in [m]$ as the intervals of length $k$ in the order. We begin by showing that $\dim(\text{Span}\{B_{\pi(S)}\}) = k$ for all $S \in \mathcal{T}$. 
Fix $S \in \mathcal{T}$ and note that by \eqref{GapUpperBound}, all unit vectors $\mathbf{u} \in \text{Span}\{A_{S}\}$ satisfy $d(\mathbf{u}, \text{Span}\{B_{\pi(S)}\}) \leq \frac{\phi_\mathcal{T}(A)}{\rho k} \delta$ for $\delta < \frac{L_2(A)}{ \sqrt{2}}$. By definition of $L_2(A)$, for all $2$-sparse $\mathbf{x} \in \mathbb{R}^m$:
\begin{align*}
L_2(A) \leq \frac{|A\mathbf{x}|_2}{|\mathbf{x}|_2} \leq \rho \frac{|\mathbf{x}|_1}{|\mathbf{x}|_2} \leq \rho \sqrt{2}.
\end{align*}

It follows that $\delta < \rho$. Since $\phi_\mathcal{T}(A) \leq 1$, we also have $d(\mathbf{u}, \text{Span}\{B_{\pi(S)}\}) < 1$, and so Lem.~\ref{MinDimLemma} implies that $\dim(\text{Span}\{B_{\pi(S)}\}) \geq \dim(\text{Span}\{A_{S}\}) = k$. Since $|\pi(S)| = k$, we in fact have $\dim(\text{Span}\{B_{\pi(S)}\}) = k$, i.e. the columns of $B_{\pi(S)}$ are linearly independent. %indep. is a trivial consequence of dim = k, but i state it here because it's referred to later -- easier to just glance back and see the statement.

We will now show that:
\begin{align}\label{fact2}
|\bigcap_{j \in J} \pi(S_j)| \leq |\bigcap_{j \in J} S_j |, \ \ \text{for } \ J \in {[m] \choose k}.
\end{align}

Fix $J \in {[m] \choose k}$. By \eqref{GapUpperBound}, for all unit vectors \mbox{$\mathbf{u} \in \cap_{j \in J} \text{Span}\{B_{\pi(S_j)}\}$}, we have  that $d(\mathbf{u}, A_{S_j}) \leq \frac{\phi_\mathcal{T}(A)}{\rho k} \delta$ for all $j \in J$, where $\delta < \frac{L_2(A)}{\sqrt{2}}$. It follows from Lem.~\ref{DistanceToIntersectionLemma} that:
\begin{align*}
d\left( \mathbf{u}, \bigcap_{j \in J} \text{Span}\{A_{S_j}\} \right) 
\leq \frac{\delta}{\rho} \left( \frac{ \phi_\mathcal{T}(A) }{1 - \xi( \{ A_{S_j}: j \in J\} ) } \right) \leq \frac{\delta}{\rho},
\end{align*}
%
where the second inequality follows from Def.~\ref{SpecialSupportSet}. % Recall that $\xi < 1$ always.

Now, since \mbox{$\text{Span}\{B_{\cap_{j \in J}\pi(S_j)}\} \subseteq \cap_{j \in J} \text{Span}\{B_{\pi(S_j)}\}$} and (by Lem.~\ref{SpanIntersectionLemma}) $\cap_{j \in J}  \text{Span}\{A_{S_j}\} = \text{Span}\{A_{\cap_{j \in J}  S_j}\}$, we have:
\begin{align}\label{fact1}
d\left( \mathbf{u}, A_{\cap_{j \in J} S_j} \right) \leq \frac{\delta}{\rho}, \indent \text{for unit } \mathbf{u} \in \text{Span}\{B_{\cap_{j \in J}\pi(S_j)}\}.
\end{align}
In particular, Lem.~\ref{MinDimLemma} (since $\delta/\rho < 1$) implies that $\dim(\text{Span}\{B_{\cap_{j \in J}\pi(S_j)}\}) \leq \dim(\text{Span}\{A_{\cap_{j \in J} S_j}\})$ and \eqref{fact2} follows from the linear independence of the columns of $A_{S_j}$ and $B_{\pi(S_j)}$ for all $j \in [m]$.

Suppose now that $J = \{\ell-k+1, \ldots, \ell\}$ for some $\ell \in [m]$ so that \mbox{$\cap_{j \in J} S_j = \{\ell\}$}. By \eqref{fact2}, we have that $\cap_{j \in J} \pi(S_j)$ is either empty or it contains a single element. Lem.~\ref{NonEmptyLemma} ensures that the latter case is the only possibility. Thus, the association $\ell \mapsto \cap_{j \in J} \pi(S_j)$ defines a map $\hat \pi: [m] \to [m]$. Recalling \eqref{SubspaceMetricSameDim}, it follows from \eqref{fact1} that for all unit vectors $\mathbf{u} \in \text{Span}\{A_\ell\}$, we have $d\left( \mathbf{u}, B_{\hat \pi(\ell)}\right) \leq \delta/\rho$ also. Since $\ell$ is arbitrary, it follows that for every basis vector $\mathbf{e}_\ell \in \mathbb{R}^m$, letting $c_\ell = |A\mathbf{e}_\ell |_2^{-1}$ and $\varepsilon = \delta/\rho$, there exists some $c'_\ell \in \mathbb{R}$ such that $|c_\ell A\mathbf{e}_\ell - c'_\ell B\mathbf{e}_{\hat \pi(\ell)}|_2 \leq \varepsilon$ where $\varepsilon < \frac{L_2(A)}{\sqrt{2}} \min_{\ell \in [m]} c_{\ell}$. This is exactly the supposition in \eqref{1D}, and the result follows from the subsequent arguments of Sec.~\ref{DUT}.
\end{proof}

The strategy above can be easily modified to prove the following variation of Lem.~\ref{MainLemma}, key to proving Thm.~\ref{DeterministicUniquenessTheorem2}. Let $\pi(\mathcal{T})$ denote the set $\{ \pi(S): S \in \mathcal{T}\}$. 
\begin{lemma}[Main Lemma~for $m < m'$]\label{MainLemma2}
Fix positive integers $n, m, m'$ and $k$, where $k < m < m'$, and let $\mathcal{T}$ be the set of intervals of length $k$ in some cyclic order on $[m]$. Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times m'}$ both satisfy spark condition \eqref{SparkCondition} with $A$ having maximum column $\ell_2$-norm $\rho$. If there exists a map $\pi: \mathcal{T} \to {[m'] \choose k}$ and some $\delta < \frac{L_{2}(A)}{\sqrt{2}}$ such that for $S \in \mathcal{T}$:
\begin{equation}\label{GapUpperBound2}
\Theta(A_{S}, B_{\pi(S)}) \leq \frac{ \delta }{\rho k} \min(\phi_\mathcal{T}(A), \phi_{\pi(\mathcal{T})}(B)),
\end{equation}
then \eqref{MainLemmaBPD} holds for some $n \times m$ submatrix of $B$. 
\end{lemma}

We state the required modifications briefly. Since $m' > m$, we may not invoke Lem.~\ref{NonEmptyLemma} (which requires $m = m'$) to show that $|\cap_{j \in J} \pi(S_j)| = 1$ when $J = \{\ell-k+1, \ldots, \ell\}$ for any $\ell \in [m]$. Instead, under the additional assumption that $B$ satisfies the spark condition, we may simply swap the roles of $A$ and $B$ in the proof of \eqref{fact1} to show that $\dim(\text{Span}\{B_{\cap_{j \in J}\pi(S_j)}\}) = \dim(\text{Span}\{A_{\cap_{j \in J} S_j}\}) = 1$, from which the required fact then follows. The map $\hat \pi$ is then defined similarly, only now with codomain $[m']$, thereby reducing the proof to the $k=1$ case where the $n \times m$ submatrix of $B$ is formed from the columns indexed by the image of $\hat \pi$. 

% TODO: Can we prove theorem 3 without spark condition on B via the inductive method of the last paper, i.e. filling all the supports instead of just those in T?

%\begin{remark} In general, there may exist combinations of fewer supports with intersection $\{i\}$, e.g. if $m \geq 2k-1$ then $S_{i - (k-1)} \cap S_i = \{i\}$. For brevity, we have considered a construction that is valid for any $k < m$.
%\end{remark}

\section{Proofs of Thms.~\ref{robustPolythm} \& \ref{DeterministicUniquenessTheorem2} and Cors.~\ref{DeterministicUniquenessCorollary} \& \ref{ProbabilisticCor}}\label{AppendixB}

\begin{proof}[Proof of Cor.~\ref{DeterministicUniquenessCorollary}]
We need only demonstrate how to produce $N$ vectors $\mathbf{a}_i$ such that for every interval of length $k$ in some cyclic order on $[m]$, there are  \mbox{$(k-1){m \choose k}+1$} vectors in general linear position supported there. Let $\gamma_1, \ldots, \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^{\ell}_i)^{k,N}_{\ell,i=1}$ are in general linear position (since the $\gamma_i$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Next, fix a cyclic order on $[m]$ and let $\mathcal{T}$ be the set of intervals of length $k$ in the order. Finally, form the $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ with supports $S \in \mathcal{T}$ (partitioning the $a_i$ evenly among these supports so that each contains $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$) by setting the nonzero values $\mathbf{a}_i$ to be those contained in the $i$th column of $V$.
\end{proof}

%For the proof of Thm.~\ref{ProbabilisticTheorem}, as in \cite{Hillar15}, consider ``symbolic" datasets $Y$ involving indeterminates $A$ and $\mathbf{a}_i$.
We now determine classes of datasets $Y$ having a stable sparse coding that are cut out by a single polynomial equation.
%We have the following statement for uniqueness in sparse coding.
% borrow the following observation from \cite{Hillar15}.  
%Consider the collection of indeterminates in a set of $k$-sparse vectors $\{\mathbf{a}_1, \ldots, \mathbf{a}_k\}$.
% Here, for simplicity ``random" means that the nonzero entries of the sparse vectors $\mathbf{a}$ are i.i.d. uniform in $[0,1]$.

%\begin{lemma}\label{Hillar15lemma2}
%Fix $n, m$, $k < m$, and a matrix $A = (A_{ij})_{i,j=1}^{n,m}$ of indeterminates. 
%Consider the symbolic dataset $Y = \{A\mathbf{a}_1,\ldots,A \mathbf{a}_N\}$, where the supports of the indeterminate $k$-sparse vectors $\mathbf{a}_i$ are consecutive in some cyclic order. Those substitutions of real numbers for the indeterminates denying $Y$ a robust sparse coding has Borel measure zero.
%\end{lemma}
\begin{proof}[Proof of Thm.~\ref{robustPolythm}]
We sketch the argument, leaving the details to the reader.
Let $M$ be the $n \times m$ matrix with columns $A\mathbf{a}_i$, $i \in [N]$.  Consider the following polynomial \cite[Sec.~IV]{Hillar15} in the entries of $A$ and the $\mathbf{a}_i$:
\begin{align*}
g(A, \{\mathbf{a}_i\}_{i=1}^N) = \prod_{S \in {[n] \choose k}} \sum_{S' \in {[N] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  

It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $A$ and $\mathbf{a}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $A$). The statement of the theorem now follows directly.
\end{proof}

 \begin{proof}[Proof of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ is such that $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell = 1, \ldots, p$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$ (e.g.,  \cite[Ex. ***]{folland2013real}). By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero whenever $Y$ has a stable $k$-sparse representation in $\mathbb R^m$; in particular, this property (stability) holds with probability one.
%form a set of Borel measure 0. Therefore, any matrix $A$ with samples drawn according to probability measures $\nu_{ij}$ which are absolutely continuous with respect to $\mu$ satisfy the spark condition with probability one.  Finally, apply Thm.~\ref{robustPolythm}.
\end{proof}


\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessTheorem2}]
The proof is very similar to the proof of Thm.~\ref{DeterministicUniquenessTheorem} in Sec.~\ref{DUT}, the main difference being that now we establish a map $\pi: [m] \to [m']$ satisfying the requirements of Lem.~\ref{MainLemma2} by pigeonholing $(k-1){m' \choose k} + 1$ vectors with respect to holes $[m']$ and eventually applying Lem.~\ref{MainLemma2} in place of Lem.~\ref{MainLemma}. The value of $C$ in this case is then:
\begin{align}\label{Cdef2}
C= \left( \frac{ \sqrt{k^3}}{ \min(\phi_\mathcal{T}(A), \phi_{{[m'] \choose k}}(B)) } \right) \frac{\max_{j \in [m]} |A_j|_2}{\min_{S \in \mathcal{T}} L_k(AX_{I(S)})}.
\end{align}
%
%for whichever map $\pi$ is implied by the fact that $C$ is not less than that defined in \eqref{Cdef}.
%This insures that we can establish a one-to-one correspondence between subspaces spanned by the $m'$ columns of $B$ and nearby subspaces spanned by the $m$ columns of $A$ despite the fact that $m < m'$. 
\end{proof}


%\begin{lemma}\label{Hillar15lemma2}
%Fix $n, m$, $k < m$, and a matrix $M \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. 
%Consider the collection of $k^2$ indeterminates in a set of $k$-sparse vectors $\{\mathbf{a}_1, \ldots, \mathbf{a}_k\}$.  Those substitutions of real numbers for those indeterminates for which $\{M\mathbf{a}_1, \ldots, M\mathbf{a}_k\}$ are linearly dependent has Borel measure zero (in $\mathbb R^{k^2}$).
%\end{lemma}

% [ *** TODO: Define `random' *** ]

% Also according to \cite{Hillar15}, fixing $S \in {[m] \choose k}$, any set of $k$-sparse vectors $\mathbf{a}_1, \ldots, \mathbf{a}_k \in \mathbb{R}^m$ supported on $S$ are in general linear position with probability one. 

%\begin{theorem}\label{Theorem2}
%Fix $n, m$, $k < m$, and $A \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition}. If  $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for each interval of length $k$ in some cyclic order on $[m]$ there are $(k-1){m \choose k} + 1$ vectors $\mathbf{a}_i$ supported on that interval then $\{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ has a robust  $k$-sparse representation with probability one.
%\end{theorem}

%\begin{proof}
%By Lem.~\ref{Hillar15lemma2} (taking $M = I$), with probability one the $\mathbf{a}_i$ are in general linear position. Apply Thm.~\ref{DeterministicUniquenessTheorem}.
%\end{proof} 

%\begin{corollary}
%Suppose $m, n$, and $k$ satisfy inequality \eqref{CScondition}. With probability one, a random $n \times m$ generation matrix $A$ satisfies \eqref{SparkCondition}. Fixing such an $A$, we have with probability one that a dataset $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N\}$ generated from a random draw of $N = m(k-1){m \choose k}+m$ $k$-sparse vectors $\mathbf{a}_i$, consisting of $(k-1){m \choose k}+1$ samples supported on each interval of length $k$ in some cyclic ordering of $[m]$, has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
%\end{corollary}

%Note that an \emph{algebraic set} is a solution to a finite set of polynomial equations. 

%\begin{theorem}\label{Theorem3}
%Fix $k < m$ and $n$ . If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval, then with probability one the following holds. There is an algebraic set $Z \subset \mathbb{R}^{n \times m}$ of Lebesgue measure zero with the following property: if $A \notin Z$ then $\{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$ has a robustly identifiable $k$-sparse representation in $\mathbb{R}^m$.
%\end{theorem}

%\begin{proof}
%By Lem.~\ref{Hillar15lemma2} (setting $M$ to be the identity matrix), with probability one the $\mathbf{a}_i$ are in general linear position. By the same arguments made in the proof of Thm.~3 in \cite{Hillar15}, the set of matrices $A$ that fail to satisfy \eqref{SparkCondition} form an algebraic set of $\mu$-measure zero. Apply Thm.~\ref{DeterministicUniquenessTheorem}.
%\end{proof}

%\begin{corollary}
%Suppose $m, n$, and $k$ obey inequality \eqref{CScondition}.  If $N = m(k-1){m \choose k}+m$ randomly drawn vectors $\mathbf{a}_1, \ldots, \mathbf{a}_N \in \mathbb{R}^m$ are such that for every interval of length $k$ in some cyclic ordering of $[m]$ there are $(k-1){m \choose k}+1$ vectors $\mathbf{a}_i$ supported on that interval then, with probability one, almost every matrix $A \in \mathbb{R}^{n \times m}$ gives a robustly identifiable $Y = \{A\mathbf{a}_1, \ldots , A\mathbf{a}_N \}$.
%\end{corollary}

%\begin{proof}
%In \cite{Hillar15}, it was demonstrated how to construct a polynomial in the entries of the matrix $A$ and $k$-sparse vectors $\mathbf{a}_i$ which does not evaluate to zero if and only if $A$ satisfies \eqref{SparkCondition} and  the $\mathbf{a}_i$ which share supports are in general linear position. By [Baraniuk], there is an $A$ which satisfies the spark condition and by the Vandermonde construction there are $\mathbf{a}_i$ in general linear position. By poly trick, the set of zeroes of $f$ form a set of $\mu$-measure 0. Hence the set of matrices and vectors which do not satisfy our constraints form a set of $\mu$-measure 0, where $\mu$ is the standard Borel product measure. By Theorem 1, the set of datasets $Y =  \{A\mathbf{a}_1, \ldots, A\mathbf{a}_N\}$ for which the entries of $A$ and the $k$-sparse vectors $\mathbf{a}_i$ are all independently drawn according to probability measures which are absolutely continuous with respect to $\mu$ form a set of $\mu$-measure 0. 
%\end{proof}

%\section{Extras}
%
%\begin{remark}
%We demonstrate with the following counter-example that for $C$ as defined in \eqref{Cdef} the condition $\varepsilon < \frac{L_2(A)}{\sqrt{2}}C^{-1}$ is necessary to guarantee in general that \eqref{Cstable} follows from the remaining assumptions of Theorem \ref{DeterministicUniquenessTheorem}. Let $A$ be the identity matrix in $\mathbb{R}^{m \times m}$ and let $\mathbf{a}_i = \mathbf{e}_i$ for $i = 1, \ldots, m$. Then $L_2(A) = 1$ (we have $|A\mathbf{x}|_2 = |\mathbf{x}|_2$ for all $\mathbf{x} \in \mathbb{R}^m$) and $C = 1$; hence $\frac{L_2(A)}{\sqrt{2}}C^{-1} = 1/\sqrt{2}$. Suppose $\varepsilon \geq 1/\sqrt{2}$. Then the alternate dictionary $B = \left(\mathbf{0}, \frac{1}{2}(\mathbf{e}_1 + \mathbf{e}_2), \mathbf{e}_3, \ldots, \mathbf{e}_{m} \right)$ and sparse codes $\mathbf{b}_i = \mathbf{e}_2$ for $i = 1, 2$ and $\mathbf{b}_i = \mathbf{e}_i$ for $i = 3, \ldots, m$ satisfy $|A\mathbf{a}_i - B\mathbf{b}_i| = 1/\sqrt{2}$ for $i = 1, 2$ (and $0$ otherwise), and yet there exist no permutation matrix $P \in \mathbb{R}^{m \times m}$ and invertible diagonal matrix $D \in \mathbb{R}^{m \times m}$ satisfying $|A_i-(BPD)_i| \leq C\varepsilon$ for all $i \in [m]$, since we always have $|(A-BPD)P^{-1}\mathbf{e}_1|_2 = |P^{-1}\mathbf{e}_1|_2 =  1$. 
%\end{remark}

%
%\section{TODO}
%\begin{itemize}
%\item steamroll supplemental section
%\item generate figure showing linear regime
%\item can the conclusion of Lemma \ref{NonEmptyLemma} be that $\{ \pi(S): S \in \mathcal{T} \}$ is the set of consecutive length-$k$ intervals for some cyclic order on $[m]$?
%\end{itemize}

\end{document}